---
layout: post
title: Module 6 – Week 3
date: 17-11-2025
categories: [AIO2025, Module6, Deep Learning]
use_math: true
---

{% include mathjax.html %}

# Multi-layer Perceptron

If Deep Learning were a "big family" of models, the **Multi-layer Perceptron (MLP)** would be the first-born child — simple, intuitive, and yet responsible for laying the foundation for nearly every advanced architecture that followed, including CNNs, RNNs, Transformers, and modern diffusion models.

Before MLPs became widely adopted, many tasks relied on linear models such as **Linear Regression**, **Logistic Regression**, or **Softmax Regression**. While effective in basic scenarios, these models share a fundamental limitation: **they can only learn linear relationships between inputs and outputs**.

As a result, they fail when the data contains nonlinear structures, for example:

- when the decision boundary is curved or highly complex,
- when features interact in nonlinear ways,
- or when the data forms shapes that cannot be separated by a straight line, such as concentric circles.

In these cases, linear models are inherently limited because **they can only produce straight, flat decision boundaries**.

The need to capture nonlinear patterns naturally led to the introduction of the MLP. By stacking **multiple linear layers** and applying **nonlinear activation functions**, an MLP can "bend" the input space, allowing the model to learn complex mappings that traditional linear models cannot represent.

## Part 1 – Review of Softmax Regression

Softmax Regression is an extension of logistic regression designed for **multi-class classification**. Instead of producing a single probability for the positive class, it outputs a **probability distribution** across all possible classes.

### 1. Model Formulation

For an input vector $x$, each class $k$ is assigned a score:

$$
z_k = w_k^T x + b_k
$$

These scores are then converted into normalized probabilities using the softmax function:

$$
\text{softmax}(z_k) = \frac{\exp(z_k)}{\sum_{j=1}^{K} \exp(z_j)}
$$

This ensures two properties essential for multi-class prediction:

- each predicted value is non-negative,
- and the entire set of outputs sums to 1.

**Cross-Entropy Loss**

Softmax Regression is typically trained using the **Cross-Entropy Loss**, which compares the model's predicted distribution with the true class distribution (commonly represented as a one-hot vector).

For the correct class $y$:

$$
L = - \log(p_y)
$$

This loss penalizes the model proportionally to how much probability it assigns to the true class.

General procedure of **Softmax Regression** using **Gradient Descent**:

<p align="center">
  <img src="{{ '/assets/module6-week3/softmaxregression.png' | relative_url }}" alt="Pipeline Diagram" width="600">
</p> 

### 2. Relation to MLP Output Layers

The combination of **Softmax** and **Cross-Entropy** is widely regarded as the standard choice for the output layer in classification models. Softmax converts the raw logits into a meaningful probability distribution over the classes, allowing the network to express how confident it is in each prediction.

At the same time, the Cross-Entropy loss measures how far the predicted probability distribution is from the true label distribution. This loss provides a clear learning signal that guides the model during training, helping it adjust its weights in the right direction.

When used as the final stage of an MLP, the Softmax and Cross-Entropy pair forms an effective prediction mechanism. The hidden layers focus on extracting and representing relevant features, while the output layer interprets these representations by producing probabilities and evaluating how accurate they are. This combination has become a widely adopted choice in modern neural network architectures because of its stability, efficiency, and strong performance in classification tasks.

## Part 1 – Multi-layer Perceptron

### 1. Single-layer Perceptron

A single-layer perceptron is the simplest form of an artificial neural network. It consists of a single computational unit that receives several input features, combines them through a weighted linear function, and produces an output through an activation function. Despite its simplicity, it is an important conceptual foundation for understanding how more complex neural networks operate.

### How it works

<p align="center">
  <img src="{{ '/assets/module6-week3/single-perceptron.png' | relative_url }}" alt="Pipeline Diagram" width="600">
</p> 

The perceptron takes an input vector $x = (x_1, x_2, ..., x_m)$. Each input is multiplied by a corresponding weight $w_i$, and together with a bias term $b$, they form a linear combination:

$$
z = \sum_{i=1}^{m} w_i x_i + b
$$

The resulting value is then passed through an activation function $\phi(z)$, typically a threshold-based function for binary decisions:

$$
\hat{y} = \phi(z)
$$

This computation enables the perceptron to determine how strongly each input feature contributes to the final decision.

### Interpretation

A single-layer perceptron acts as a linear classifier. It divides the input space using a hyperplane: points on one side of the hyperplane are assigned to one class, while points on the other side are assigned to another. This makes the perceptron suitable for problems where the classes are linearly separable — that is, when a straight boundary is sufficient to distinguish between them.

### Limitations

Because all computations in the perceptron are linear, it cannot capture nonlinear patterns in data. Tasks where decision boundaries are curved or where class structures are intertwined cannot be learned by this model. As a result, the single-layer perceptron is inadequate for many practical problems where feature interactions are complex and inherently nonlinear.

### Direction of development

To overcome these limitations, multiple perceptrons can be stacked into successive layers. By introducing intermediate hidden layers along with nonlinear activation functions, the model gains the ability to represent complex, nonlinear mappings. This layered extension leads to the architecture known as the **Multi-layer Perceptron (MLP)**, which forms the basis for more advanced deep learning models.

### 2. Multi-layer Perceptron

A Multi-layer Perceptron (MLP) is a neural network architecture composed of multiple layers of transformations applied sequentially. At each layer, the network performs a linear combination of the incoming signals, followed by a nonlinear activation function. Through this layered composition, an MLP can model complex relationships between the input and the output, far beyond the capability of a single-layer perceptron.

### General Structure

An MLP consists of three main parts: the input layer, the hidden layers, and the output layer.

At layer $\ell$, each neuron receives all activations from the previous layer, combines them using weights and a bias term, and produces a pre-activation value:

$$
z_j^{(\ell)} = \sum_i w_{ij}^{(\ell)} a_i^{(\ell-1)} + b_j^{(\ell)}
$$

Where:

- $a_i^{(\ell-1)}$: activation of neuron $i$ in the previous layer
- $w_{ij}^{(\ell)}$: weight connecting neuron $i$ (layer $\ell - 1$) to neuron $j$ (layer $\ell$)
- $b_j^{(\ell)}$: bias of neuron $j$ in layer $\ell$
- $z_j^{(\ell)}$: pre-activation value at layer $\ell$

The pre-activation is then passed through a nonlinear activation function to produce the neuron's output:

$$
a_j^{(\ell)} = \phi\left(z_j^{(\ell)}\right)
$$

Where:

- $\phi(\cdot)$: nonlinear activation function (sigmoid, tanh, ReLU, etc.)
- $a_j^{(\ell)}$: activation of neuron $j$ in layer $\ell$

These transformations repeat across all layers, similar to letting data pass through a series of "filters," with each layer reshaping the representation in its own way.

### Role of Each Layer

<p align="center">
  <img src="{{ '/assets/module6-week3/mlp.png' | relative_url }}" alt="Pipeline Diagram" width="600">
</p> 

The input layer passes the raw data into the network as a feature vector.

The hidden layers perform the essential work of representation learning: they mix the features through linear transformations and then apply nonlinear activations, allowing the model to gradually uncover richer and more abstract patterns. Across layers, the data becomes increasingly structured toward the final task.

The output layer takes the final transformed representation and produces a prediction—either a numeric value for regression or a probability distribution for classification.

### Mathematical Representation

From a mathematical perspective, an MLP is a composition of functions.

For layer $\ell$, the parameters consist of a weight matrix and bias vector:

$$
W^{(\ell)} \in \mathbb{R}^{d^{(\ell)} \times d^{(\ell-1)}}, \quad b^{(\ell)} \in \mathbb{R}^{d^{(\ell)}}
$$

The output layer is defined using:

$$
W^{(L+1)} \in \mathbb{R}^{d^{(L+1)} \times d^{(L)}}, \quad b^{(L+1)} \in \mathbb{R}^{d^{(L+1)}}
$$

The alternating sequence of linear and nonlinear operations allows the model to express highly flexible functions.


### Function and Interpretation of Layer Transformations

Each layer in the MLP can be seen as a mapping from one representation space to another.

If only linear mappings were used, the entire network would collapse into a single linear function. However, the inclusion of nonlinear activations after each layer gives the network the ability to bend, reshape, and reorganize the data distribution. This allows the MLP to separate patterns that are not linearly separable in the original input space.

### Representation Capability of MLP

With a sufficient number of neurons and layers, an MLP can approximate a wide variety of continuous functions. This makes it a powerful model for vector-based data and an essential foundation for many modern deep learning architectures.
