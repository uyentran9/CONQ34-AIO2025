---
layout: post
title: Module 6 – Week 3
date: 17-11-2025
categories: [AIO2025, Module6, Deep Learning]
use_math: true
---

#  Multi-layer Perceptron

If Deep Learning were a "big family" of models, the **Multi-layer Perceptron (MLP)** would be the first-born child — simple, intuitive, and yet responsible for laying the foundation for nearly every advanced architecture that followed, including CNNs, RNNs, Transformers, and modern diffusion models.

Before MLPs became widely adopted, many tasks relied on linear models such as **Linear Regression**, **Logistic Regression**, or **Softmax Regression**. While effective in basic scenarios, these models share a fundamental limitation: **they can only learn linear relationships between inputs and outputs**.

As a result, they fail when the data contains nonlinear structures, for example:

- when the decision boundary is curved or highly complex,
- when features interact in nonlinear ways,
- or when the data forms shapes that cannot be separated by a straight line, such as concentric circles.

In these cases, linear models are inherently limited because **they can only produce straight, flat decision boundaries**.

The need to capture nonlinear patterns naturally led to the introduction of the MLP. By stacking **multiple linear layers** and applying **nonlinear activation functions**, an MLP can "bend" the input space, allowing the model to learn complex mappings that traditional linear models cannot represent.

## Part 1 – Review of Softmax Regression

Softmax Regression is an extension of logistic regression designed for **multi-class classification**. Instead of producing a single probability for the positive class, it outputs a **probability distribution** across all possible classes.

### 1. Model Formulation

For an input vector $x$, each class $k$ is assigned a score:

$$
z_k = w_k^T x + b_k
$$

These scores are then converted into normalized probabilities using the softmax function:

$$
\text{softmax}(z_k) = \frac{\exp(z_k)}{\sum_{j=1}^{K} \exp(z_j)}
$$

This ensures two properties essential for multi-class prediction:

- each predicted value is non-negative,
- and the entire set of outputs sums to 1.

**Cross-Entropy Loss**

Softmax Regression is typically trained using the **Cross-Entropy Loss**, which compares the model's predicted distribution with the true class distribution (commonly represented as a one-hot vector).

For the correct class $y$:

$$
L = - \log(p_y)
$$

This loss penalizes the model proportionally to how much probability it assigns to the true class.

General procedure of **Softmax Regression** using **Gradient Descent**:

<p align="center">
  <img src="{{ '/assets/module6-week3/softmaxregression.png' | relative_url }}" alt="Pipeline Diagram" width="600">
</p> 

### 2. Relation to MLP Output Layers

The Softmax + Cross-Entropy combination forms the standard output layer for many classification networks.

In the context of the Basic MLP slides, this review highlights the final-stage prediction mechanism that is commonly used after the network's hidden layers.
