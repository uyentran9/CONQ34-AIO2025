---
layout: post
title: Module 6 – Week 3
date: 17-11-2025
categories: [AIO2025, Module6, Deep Learning]
use_math: true
---

{% include mathjax.html %}

# Multi-layer Perceptron

If Deep Learning were a "big family" of models, the **Multi-layer Perceptron (MLP)** would be the first-born child — simple, intuitive, and yet responsible for laying the foundation for nearly every advanced architecture that followed, including CNNs, RNNs, Transformers, and modern diffusion models.

Before MLPs became widely adopted, many tasks relied on linear models such as **Linear Regression**, **Logistic Regression**, or **Softmax Regression**. While effective in basic scenarios, these models share a fundamental limitation: **they can only learn linear relationships between inputs and outputs**.

As a result, they fail when the data contains nonlinear structures, for example:

- when the decision boundary is curved or highly complex,
- when features interact in nonlinear ways,
- or when the data forms shapes that cannot be separated by a straight line, such as concentric circles.

In these cases, linear models are inherently limited because **they can only produce straight, flat decision boundaries**.

The need to capture nonlinear patterns naturally led to the introduction of the MLP. By stacking **multiple linear layers** and applying **nonlinear activation functions**, an MLP can "bend" the input space, allowing the model to learn complex mappings that traditional linear models cannot represent.

## Part 1 – Review of Softmax Regression

Softmax Regression is an extension of logistic regression designed for **multi-class classification**. Instead of producing a single probability for the positive class, it outputs a **probability distribution** across all possible classes.

### 1. Model Formulation

For an input vector $x$, each class $k$ is assigned a score:

$$
z_k = w_k^T x + b_k
$$

These scores are then converted into normalized probabilities using the softmax function:

$$
\text{softmax}(z_k) = \frac{\exp(z_k)}{\sum_{j=1}^{K} \exp(z_j)}
$$

This ensures two properties essential for multi-class prediction:

- each predicted value is non-negative,
- and the entire set of outputs sums to 1.

**Cross-Entropy Loss**

Softmax Regression is typically trained using the **Cross-Entropy Loss**, which compares the model's predicted distribution with the true class distribution (commonly represented as a one-hot vector).

For the correct class $y$:

$$
L = - \log(p_y)
$$

This loss penalizes the model proportionally to how much probability it assigns to the true class.

General procedure of **Softmax Regression** using **Gradient Descent**:

<p align="center">
  <img src="{{ '/assets/module6-week3/softmaxregression.png' | relative_url }}" alt="Pipeline Diagram" width="600">
</p> 

### 2. Relation to MLP Output Layers

The combination of **Softmax** and **Cross-Entropy** is widely regarded as the standard choice for the output layer in classification models. Softmax converts the raw logits into a meaningful probability distribution over the classes, allowing the network to express how confident it is in each prediction.

At the same time, the Cross-Entropy loss measures how far the predicted probability distribution is from the true label distribution. This loss provides a clear learning signal that guides the model during training, helping it adjust its weights in the right direction.

When used as the final stage of an MLP, the Softmax and Cross-Entropy pair forms an effective prediction mechanism. The hidden layers focus on extracting and representing relevant features, while the output layer interprets these representations by producing probabilities and evaluating how accurate they are. This combination has become a widely adopted choice in modern neural network architectures because of its stability, efficiency, and strong performance in classification tasks.

## Part 1 – Multi-layer Perceptron

### 1. Single-layer Perceptron

A single-layer perceptron is the simplest form of an artificial neural network. It consists of a single computational unit that receives several input features, combines them through a weighted linear function, and produces an output through an activation function. Despite its simplicity, it is an important conceptual foundation for understanding how more complex neural networks operate.

### How it works

<p align="center">
  <img src="{{ '/assets/module6-week3/single-perceptron.png' | relative_url }}" alt="Pipeline Diagram" width="600">
</p> 

The perceptron takes an input vector $x = (x_1, x_2, ..., x_m)$. Each input is multiplied by a corresponding weight $w_i$, and together with a bias term $b$, they form a linear combination:

$$
z = \sum_{i=1}^{m} w_i x_i + b
$$

The resulting value is then passed through an activation function $\phi(z)$, typically a threshold-based function for binary decisions:

$$
\hat{y} = \phi(z)
$$

This computation enables the perceptron to determine how strongly each input feature contributes to the final decision.

### Interpretation

A single-layer perceptron acts as a linear classifier. It divides the input space using a hyperplane: points on one side of the hyperplane are assigned to one class, while points on the other side are assigned to another. This makes the perceptron suitable for problems where the classes are linearly separable — that is, when a straight boundary is sufficient to distinguish between them.

### Limitations

Because all computations in the perceptron are linear, it cannot capture nonlinear patterns in data. Tasks where decision boundaries are curved or where class structures are intertwined cannot be learned by this model. As a result, the single-layer perceptron is inadequate for many practical problems where feature interactions are complex and inherently nonlinear.

### -> Direction of development

To overcome these limitations, multiple perceptrons can be stacked into successive layers. By introducing intermediate hidden layers along with nonlinear activation functions, the model gains the ability to represent complex, nonlinear mappings. This layered extension leads to the architecture known as the **Multi-layer Perceptron (MLP)**, which forms the basis for more advanced deep learning models.
