## ğŸ“ˆ Pháº§n 1 â€“ Ã”n láº¡i Linear Regression

Linear Regression dá»± Ä‘oÃ¡n **giÃ¡ trá»‹ liÃªn tá»¥c**:  
$ \hat{y} = \mathbf{w}^\top \mathbf{x} + b $

Má»¥c tiÃªu: tÃ¬m $\mathbf{w}, b$ Ä‘á»ƒ **giáº£m sai sá»‘**:
$$
L_{\text{MSE}} = \frac{1}{n}\sum_{i=1}^n\left(\hat{y}^{(i)} - y^{(i)}\right)^2
$$

> NhÆ°á»£c Ä‘iá»ƒm khi dÃ¹ng cho phÃ¢n lá»›p: $\hat{y}$ cÃ³ thá»ƒ ngoÃ i $[0,1]$, Ä‘Æ°á»ng quyáº¿t Ä‘á»‹nh tuyáº¿n tÃ­nh dá»… â€œvÆ°á»£t rÃ oâ€.

---

## ğŸ”€ Pháº§n 2 â€“ VÃ¬ sao cáº§n Logistic Regression?

BÃ i toÃ¡n **nhá»‹ phÃ¢n** (0/1) cáº§n **xÃ¡c suáº¥t** $P(y=1\mid \mathbf{x}) \in [0,1]$.  
Ta â€œnÃ©nâ€ Ä‘áº§u ra báº±ng **Sigmoid**:
$$
\sigma(z) = \frac{1}{1+e^{-z}},\qquad z=\mathbf{w}^\top \mathbf{x} + b
$$

Dá»± Ä‘oÃ¡n:
- XÃ¡c suáº¥t: $\hat{p} = \sigma(\mathbf{w}^\top \mathbf{x} + b)$
- NhÃ£n: $\hat{y} = \mathbb{1}[\hat{p} \ge \tau]$ (thÆ°á»ng $\tau = 0.5$)

---

## ğŸ¯ Pháº§n 3 â€“ HÃ m máº¥t mÃ¡t: MSE hay BCE?

- DÃ¹ng MSE cho xÃ¡c suáº¥t **khÃ´ng tá»‘t**: Ä‘áº¡o hÃ m nhá» khi gáº§n 0/1 â†’ há»c cháº­m; tá»‘i Æ°u dá»… â€œbÆ°á»›ngâ€.
- **Binary Cross-Entropy (Log Loss)** phÃ¹ há»£p hÆ¡n:
$$
L_{\text{BCE}}
= -\frac{1}{n}\sum_{i=1}^n \Big( y^{(i)}\log \hat{p}^{(i)} + (1-y^{(i)})\log\big(1-\hat{p}^{(i)}\big) \Big)
$$
Trong Ä‘Ã³ $\hat{p}^{(i)}=\sigma(\mathbf{w}^\top \mathbf{x}^{(i)} + b)$.

> Trá»±c giÃ¡c: pháº¡t náº·ng khi dá»± Ä‘oÃ¡n xÃ¡c suáº¥t tháº¥p cho lá»›p Ä‘Ãºng (vÃ­ dá»¥ sá»± kiá»‡n xáº£y ra mÃ  em láº¡i tá»± tin nÃ³i â€œkhÃ´ng xáº£y raâ€).

---

## ğŸ› ï¸ Pháº§n 4 â€“ Há»c tham sá»‘ báº±ng Gradient Descent

Vá»›i 1 máº«u $(\mathbf{x}, y)$, Ä‘áº¡o hÃ m theo $\mathbf{w}$ cá»§a BCE:
$$
\nabla_{\mathbf{w}} L = (\hat{p}-y)\,\mathbf{x},\qquad
\frac{\partial L}{\partial b} = \hat{p}-y
$$
Cáº­p nháº­t:
$$
\mathbf{w} \leftarrow \mathbf{w} - \eta \, (\hat{p}-y)\mathbf{x},\qquad
b \leftarrow b - \eta \, (\hat{p}-y)
$$
($\eta$: learning rate)

---

## ğŸ§­ Pháº§n 5 â€“ Decision Boundary & Prob. View

- BiÃªn quyáº¿t Ä‘á»‹nh khi $\hat{p}=0.5 \iff \mathbf{w}^\top \mathbf{x}+b=0$ â‡’ lÃ  **siÃªu pháº³ng tuyáº¿n tÃ­nh**.
- NhÃ¬n theo **log-odds**:
$$
\log\frac{\hat{p}}{1-\hat{p}} = \mathbf{w}^\top \mathbf{x} + b
$$
â†’ Logistic Regression chÃ­nh lÃ  **há»“i quy tuyáº¿n tÃ­nh trÃªn log-odds**.

---

## ğŸ“š TÃ i nguyÃªn
- [Coding cho bÃ i giáº£ng cá»§a tháº§y Quang-Vinh Dinh (Google Drive)](https://â€¦)  
- [Wikipedia â€“ Logistic Function](https://en.wikipedia.org/wiki/Logistic_function)

---

## ğŸ§© Mini Quiz (vui há»c ğŸ²)  
1ï¸âƒ£ Logistic Regression dá»± Ä‘oÃ¡n **cÃ¡i gÃ¬**? â†’ **XÃ¡c suáº¥t** $P(y=1\mid \mathbf{x})$ trong $[0,1]$.  

2ï¸âƒ£ VÃ¬ sao chá»n **BCE** thay vÃ¬ **MSE**? â†’ PhÃ¹ há»£p xÃ¡c suáº¥t + gradient â€œkhá»eâ€ hÆ¡n.  

3ï¸âƒ£ Khi nÃ o $\hat{y}=1$? â†’ Khi $\sigma(\mathbf{w}^\top\mathbf{x}+b)\ge0.5$ (tÆ°Æ¡ng Ä‘Æ°Æ¡ng $\mathbf{w}^\top\mathbf{x}+b\ge0$). 

4ï¸âƒ£ Sigmoid function Ä‘áº£m báº£o Ä‘iá»u gÃ¬?  
   â˜‘ Äáº§u ra luÃ´n trong [0,1]  
   â˜ Tá»‘c Ä‘á»™ há»c nhanh hÆ¡n  

---

ğŸ’¡ *Háº¹n gáº·p á»Ÿ bÃ i giáº£ng tiáº¿p theo cá»§a tuáº§n 1 â€“ Khi chÃºng ta dive sÃ¢u hÆ¡n vÃ o Multi-feature Logistic Regression nhÃ©!* ğŸš€


---

[ğŸ  Vá» trang chá»§]({{ '/' | relative_url }})  



