## ğŸ“ˆ Pháº§n 1 â€“ Ã”n láº¡i Linear Regression

**Linear Regression (Há»“i quy tuyáº¿n tÃ­nh)** lÃ  má»™t mÃ´ hÃ¬nh dÃ¹ng Ä‘á»ƒ **dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c** dá»±a trÃªn cÃ¡c biáº¿n Ä‘áº§u vÃ o (*features*).  
Má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh lÃ  tÃ¬m má»™t Ä‘Æ°á»ng tháº³ng (hoáº·c siÃªu pháº³ng trong khÃ´ng gian nhiá»u chiá»u) sao cho tá»•ng sai sá»‘ giá»¯a dá»± Ä‘oÃ¡n vÃ  dá»¯ liá»‡u tháº­t lÃ  nhá» nháº¥t.

PhÆ°Æ¡ng trÃ¬nh tá»•ng quÃ¡t:

$$
\hat{y} = w_1x_1 + w_2x_2 + \dots + w_nx_n + b = \mathbf{X}\boldsymbol{\theta}
$$

Trong Ä‘Ã³:

- $\mathbf{X} \in \mathbb{R}^{m \times n}$: ma tráº­n dá»¯ liá»‡u Ä‘áº§u vÃ o gá»“m **m máº«u** vÃ  **n Ä‘áº·c trÆ°ng**  
  (má»—i hÃ ng lÃ  1 máº«u dá»¯ liá»‡u, má»—i cá»™t lÃ  1 Ä‘áº·c trÆ°ng)  
- $\boldsymbol{\theta} = [w_1, w_2, ..., w_n, b]^T$: vector trá»ng sá»‘ vÃ  bias  
- $\hat{y}$: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n Ä‘áº§u ra  
- $y$: giÃ¡ trá»‹ thá»±c táº¿ cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n  
- $b$: há»‡ sá»‘ bias giÃºp Ä‘iá»u chá»‰nh vá»‹ trÃ­ Ä‘Æ°á»ng há»“i quy  

---

Äá»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh, ta dÃ¹ng **Mean Squared Error (MSE)**:

$$
L(\hat{y}, y) = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2
$$

Trong Ä‘Ã³:

- $y_i$: giÃ¡ trá»‹ tháº­t  
- $\hat{y}_i$: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n  
- $m$: sá»‘ lÆ°á»£ng máº«u  

---

Má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh **Linear Regression** lÃ  **tÃ¬m ra cÃ¡c tham sá»‘ tá»‘i Æ°u** â€” cá»¥ thá»ƒ lÃ  **vector trá»ng sá»‘** $\mathbf{w}$ vÃ  **há»‡ sá»‘ bias** $b$ â€” sao cho giÃ¡ trá»‹ cá»§a $L_{\text{MSE}}$ lÃ  **nhá» nháº¥t cÃ³ thá»ƒ**.

![Loss Function](/assets/module6-week1/loss%20function.png)
- Má»—i cáº·p $(\hat{y}^{(i)}, y^{(i)})$ thá»ƒ hiá»‡n **má»©c Ä‘á»™ sai lá»‡ch** giá»¯a dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ thá»±c.  
- BÃ¬nh phÆ°Æ¡ng sai sá»‘ $(\hat{y}^{(i)} - y^{(i)})^2$ giÃºp **loáº¡i bá» dáº¥u Ã¢m** vÃ  **pháº¡t máº¡nh hÆ¡n cÃ¡c dá»± Ä‘oÃ¡n sai lá»‡ch lá»›n**.  
- Sau khi láº¥y trung bÃ¬nh trÃªn táº¥t cáº£ cÃ¡c máº«u, ta thu Ä‘Æ°á»£c **má»©c sai sá»‘ trung bÃ¬nh toÃ n bá»™ mÃ´ hÃ¬nh**.  
- Báº±ng cÃ¡ch Ä‘iá»u chá»‰nh $\mathbf{w}$ vÃ  $b$, mÃ´ hÃ¬nh â€œxoayâ€ hoáº·c â€œdá»‹ch chuyá»ƒnâ€ Ä‘Æ°á»ng há»“i quy Ä‘á»ƒ **lÃ m giáº£m sai sá»‘ tá»•ng thá»ƒ** nÃ y.\

---

**Quy trÃ¬nh tá»•ng quÃ¡t (Pipeline) cá»§a Linear Regression**

Sau khi hiá»ƒu cÃ¡ch mÃ´ hÃ¬nh há»c Ä‘á»ƒ giáº£m sai sá»‘ (loss), ta cÃ³ thá»ƒ khÃ¡i quÃ¡t quy trÃ¬nh huáº¥n luyá»‡n Logistic Regression qua cÃ¡c bÆ°á»›c sau:

![Pipeline Diagram](/assets/module6-week1/pipeline.png)

---

âš ï¸ Háº¡n cháº¿ cá»§a Linear Regression trong bÃ i toÃ¡n phÃ¢n loáº¡i

Máº·c dÃ¹ Linear Regression lÃ  mÃ´ hÃ¬nh ná»n táº£ng, dá»… hiá»ƒu vÃ  hiá»‡u quáº£ trong cÃ¡c bÃ i toÃ¡n **dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c**,  
nhÆ°ng khi Ã¡p dá»¥ng cho **bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n (0/1)**, nÃ³ bá»™c lá»™ má»™t sá»‘ **nhÆ°á»£c Ä‘iá»ƒm nghiÃªm trá»ng**:

**KhÃ´ng giá»›i háº¡n Ä‘áº§u ra trong [0, 1]**
- MÃ´ hÃ¬nh tuyáº¿n tÃ­nh cÃ³ dáº¡ng $\hat{y} = \mathbf{w}^T\mathbf{x} + b$.  
- Äáº§u ra $\hat{y}$ cÃ³ thá»ƒ nháº­n **báº¥t ká»³ giÃ¡ trá»‹ nÃ o** trong khoáº£ng $(-\infty, +\infty)$.  
> Äiá»u nÃ y **khÃ´ng thá»ƒ diá»…n giáº£i nhÆ° má»™t xÃ¡c suáº¥t**, vÃ  khiáº¿n viá»‡c **Ä‘áº·t ngÆ°á»¡ng phÃ¢n loáº¡i (threshold)** trá»Ÿ nÃªn tÃ¹y tiá»‡n.

**MÃ´ hÃ¬nh tuyáº¿n tÃ­nh khÃ´ng phÃ¹ há»£p vá»›i ranh giá»›i phÃ¢n loáº¡i phi tuyáº¿n**
- Trong nhiá»u bÃ i toÃ¡n thá»±c táº¿, dá»¯ liá»‡u cá»§a hai lá»›p (0 vÃ  1) **khÃ´ng thá»ƒ phÃ¢n tÃ¡ch báº±ng má»™t Ä‘Æ°á»ng tháº³ng**.  
- Linear Regression chá»‰ há»c Ä‘Æ°á»£c má»‘i quan há»‡ tuyáº¿n tÃ­nh giá»¯a Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra, nÃªn **khÃ´ng thá»ƒ mÃ´ hÃ¬nh hÃ³a ranh giá»›i phi tuyáº¿n**.

**HÃ m máº¥t mÃ¡t (MSE) khÃ´ng hiá»‡u quáº£ cho phÃ¢n loáº¡i**
- Linear Regression sá»­ dá»¥ng **Mean Squared Error (MSE)**, vá»‘n Ä‘Æ°á»£c thiáº¿t káº¿ cho bÃ i toÃ¡n há»“i quy.  
- Trong phÃ¢n loáº¡i, hÃ m nÃ y:
  - KhÃ´ng pháº£n Ã¡nh tá»‘t **má»©c Ä‘á»™ tin cáº­y** cá»§a dá»± Ä‘oÃ¡n xÃ¡c suáº¥t.  
  - Khi káº¿t há»£p vá»›i hÃ m sigmoid, MSE khiáº¿n **gradient há»™i tá»¥ cháº­m** vÃ  **dá»… máº¯c káº¹t táº¡i cá»±c trá»‹ cá»¥c bá»™**.

**KhÃ´ng Ä‘áº£m báº£o mÃ´ hÃ¬nh há»™i tá»¥ á»•n Ä‘á»‹nh**
- VÃ¬ Ä‘áº§u ra $\hat{y}$ khÃ´ng bá»‹ giá»›i háº¡n, mÃ´ hÃ¬nh cÃ³ thá»ƒ **tÄƒng/giáº£m vÃ´ háº¡n** Ä‘á»ƒ cá»‘ gáº¯ng giáº£m MSE.  
- Káº¿t quáº£ lÃ :
  - Gradient descent **dao Ä‘á»™ng hoáº·c diverge** (khÃ´ng há»™i tá»¥).  
  - MÃ´ hÃ¬nh **quÃ¡ nháº¡y cáº£m vá»›i nhiá»…u hoáº·c ngoáº¡i lá»‡ (outliers)**.

ğŸ‘‰ Giáº£i phÃ¡p lÃ  **Logistic Regression**, má»™t mÃ´ hÃ¬nh má»Ÿ rá»™ng Linear Regression vá»›i **hÃ m kÃ­ch hoáº¡t sigmoid** Ä‘á»ƒ Ã¡nh xáº¡ Ä‘áº§u ra vá» khoáº£ng (0, 1) â€” giÃºp mÃ´ hÃ¬nh **dá»± Ä‘oÃ¡n xÃ¡c suáº¥t chÃ­nh xÃ¡c hÆ¡n**.

---

## ğŸ”€ Pháº§n 2 â€“ Logistic Regression

**1. KhÃ¡i niá»‡m cÆ¡ báº£n**

**Logistic Regression** lÃ  má»™t mÃ´ hÃ¬nh **há»c cÃ³ giÃ¡m sÃ¡t (supervised learning)** dÃ¹ng cho **bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n (binary classification)** 

âš–ï¸ So sÃ¡nh nhanh:

| MÃ´ hÃ¬nh              | Dáº¡ng Ä‘áº§u ra                     | VÃ­ dá»¥                          |
|----------------------|----------------------------------|---------------------------------|
| **Linear Regression**  | LiÃªn tá»¥c (*continuous*)          | Dá»± Ä‘oÃ¡n nhiá»‡t Ä‘á»™, giÃ¡ nhÃ        |
| **Logistic Regression**| Rá»i ráº¡c (*discrete*, 0 hoáº·c 1)   | Dá»± Ä‘oÃ¡n cÃ³ bá»‡nh hay khÃ´ng       |


**2. Báº¯t Ä‘áº§u tá»« mÃ´ hÃ¬nh tuyáº¿n tÃ­nh**

TÆ°Æ¡ng tá»± nhÆ° **Linear Regression**, **Logistic Regression** cÅ©ng báº¯t Ä‘áº§u vá»›i **má»™t mÃ´ hÃ¬nh tuyáº¿n tÃ­nh** Ä‘á»ƒ káº¿t há»£p cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o:

$$
z = wx + b
$$

Trong Ä‘Ã³:

- $x$: giÃ¡ trá»‹ Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o (*feature*)  
- $w$: trá»ng sá»‘ (*weight*) mÃ  mÃ´ hÃ¬nh cáº§n há»c  
- $b$: há»‡ sá»‘ chá»‡ch (*bias*)  
- $z$: giÃ¡ trá»‹ tuyáº¿n tÃ­nh (*logit*)

á» bÆ°á»›c nÃ y, $z$ cÃ³ thá»ƒ nháº­n má»i giÃ¡ trá»‹ thá»±c tá»« $-\infty$ Ä‘áº¿n $+\infty$.  
Tuy nhiÃªn, má»¥c tiÃªu cá»§a ta lÃ  **dá»± Ä‘oÃ¡n xÃ¡c suáº¥t** má»™t máº«u thuá»™c lá»›p 1 (vÃ­ dá»¥: â€œcÃ³ bá»‡nhâ€, â€œÄ‘á»—â€, â€œspamâ€â€¦), nÃªn Ä‘áº§u ra cáº§n náº±m trong khoáº£ng **(0, 1)**.

ğŸ‘‰ Äá»ƒ lÃ m Ä‘Æ°á»£c Ä‘iá»u Ä‘Ã³, ta **khÃ´ng dÃ¹ng trá»±c tiáº¿p** $z$ lÃ m káº¿t quáº£ dá»± Ä‘oÃ¡n mÃ  cáº§n má»™t **hÃ m chuyá»ƒn Ä‘á»•i** phÃ¹ há»£p.

---

**3. HÃ m sigmoid**

Náº¿u ta dÃ¹ng luÃ´n cÃ´ng thá»©c  

$$
\hat{y} = z = wx + b
$$

Ä‘á»ƒ dá»± Ä‘oÃ¡n cho bÃ i toÃ¡n **phÃ¢n loáº¡i** (vÃ­ dá»¥ â€œcÃ³ bá»‡nhâ€ hay â€œkhÃ´ng bá»‡nhâ€), ta gáº·p ngay váº¥n Ä‘á»:

- $z$ cÃ³ thá»ƒ nháº­n **báº¥t ká»³ giÃ¡ trá»‹ nÃ o** tá»« $-\infty$ Ä‘áº¿n $+\infty$.  
- NhÆ°ng **xÃ¡c suáº¥t thá»±c táº¿** pháº£i náº±m trong **[0, 1]**.

ChÃ­nh vÃ¬ tháº¿, hÃ m **sigmoid** (hay *logistic function*) xuáº¥t hiá»‡n Ä‘á»ƒ biáº¿n Ä‘á»•i giÃ¡ trá»‹ tuyáº¿n tÃ­nh $z$ thÃ nh má»™t **xÃ¡c suáº¥t há»£p lá»‡**:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

![Sigmoid Function](/assets/module6-week1/sigmoid_function.png)

**ğŸ§  Giáº£i thÃ­ch:**

- Khi $z \to +\infty \Rightarrow e^{-z} \to 0 \Rightarrow \sigma(z) \to 1$  
- Khi $z = 0 \Rightarrow \sigma(0) = \frac{1}{2} = 0.5$  
- Khi $z \to -\infty \Rightarrow e^{-z} \to +\infty \Rightarrow \sigma(z) \to 0$

VÃ¬ váº­y, **sigmoid nÃ©n toÃ n bá»™ giÃ¡ trá»‹ thá»±c cá»§a $z$ vÃ o khoáº£ng (0, 1)** â€” chÃ­nh xÃ¡c lÃ  miá»n giÃ¡ trá»‹ cáº§n thiáº¿t Ä‘á»ƒ mÃ´ táº£ xÃ¡c suáº¥t.

---

## ğŸ¯ Pháº§n 3 â€“ HÃ m máº¥t mÃ¡t: MSE hay BCE?

**1. Thá»­ dÃ¹ng MSE (Mean Squared Error)**

ÄÃ¢y lÃ  **hÃ m máº¥t mÃ¡t quen thuá»™c** cá»§a há»“i quy tuyáº¿n tÃ­nh:

$$
L_{MSE} = (\hat{y} - y)^2
$$

Khi Ã¡p dá»¥ng cho **Logistic Regression**:

$$
\hat{y} = \sigma(wx + b) = \frac{1}{1 + e^{-(wx + b)}}
$$

â†’ Ta cÃ³ thá»ƒ tÃ­nh Ä‘Æ°á»£c Ä‘áº¡o hÃ m vÃ  cáº­p nháº­t tham sá»‘ báº±ng **Gradient Descent**.

**âš ï¸ Tuy nhiÃªn, MSE *khÃ´ng phÃ¹ há»£p* cho Logistic Regression vÃ¬:**

**HÃ m sigmoid phi tuyáº¿n â†’ Ä‘á»™ dá»‘c phá»©c táº¡p**

- Káº¿t há»£p **sigmoid + MSE** lÃ m Ä‘á»“ thá»‹ hÃ m máº¥t mÃ¡t trá»Ÿ nÃªn **khÃ´ng lá»“i (non-convex)**.  
- **Gradient** cÃ³ thá»ƒ bá»‹ ráº¥t nhá» (*hiá»‡n tÆ°á»£ng vanishing gradient*) khi $\hat{y}$ tiáº¿n gáº§n 0 hoáº·c 1.  
  â†’ MÃ´ hÃ¬nh há»c **ráº¥t cháº­m**, dá»… **máº¯c káº¹t á»Ÿ cá»±c tiá»ƒu cá»¥c bá»™**.

![MSE + Sigmoid](/assets/module6-week1/MSE+Sigmoid.png)

ğŸ‘‰ ChÃ­nh vÃ¬ sá»± nguyÃªn hiá»ƒm nÃ y, má»™t hÃ m máº¥t mÃ¡t Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t káº¿ riÃªng cho bÃ i toÃ¡n phÃ¢n loáº¡i xÃ¡c suáº¥t, Ä‘Ã³ lÃ  Binary Cross-Entropy (BCE), hay cÃ²n gá»i lÃ  Log Loss.

![E + Sigmoid](/assets/module6-week1/BCE+Sigmoid.png)

---

**2. BCE (Binary Cross-Entropy)**

**a) CÃ´ng thá»©c tá»•ng quÃ¡t**

$$
L_{BCE}(\hat{y}, y) = - [ y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) ]
$$

---

**b) Giáº£i thÃ­ch chi tiáº¿t cÃ¡c thÃ nh pháº§n trong cÃ´ng thá»©c**

| KÃ½ hiá»‡u | Ã nghÄ©a | Vai trÃ² |
|----------|----------|----------|
| $y$ | **GiÃ¡ trá»‹ tháº­t (label)** cá»§a máº«u dá»¯ liá»‡u, chá»‰ nháº­n hai giÃ¡ trá»‹: 0 hoáº·c 1. | Cho biáº¿t máº«u thuá»™c lá»›p nÃ o (lá»›p dÆ°Æ¡ng = 1, lá»›p Ã¢m = 0). |
| $\hat{y}$ | **XÃ¡c suáº¥t dá»± Ä‘oÃ¡n** mÃ  mÃ´ hÃ¬nh logistic regression sinh ra, Ä‘Æ°á»£c tÃ­nh báº±ng hÃ m sigmoid:  $\hat{y} = \frac{1}{1 + e^{-(wx + b)}}$ | Biá»ƒu diá»…n Ä‘á»™ tin cáº­y cá»§a mÃ´ hÃ¬nh ráº±ng máº«u nÃ y thuá»™c lá»›p 1. |
| $\log(\hat{y})$ | Logarithm tá»± nhiÃªn cá»§a xÃ¡c suáº¥t dá»± Ä‘oÃ¡n lá»›p 1 | DÃ¹ng Ä‘á»ƒ Ä‘o â€œ**má»©c Ä‘á»™ báº¥t ngá»**â€ (*information content*) khi mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n Ä‘Ãºng lá»›p 1. |
| $\log(1 - \hat{y})$ | Logarithm tá»± nhiÃªn cá»§a xÃ¡c suáº¥t dá»± Ä‘oÃ¡n lá»›p 0 | Äo â€œ**má»©c Ä‘á»™ báº¥t ngá»**â€ khi mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n Ä‘Ãºng lá»›p 0. |
| Dáº¥u trá»« (-) bÃªn ngoÃ i | Äáº£o ngÆ°á»£c dáº¥u vÃ¬ log-likelihood cÃ³ giÃ¡ trá»‹ Ã¢m | GiÃºp hÃ m loss trá»Ÿ thÃ nh giÃ¡ trá»‹ dÆ°Æ¡ng cáº§n **minimize**. |

---

**c) Ã nghÄ©a trá»±c quan**

CÃ´ng thá»©c trÃªn cÃ³ thá»ƒ hiá»ƒu lÃ :

- **Náº¿u máº«u tháº­t $y = 1$**, cÃ²n láº¡i:

  $$
  L = -\log(\hat{y})
  $$

  â†’ MÃ´ hÃ¬nh sáº½ bá»‹ **pháº¡t máº¡nh** khi $\hat{y}$ nhá» (vÃ¬ $\log(\hat{y})$ ráº¥t Ã¢m khi $\hat{y}$ gáº§n 0).  
  â†’ Äiá»u nÃ y **khuyáº¿n khÃ­ch mÃ´ hÃ¬nh Ä‘áº©y $\hat{y}$ gáº§n 1** cho máº«u thuá»™c lá»›p 1.

- **Náº¿u máº«u tháº­t $y = 0$**, cÃ²n láº¡i:

  $$
  L = -\log(1 - \hat{y})
  $$

  â†’ MÃ´ hÃ¬nh bá»‹ **pháº¡t máº¡nh** khi $\hat{y}$ lá»›n (tá»©c dá»± Ä‘oÃ¡n lá»›p 1 sai).  
  â†’ Khuyáº¿n khÃ­ch mÃ´ hÃ¬nh **Ä‘áº©y $\hat{y}$ gáº§n 0** cho máº«u thuá»™c lá»›p 0.

![BCE](/assets/module6-week1/BCE.png)

---

## ğŸ› ï¸ Pháº§n 4 â€“ Há»c tham sá»‘ báº±ng Gradient Descent

Vá»›i 1 máº«u $(\mathbf{x}, y)$, Ä‘áº¡o hÃ m theo $\mathbf{w}$ cá»§a BCE:
$$
\nabla_{\mathbf{w}} L = (\hat{p}-y)\,\mathbf{x},\qquad
\frac{\partial L}{\partial b} = \hat{p}-y
$$
Cáº­p nháº­t:
$$
\mathbf{w} \leftarrow \mathbf{w} - \eta \, (\hat{p}-y)\mathbf{x},\qquad
b \leftarrow b - \eta \, (\hat{p}-y)
$$
($\eta$: learning rate)

---

## ğŸ§­ Pháº§n 5 â€“ Decision Boundary & Prob. View

- BiÃªn quyáº¿t Ä‘á»‹nh khi $\hat{p}=0.5 \iff \mathbf{w}^\top \mathbf{x}+b=0$ â‡’ lÃ  **siÃªu pháº³ng tuyáº¿n tÃ­nh**.
- NhÃ¬n theo **log-odds**:
$$
\log\frac{\hat{p}}{1-\hat{p}} = \mathbf{w}^\top \mathbf{x} + b
$$
â†’ Logistic Regression chÃ­nh lÃ  **há»“i quy tuyáº¿n tÃ­nh trÃªn log-odds**.

---

## ğŸ“š TÃ i nguyÃªn
- [Coding cho bÃ i giáº£ng cá»§a tháº§y Quang-Vinh Dinh (Google Drive)](https://â€¦)  
- [Wikipedia â€“ Logistic Function](https://en.wikipedia.org/wiki/Logistic_function)

---

## ğŸ§© Mini Quiz (vui há»c ğŸ²)  
1ï¸âƒ£ Logistic Regression dá»± Ä‘oÃ¡n **cÃ¡i gÃ¬**? â†’ **XÃ¡c suáº¥t** $P(y=1\mid \mathbf{x})$ trong $[0,1]$.  

2ï¸âƒ£ VÃ¬ sao chá»n **BCE** thay vÃ¬ **MSE**? â†’ PhÃ¹ há»£p xÃ¡c suáº¥t + gradient â€œkhá»eâ€ hÆ¡n.  

3ï¸âƒ£ Khi nÃ o $\hat{y}=1$? â†’ Khi $\sigma(\mathbf{w}^\top\mathbf{x}+b)\ge0.5$ (tÆ°Æ¡ng Ä‘Æ°Æ¡ng $\mathbf{w}^\top\mathbf{x}+b\ge0$). 

4ï¸âƒ£ Sigmoid function Ä‘áº£m báº£o Ä‘iá»u gÃ¬?  
   â˜‘ Äáº§u ra luÃ´n trong [0,1]  
   â˜ Tá»‘c Ä‘á»™ há»c nhanh hÆ¡n  

---

ğŸ’¡ *Háº¹n gáº·p á»Ÿ bÃ i giáº£ng tiáº¿p theo cá»§a tuáº§n 1 â€“ Khi chÃºng ta dive sÃ¢u hÆ¡n vÃ o Multi-feature Logistic Regression nhÃ©!* ğŸš€

{% include mathjax.html %}  

---

[ğŸ  Vá» trang chá»§]({{ '/' | relative_url }})  



