## ğŸ“ˆ Pháº§n 1 â€“ Ã”n láº¡i Linear Regression

**Linear Regression (Há»“i quy tuyáº¿n tÃ­nh)** lÃ  má»™t mÃ´ hÃ¬nh dÃ¹ng Ä‘á»ƒ **dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c** dá»±a trÃªn cÃ¡c biáº¿n Ä‘áº§u vÃ o (*features*).  
Má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh lÃ  tÃ¬m má»™t Ä‘Æ°á»ng tháº³ng (hoáº·c siÃªu pháº³ng trong khÃ´ng gian nhiá»u chiá»u) sao cho tá»•ng sai sá»‘ giá»¯a dá»± Ä‘oÃ¡n vÃ  dá»¯ liá»‡u tháº­t lÃ  nhá» nháº¥t.

PhÆ°Æ¡ng trÃ¬nh tá»•ng quÃ¡t:

$$
\hat{y} = w_1x_1 + w_2x_2 + \dots + w_nx_n + b = \mathbf{X}\boldsymbol{\theta}
$$

Trong Ä‘Ã³:

- $\mathbf{X} \in \mathbb{R}^{m \times n}$: ma tráº­n dá»¯ liá»‡u Ä‘áº§u vÃ o gá»“m **m máº«u** vÃ  **n Ä‘áº·c trÆ°ng**  
  (má»—i hÃ ng lÃ  1 máº«u dá»¯ liá»‡u, má»—i cá»™t lÃ  1 Ä‘áº·c trÆ°ng)  
- $\boldsymbol{\theta} = [w_1, w_2, ..., w_n, b]^T$: vector trá»ng sá»‘ vÃ  bias  
- $\hat{y}$: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n Ä‘áº§u ra  
- $y$: giÃ¡ trá»‹ thá»±c táº¿ cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n  
- $b$: há»‡ sá»‘ bias giÃºp Ä‘iá»u chá»‰nh vá»‹ trÃ­ Ä‘Æ°á»ng há»“i quy  

---

Äá»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh, ta dÃ¹ng **Mean Squared Error (MSE)**:

$$
L(\hat{y}, y) = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2
$$

Trong Ä‘Ã³:

- $y_i$: giÃ¡ trá»‹ tháº­t  
- $\hat{y}_i$: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n  
- $m$: sá»‘ lÆ°á»£ng máº«u  

---

Má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh **Linear Regression** lÃ  **tÃ¬m ra cÃ¡c tham sá»‘ tá»‘i Æ°u** â€” cá»¥ thá»ƒ lÃ  **vector trá»ng sá»‘** $\mathbf{w}$ vÃ  **há»‡ sá»‘ bias** $b$ â€” sao cho giÃ¡ trá»‹ cá»§a $L_{\text{MSE}}$ lÃ  **nhá» nháº¥t cÃ³ thá»ƒ**.

![Loss Function](/assets/module6-week1/loss%20function.png)
- Má»—i cáº·p $(\hat{y}^{(i)}, y^{(i)})$ thá»ƒ hiá»‡n **má»©c Ä‘á»™ sai lá»‡ch** giá»¯a dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ thá»±c.  
- BÃ¬nh phÆ°Æ¡ng sai sá»‘ $(\hat{y}^{(i)} - y^{(i)})^2$ giÃºp **loáº¡i bá» dáº¥u Ã¢m** vÃ  **pháº¡t máº¡nh hÆ¡n cÃ¡c dá»± Ä‘oÃ¡n sai lá»‡ch lá»›n**.  
- Sau khi láº¥y trung bÃ¬nh trÃªn táº¥t cáº£ cÃ¡c máº«u, ta thu Ä‘Æ°á»£c **má»©c sai sá»‘ trung bÃ¬nh toÃ n bá»™ mÃ´ hÃ¬nh**.  
- Báº±ng cÃ¡ch Ä‘iá»u chá»‰nh $\mathbf{w}$ vÃ  $b$, mÃ´ hÃ¬nh â€œxoayâ€ hoáº·c â€œdá»‹ch chuyá»ƒnâ€ Ä‘Æ°á»ng há»“i quy Ä‘á»ƒ **lÃ m giáº£m sai sá»‘ tá»•ng thá»ƒ** nÃ y.\

---

**Quy trÃ¬nh tá»•ng quÃ¡t (Pipeline) cá»§a Linear Regression**

Sau khi hiá»ƒu cÃ¡ch mÃ´ hÃ¬nh há»c Ä‘á»ƒ giáº£m sai sá»‘ (loss), ta cÃ³ thá»ƒ khÃ¡i quÃ¡t quy trÃ¬nh huáº¥n luyá»‡n Logistic Regression qua cÃ¡c bÆ°á»›c sau:

![Pipeline Diagram](/assets/module6-week1/pipeline.png)

---

**âš ï¸ Háº¡n cháº¿ cá»§a Linear Regression trong bÃ i toÃ¡n phÃ¢n loáº¡i**

Máº·c dÃ¹ Linear Regression lÃ  mÃ´ hÃ¬nh ná»n táº£ng, dá»… hiá»ƒu vÃ  hiá»‡u quáº£ trong cÃ¡c bÃ i toÃ¡n **dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c**,  
nhÆ°ng khi Ã¡p dá»¥ng cho **bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n (0/1)**, nÃ³ bá»™c lá»™ má»™t sá»‘ **nhÆ°á»£c Ä‘iá»ƒm nghiÃªm trá»ng**:

**KhÃ´ng giá»›i háº¡n Ä‘áº§u ra trong [0, 1]**
- MÃ´ hÃ¬nh tuyáº¿n tÃ­nh cÃ³ dáº¡ng $\hat{y} = \mathbf{w}^T\mathbf{x} + b$.  
- Äáº§u ra $\hat{y}$ cÃ³ thá»ƒ nháº­n **báº¥t ká»³ giÃ¡ trá»‹ nÃ o** trong khoáº£ng $(-\infty, +\infty)$.  
> Äiá»u nÃ y **khÃ´ng thá»ƒ diá»…n giáº£i nhÆ° má»™t xÃ¡c suáº¥t**, vÃ  khiáº¿n viá»‡c **Ä‘áº·t ngÆ°á»¡ng phÃ¢n loáº¡i (threshold)** trá»Ÿ nÃªn tÃ¹y tiá»‡n.

**MÃ´ hÃ¬nh tuyáº¿n tÃ­nh khÃ´ng phÃ¹ há»£p vá»›i ranh giá»›i phÃ¢n loáº¡i phi tuyáº¿n**
- Trong nhiá»u bÃ i toÃ¡n thá»±c táº¿, dá»¯ liá»‡u cá»§a hai lá»›p (0 vÃ  1) **khÃ´ng thá»ƒ phÃ¢n tÃ¡ch báº±ng má»™t Ä‘Æ°á»ng tháº³ng**.  
- Linear Regression chá»‰ há»c Ä‘Æ°á»£c má»‘i quan há»‡ tuyáº¿n tÃ­nh giá»¯a Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra, nÃªn **khÃ´ng thá»ƒ mÃ´ hÃ¬nh hÃ³a ranh giá»›i phi tuyáº¿n**.

**HÃ m máº¥t mÃ¡t (MSE) khÃ´ng hiá»‡u quáº£ cho phÃ¢n loáº¡i**
- Linear Regression sá»­ dá»¥ng **Mean Squared Error (MSE)**, vá»‘n Ä‘Æ°á»£c thiáº¿t káº¿ cho bÃ i toÃ¡n há»“i quy.  
- Trong phÃ¢n loáº¡i, hÃ m nÃ y:
  - KhÃ´ng pháº£n Ã¡nh tá»‘t **má»©c Ä‘á»™ tin cáº­y** cá»§a dá»± Ä‘oÃ¡n xÃ¡c suáº¥t.  
  - Khi káº¿t há»£p vá»›i hÃ m sigmoid, MSE khiáº¿n **gradient há»™i tá»¥ cháº­m** vÃ  **dá»… máº¯c káº¹t táº¡i cá»±c trá»‹ cá»¥c bá»™**.

**KhÃ´ng Ä‘áº£m báº£o mÃ´ hÃ¬nh há»™i tá»¥ á»•n Ä‘á»‹nh**
- VÃ¬ Ä‘áº§u ra $\hat{y}$ khÃ´ng bá»‹ giá»›i háº¡n, mÃ´ hÃ¬nh cÃ³ thá»ƒ **tÄƒng/giáº£m vÃ´ háº¡n** Ä‘á»ƒ cá»‘ gáº¯ng giáº£m MSE.  
- Káº¿t quáº£ lÃ :
  - Gradient descent **dao Ä‘á»™ng hoáº·c diverge** (khÃ´ng há»™i tá»¥).  
  - MÃ´ hÃ¬nh **quÃ¡ nháº¡y cáº£m vá»›i nhiá»…u hoáº·c ngoáº¡i lá»‡ (outliers)**.

ğŸ‘‰ Giáº£i phÃ¡p lÃ  **Logistic Regression**, má»™t mÃ´ hÃ¬nh má»Ÿ rá»™ng Linear Regression vá»›i **hÃ m kÃ­ch hoáº¡t sigmoid** Ä‘á»ƒ Ã¡nh xáº¡ Ä‘áº§u ra vá» khoáº£ng (0, 1) â€” giÃºp mÃ´ hÃ¬nh **dá»± Ä‘oÃ¡n xÃ¡c suáº¥t chÃ­nh xÃ¡c hÆ¡n**.


## ğŸ”€ Pháº§n 2 â€“ Logistic Regression

**1. KhÃ¡i niá»‡m cÆ¡ báº£n**

**Logistic Regression** lÃ  má»™t mÃ´ hÃ¬nh **há»c cÃ³ giÃ¡m sÃ¡t (supervised learning)** dÃ¹ng cho **bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n (binary classification)** 

âš–ï¸ So sÃ¡nh nhanh:

| MÃ´ hÃ¬nh              | Dáº¡ng Ä‘áº§u ra                     | VÃ­ dá»¥                          |
|----------------------|----------------------------------|---------------------------------|
| **Linear Regression**  | LiÃªn tá»¥c (*continuous*)          | Dá»± Ä‘oÃ¡n nhiá»‡t Ä‘á»™, giÃ¡ nhÃ        |
| **Logistic Regression**| Rá»i ráº¡c (*discrete*, 0 hoáº·c 1)   | Dá»± Ä‘oÃ¡n cÃ³ bá»‡nh hay khÃ´ng       |


**2. Báº¯t Ä‘áº§u tá»« mÃ´ hÃ¬nh tuyáº¿n tÃ­nh**

TÆ°Æ¡ng tá»± nhÆ° **Linear Regression**, **Logistic Regression** cÅ©ng báº¯t Ä‘áº§u vá»›i **má»™t mÃ´ hÃ¬nh tuyáº¿n tÃ­nh** Ä‘á»ƒ káº¿t há»£p cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o:

$$
z = wx + b
$$

Trong Ä‘Ã³:

- $x$: giÃ¡ trá»‹ Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o (*feature*)  
- $w$: trá»ng sá»‘ (*weight*) mÃ  mÃ´ hÃ¬nh cáº§n há»c  
- $b$: há»‡ sá»‘ chá»‡ch (*bias*)  
- $z$: giÃ¡ trá»‹ tuyáº¿n tÃ­nh (*logit*)

á» bÆ°á»›c nÃ y, $z$ cÃ³ thá»ƒ nháº­n má»i giÃ¡ trá»‹ thá»±c tá»« $-\infty$ Ä‘áº¿n $+\infty$.  
Tuy nhiÃªn, má»¥c tiÃªu cá»§a ta lÃ  **dá»± Ä‘oÃ¡n xÃ¡c suáº¥t** má»™t máº«u thuá»™c lá»›p 1 (vÃ­ dá»¥: â€œcÃ³ bá»‡nhâ€, â€œÄ‘á»—â€, â€œspamâ€â€¦), nÃªn Ä‘áº§u ra cáº§n náº±m trong khoáº£ng **(0, 1)**.

ğŸ‘‰ Äá»ƒ lÃ m Ä‘Æ°á»£c Ä‘iá»u Ä‘Ã³, ta **khÃ´ng dÃ¹ng trá»±c tiáº¿p** $z$ lÃ m káº¿t quáº£ dá»± Ä‘oÃ¡n mÃ  cáº§n má»™t **hÃ m chuyá»ƒn Ä‘á»•i** phÃ¹ há»£p.

**3. HÃ m sigmoid**

Náº¿u ta dÃ¹ng luÃ´n cÃ´ng thá»©c  

$$
\hat{y} = z = wx + b
$$

Ä‘á»ƒ dá»± Ä‘oÃ¡n cho bÃ i toÃ¡n **phÃ¢n loáº¡i** (vÃ­ dá»¥ â€œcÃ³ bá»‡nhâ€ hay â€œkhÃ´ng bá»‡nhâ€), ta gáº·p ngay váº¥n Ä‘á»:

- $z$ cÃ³ thá»ƒ nháº­n **báº¥t ká»³ giÃ¡ trá»‹ nÃ o** tá»« $-\infty$ Ä‘áº¿n $+\infty$.  
- NhÆ°ng **xÃ¡c suáº¥t thá»±c táº¿** pháº£i náº±m trong **[0, 1]**.

ChÃ­nh vÃ¬ tháº¿, hÃ m **sigmoid** (hay *logistic function*) xuáº¥t hiá»‡n Ä‘á»ƒ biáº¿n Ä‘á»•i giÃ¡ trá»‹ tuyáº¿n tÃ­nh $z$ thÃ nh má»™t **xÃ¡c suáº¥t há»£p lá»‡**:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

![Sigmoid Function](/assets/module6-week1/sigmoid_function.png)

**ğŸ§  Giáº£i thÃ­ch:**

- Khi $z \to +\infty \Rightarrow e^{-z} \to 0 \Rightarrow \sigma(z) \to 1$  
- Khi $z = 0 \Rightarrow \sigma(0) = \frac{1}{2} = 0.5$  
- Khi $z \to -\infty \Rightarrow e^{-z} \to +\infty \Rightarrow \sigma(z) \to 0$

VÃ¬ váº­y, **sigmoid nÃ©n toÃ n bá»™ giÃ¡ trá»‹ thá»±c cá»§a $z$ vÃ o khoáº£ng (0, 1)** â€” chÃ­nh xÃ¡c lÃ  miá»n giÃ¡ trá»‹ cáº§n thiáº¿t Ä‘á»ƒ mÃ´ táº£ xÃ¡c suáº¥t.

## ğŸ¯ Pháº§n 3 â€“ HÃ m máº¥t mÃ¡t: MSE hay BCE?

**1. Thá»­ dÃ¹ng MSE (Mean Squared Error)**

ÄÃ¢y lÃ  **hÃ m máº¥t mÃ¡t quen thuá»™c** cá»§a há»“i quy tuyáº¿n tÃ­nh:

$$
L_{MSE} = (\hat{y} - y)^2
$$

Khi Ã¡p dá»¥ng cho **Logistic Regression**:

$$
\hat{y} = \sigma(wx + b) = \frac{1}{1 + e^{-(wx + b)}}
$$

â†’ Ta cÃ³ thá»ƒ tÃ­nh Ä‘Æ°á»£c Ä‘áº¡o hÃ m vÃ  cáº­p nháº­t tham sá»‘ báº±ng **Gradient Descent**.

**âš ï¸ Tuy nhiÃªn, MSE *khÃ´ng phÃ¹ há»£p* cho Logistic Regression vÃ¬:**

**HÃ m sigmoid phi tuyáº¿n â†’ Ä‘á»™ dá»‘c phá»©c táº¡p**

- Káº¿t há»£p **sigmoid + MSE** lÃ m Ä‘á»“ thá»‹ hÃ m máº¥t mÃ¡t trá»Ÿ nÃªn **khÃ´ng lá»“i (non-convex)**.  
- **Gradient** cÃ³ thá»ƒ bá»‹ ráº¥t nhá» (*hiá»‡n tÆ°á»£ng vanishing gradient*) khi $\hat{y}$ tiáº¿n gáº§n 0 hoáº·c 1.  
  â†’ MÃ´ hÃ¬nh há»c **ráº¥t cháº­m**, dá»… **máº¯c káº¹t á»Ÿ cá»±c tiá»ƒu cá»¥c bá»™**.

![MSE + Sigmoid](/assets/module6-week1/MSE+Sigmoid.png)

ğŸ‘‰ ChÃ­nh vÃ¬ sá»± nguyÃªn hiá»ƒm nÃ y, má»™t hÃ m máº¥t mÃ¡t Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t káº¿ riÃªng cho bÃ i toÃ¡n phÃ¢n loáº¡i xÃ¡c suáº¥t, Ä‘Ã³ lÃ  Binary Cross-Entropy (BCE), hay cÃ²n gá»i lÃ  Log Loss.

![E + Sigmoid](/assets/module6-week1/BCE+Sigmoid.png)


**2. BCE (Binary Cross-Entropy)**

**a) CÃ´ng thá»©c tá»•ng quÃ¡t**

$$
L_{BCE}(\hat{y}, y) = - [ y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) ]
$$

**b) Giáº£i thÃ­ch chi tiáº¿t cÃ¡c thÃ nh pháº§n trong cÃ´ng thá»©c**

| KÃ½ hiá»‡u | Ã nghÄ©a | Vai trÃ² |
|----------|----------|----------|
| $y$ | **GiÃ¡ trá»‹ tháº­t (label)** cá»§a máº«u dá»¯ liá»‡u, chá»‰ nháº­n hai giÃ¡ trá»‹: 0 hoáº·c 1. | Cho biáº¿t máº«u thuá»™c lá»›p nÃ o (lá»›p dÆ°Æ¡ng = 1, lá»›p Ã¢m = 0). |
| $\hat{y}$ | **XÃ¡c suáº¥t dá»± Ä‘oÃ¡n** mÃ  mÃ´ hÃ¬nh logistic regression sinh ra, Ä‘Æ°á»£c tÃ­nh báº±ng hÃ m sigmoid:  $\hat{y} = \frac{1}{1 + e^{-(wx + b)}}$ | Biá»ƒu diá»…n Ä‘á»™ tin cáº­y cá»§a mÃ´ hÃ¬nh ráº±ng máº«u nÃ y thuá»™c lá»›p 1. |
| $\log(\hat{y})$ | Logarithm tá»± nhiÃªn cá»§a xÃ¡c suáº¥t dá»± Ä‘oÃ¡n lá»›p 1 | DÃ¹ng Ä‘á»ƒ Ä‘o â€œ**má»©c Ä‘á»™ báº¥t ngá»**â€ (*information content*) khi mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n Ä‘Ãºng lá»›p 1. |
| $\log(1 - \hat{y})$ | Logarithm tá»± nhiÃªn cá»§a xÃ¡c suáº¥t dá»± Ä‘oÃ¡n lá»›p 0 | Äo â€œ**má»©c Ä‘á»™ báº¥t ngá»**â€ khi mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n Ä‘Ãºng lá»›p 0. |
| Dáº¥u trá»« (-) bÃªn ngoÃ i | Äáº£o ngÆ°á»£c dáº¥u vÃ¬ log-likelihood cÃ³ giÃ¡ trá»‹ Ã¢m | GiÃºp hÃ m loss trá»Ÿ thÃ nh giÃ¡ trá»‹ dÆ°Æ¡ng cáº§n **minimize**. |

**c) Ã nghÄ©a trá»±c quan**

CÃ´ng thá»©c trÃªn cÃ³ thá»ƒ hiá»ƒu lÃ :

- **Náº¿u máº«u tháº­t $y = 1$**, cÃ²n láº¡i:

    $$\displaystyle L = -\log(\hat{y})$$

    â†’ MÃ´ hÃ¬nh sáº½ bá»‹ **pháº¡t máº¡nh** khi $\hat{y}$ nhá» (vÃ¬ $\log(\hat{y})$ ráº¥t Ã¢m khi $\hat{y}$ gáº§n 0).  
    â†’ Äiá»u nÃ y **khuyáº¿n khÃ­ch mÃ´ hÃ¬nh Ä‘áº©y $\hat{y}$ gáº§n 1** cho máº«u thuá»™c lá»›p 1.

- **Náº¿u máº«u tháº­t $y = 0$**, cÃ²n láº¡i:

    $$\displaystyle L = -\log(1 - \hat{y})$$

    â†’ MÃ´ hÃ¬nh bá»‹ **pháº¡t máº¡nh** khi $\hat{y}$ lá»›n (tá»©c dá»± Ä‘oÃ¡n lá»›p 1 sai).  
    â†’ Khuyáº¿n khÃ­ch mÃ´ hÃ¬nh **Ä‘áº©y $\hat{y}$ gáº§n 0** cho máº«u thuá»™c lá»›p 0.


![BCE](/assets/module6-week1/BCE.png)

## ğŸ› ï¸ Pháº§n 4 â€“ Há»c tham sá»‘ báº±ng Gradient Descent

Vá»›i 1 máº«u $(\mathbf{x}, y)$, Ä‘áº¡o hÃ m theo $\mathbf{w}$ cá»§a BCE:
$$
\nabla_{\mathbf{w}} L = (\hat{p}-y)\,\mathbf{x},\qquad
\frac{\partial L}{\partial b} = \hat{p}-y
$$
Cáº­p nháº­t:
$$
\mathbf{w} \leftarrow \mathbf{w} - \eta \, (\hat{p}-y)\mathbf{x},\qquad
b \leftarrow b - \eta \, (\hat{p}-y)
$$
($\eta$: learning rate)

---

## ğŸ§­ Pháº§n 5 â€“ Decision Boundary & Prob. View

- BiÃªn quyáº¿t Ä‘á»‹nh khi $\hat{p}=0.5 \iff \mathbf{w}^\top \mathbf{x}+b=0$ â‡’ lÃ  **siÃªu pháº³ng tuyáº¿n tÃ­nh**.
- NhÃ¬n theo **log-odds**:
$$
\log\frac{\hat{p}}{1-\hat{p}} = \mathbf{w}^\top \mathbf{x} + b
$$
â†’ Logistic Regression chÃ­nh lÃ  **há»“i quy tuyáº¿n tÃ­nh trÃªn log-odds**.

---

## ğŸ“š TÃ i nguyÃªn
- [Coding cho bÃ i giáº£ng cá»§a tháº§y Quang-Vinh Dinh (Google Drive)](https://â€¦)  
- [Wikipedia â€“ Logistic Function](https://en.wikipedia.org/wiki/Logistic_function)

---

## ğŸ§© Mini Quiz (vui há»c ğŸ²)  
1ï¸âƒ£ Logistic Regression dá»± Ä‘oÃ¡n **cÃ¡i gÃ¬**? â†’ **XÃ¡c suáº¥t** $P(y=1\mid \mathbf{x})$ trong $[0,1]$.  

2ï¸âƒ£ VÃ¬ sao chá»n **BCE** thay vÃ¬ **MSE**? â†’ PhÃ¹ há»£p xÃ¡c suáº¥t + gradient â€œkhá»eâ€ hÆ¡n.  

3ï¸âƒ£ Khi nÃ o $\hat{y}=1$? â†’ Khi $\sigma(\mathbf{w}^\top\mathbf{x}+b)\ge0.5$ (tÆ°Æ¡ng Ä‘Æ°Æ¡ng $\mathbf{w}^\top\mathbf{x}+b\ge0$). 

4ï¸âƒ£ Sigmoid function Ä‘áº£m báº£o Ä‘iá»u gÃ¬?  
   â˜‘ Äáº§u ra luÃ´n trong [0,1]  
   â˜ Tá»‘c Ä‘á»™ há»c nhanh hÆ¡n  

---

ğŸ’¡ *Háº¹n gáº·p á»Ÿ bÃ i giáº£ng tiáº¿p theo cá»§a tuáº§n 1 â€“ Khi chÃºng ta dive sÃ¢u hÆ¡n vÃ o Multi-feature Logistic Regression nhÃ©!* ğŸš€


---

## ğŸ§  BÃ i giáº£ng ngÃ y 07/11/2025: Logistic Regression â€“ Vectorization & Application (TÃ³m táº¯t chi tiáº¿t 4 trang A4)

---

### 1ï¸âƒ£ Ã”n táº­p mÃ´ hÃ¬nh cÆ¡ báº£n

- Logistic Regression dá»± Ä‘oÃ¡n xÃ¡c suáº¥t \(P(y=1|\mathbf{x})\) báº±ng hÃ m sigmoid:
  \[
  \hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}, \quad z = \mathbf{w}^\top \mathbf{x} + b
  \]
- HÃ m máº¥t mÃ¡t (Binary Cross Entropy):
  \[
  L = -\frac{1}{m}\sum_{i=1}^m \big[y^{(i)}\log \hat{y}^{(i)} + (1-y^{(i)})\log(1-\hat{y}^{(i)})\big]
  \]
- Gradient:
  \[
  \nabla_{\mathbf{w}}L = \frac{1}{m}X^\top(\hat{\mathbf{y}}-\mathbf{y}), \quad
  \frac{\partial L}{\partial b} = \frac{1}{m}\sum_{i=1}^m (\hat{y}^{(i)}-y^{(i)})
  \]
- Cáº­p nháº­t tham sá»‘:
  \[
  \mathbf{w} \leftarrow \mathbf{w} - \eta\nabla_{\mathbf{w}}L,\quad
  b \leftarrow b - \eta\frac{\partial L}{\partial b}
  \]

---

### 2ï¸âƒ£ VÃ¬ sao chá»n Binary Cross Entropy thay vÃ¬ MSE

| So sÃ¡nh | MSE | BCE |
|----------|-----|-----|
| Äá»™ pháº¡t lá»—i | Nhá» khi sai Ã­t | Pháº¡t máº¡nh khi sai vÃ  tá»± tin |
| Gradient | Dá»… nhá», há»c cháº­m | Gradient lá»›n, há»™i tá»¥ nhanh |
| XÃ¡c suáº¥t | KhÃ´ng rÃ ng buá»™c [0,1] | LuÃ´n Ä‘Ãºng miá»n xÃ¡c suáº¥t |

> ğŸ’¡ **BCE** giÃºp mÃ´ hÃ¬nh há»c xÃ¡c suáº¥t chÃ­nh xÃ¡c hÆ¡n, Ä‘áº·c biá»‡t trong vÃ¹ng biÃªn (0/1).

---

### 3ï¸âƒ£ Vectorization (ma tráº­n hoÃ¡)

- Gom toÃ n bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n vÃ o ma tráº­n:
  \[
  \mathbf{Z} = X\mathbf{w} + b
  \]
  \[
  \hat{\mathbf{Y}} = \sigma(\mathbf{Z}) = \frac{1}{1+e^{-\mathbf{Z}}}
  \]
- Máº¥t mÃ¡t:
  \[
  L = -\frac{1}{m}\big[\mathbf{y}^\top\log \hat{\mathbf{y}} + (1-\mathbf{y})^\top\log(1-\hat{\mathbf{y}})\big]
  \]
- Gradient:
  \[
  \nabla_{\mathbf{w}}L = \frac{1}{m}X^\top(\hat{\mathbf{y}}-\mathbf{y}), \quad
  \nabla_b L = \frac{1}{m}\sum(\hat{\mathbf{y}}-\mathbf{y})
  \]

> ğŸ¯ **Vectorization** giÃºp loáº¡i bá» vÃ²ng láº·p, cháº¡y nhanh trÃªn GPU, code ngáº¯n gá»n & dá»… má»Ÿ rá»™ng sang máº¡ng nÆ¡-ron.

---

### 4ï¸âƒ£ CÃ¡c biáº¿n thá»ƒ Gradient Descent

| Loáº¡i | Cáº­p nháº­t sau | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|------|---------------|----------|-------------|
| **Batch GD** | ToÃ n bá»™ dá»¯ liá»‡u | á»”n Ä‘á»‹nh, chÃ­nh xÃ¡c | Cháº­m, tá»‘n bá»™ nhá»› |
| **Stochastic GD (SGD)** | 1 máº«u/láº§n | Nhanh, thoÃ¡t local minima | Dao Ä‘á»™ng, kÃ©m á»•n Ä‘á»‹nh |
| **Mini-Batch GD** | NhÃ³m nhá» (32â€“256) | CÃ¢n báº±ng tá»‘c Ä‘á»™ & á»•n Ä‘á»‹nh | Cáº§n chá»n batch-size phÃ¹ há»£p |

> ğŸ± Thá»±c táº¿ deep learning luÃ´n dÃ¹ng **mini-batch** Ä‘á»ƒ Ä‘áº¡t hiá»‡u quáº£ tá»‘t nháº¥t.

---

### 5ï¸âƒ£ So sÃ¡nh Sigmoid & Tanh

| HÃ m | CÃ´ng thá»©c | Miá»n giÃ¡ trá»‹ | Ghi chÃº |
|------|-----------|--------------|----------|
| **Sigmoid** | \(\sigma(x)=\frac{1}{1+e^{-x}}\) | (0,1) | DÃ¹ng cho xÃ¡c suáº¥t |
| **Tanh** | \(\tanh(x)=\frac{e^x - e^{-x}}{e^x + e^{-x}}\) | (-1,1) | Trung tÃ¢m 0, gradient khá»e hÆ¡n |

Quan há»‡ giá»¯a hai hÃ m:
\[
\tanh(x) = 2\sigma(2x)-1,\quad
\sigma(x) = \frac{\tanh(x/2)+1}{2}
\]

---

### 6ï¸âƒ£ VÃ­ dá»¥ minh hoáº¡

\[
X =
\begin{bmatrix}
1 & 1.5 & 0.2\\
1 & 4.1 & 1.3
\end{bmatrix}, \quad
\mathbf{y} =
\begin{bmatrix}
0\\
1
\end{bmatrix}
\]

\[
\theta =
\begin{bmatrix}
0.1\\
0.5\\
-0.1
\end{bmatrix}
\Rightarrow
\mathbf{z} = X\theta = [0.83,\; 2.02]
\]
\[
\hat{\mathbf{y}} = [0.696,\; 0.883], \quad
\nabla_\theta L = [0.2896,\; 0.2822,\; -0.0064]
\]

> Cáº­p nháº­t: \(\theta_{\text{new}} = [0.0971, 0.4971, -0.0990]\)

---

### 7ï¸âƒ£ Regularization (Chá»‘ng overfitting)

**L2 (Ridge):**
\[
J(\mathbf{w},b)=L+\frac{\lambda}{2m}\sum_j w_j^2
\]
Cáº­p nháº­t:
\[
\mathbf{w}\leftarrow \mathbf{w}-\eta\left(\nabla_\mathbf{w}L+\frac{\lambda}{m}\mathbf{w}\right)
\]

**L1 (Lasso):**
\[
J(\mathbf{w},b)=L+\frac{\lambda}{m}\sum_j|w_j|
\]
â†’ Táº¡o **sparsity** (chá»n lá»c Ä‘áº·c trÆ°ng).

> L2 = á»•n Ä‘á»‹nh hÆ¡n, L1 = chá»n Ä‘áº·c trÆ°ng tá»± Ä‘á»™ng.

---

### 8ï¸âƒ£ Momentum & Adam Optimizer

**Momentum:**
\[
v_t = \beta v_{t-1} + (1-\beta)\nabla L,\quad
\theta \leftarrow \theta - \eta v_t
\]

**Adam:**
\[
m_t = \beta_1 m_{t-1} + (1-\beta_1)\nabla L,\quad
v_t = \beta_2 v_{t-1} + (1-\beta_2)(\nabla L)^2
\]
\[
\theta \leftarrow \theta - \eta\frac{m_t}{\sqrt{v_t}+\epsilon}
\]

> ğŸ¯ Adam káº¿t há»£p tá»‘c Ä‘á»™ cá»§a SGD vÃ  á»•n Ä‘á»‹nh cá»§a RMSProp â†’ dÃ¹ng máº·c Ä‘á»‹nh trong háº§u háº¿t deep learning frameworks.

---

### 9ï¸âƒ£ á»¨ng dá»¥ng thá»±c táº¿

- **PhÃ¢n loáº¡i nhá»‹ phÃ¢n:** spam, bá»‡nh lÃ½, churn prediction.  
- **Æ¯u Ä‘iá»ƒm:** Ä‘Æ¡n giáº£n, huáº¥n luyá»‡n nhanh, dá»… giáº£i thÃ­ch.  
- **Háº¡n cháº¿:** biÃªn quyáº¿t Ä‘á»‹nh tuyáº¿n tÃ­nh, cáº§n feature scaling.

<p align="center">
  <img src="{{ '/assets/module6-week1/bce_surface.png' | relative_url }}" width="480" alt="BCE Surface">
  <br><em>HÃ¬nh 2 â€“ Trá»±c giÃ¡c BCE vÃ  vector hÃ³a trong khÃ´ng gian tham sá»‘</em>
</p>

---

### ğŸ”Ÿ Tá»•ng káº¿t ğŸ¯

| ThÃ nh pháº§n | Ã nghÄ©a / CÃ´ng thá»©c |
|-------------|---------------------|
| MÃ´ hÃ¬nh | \(\hat{y} = \sigma(\mathbf{w}^\top\mathbf{x}+b)\) |
| Loss | \(L=-[y\log\hat{y}+(1-y)\log(1-\hat{y})]\) |
| Gradient | \(\nabla L = X^\top(\hat{\mathbf{y}}-\mathbf{y})\) |
| Cáº­p nháº­t | \(\theta \leftarrow \theta - \eta\nabla L\) |
| Regularization | L1 (sparse), L2 (smooth) |
| Optimizer | GD, SGD, Adam |
| á»¨ng dá»¥ng | Binary classification, Medical AI, Finance |

---

> Logistic Regression lÃ  â€œcáº§u ná»‘iâ€ giá»¯a há»“i quy tuyáº¿n tÃ­nh vÃ  máº¡ng nÆ¡-ron.  
> Hiá»ƒu vectorization chÃ­nh lÃ  hiá»ƒu cÆ¡ cháº¿ tÃ­nh toÃ¡n cá»§a cÃ¡c layers trong deep learning.

ğŸ”™ [Vá» trang chá»§]({{ '/' | relative_url }})

{% include mathjax.html %}


---

[ğŸ  Vá» trang chá»§]({{ '/' | relative_url }})  



