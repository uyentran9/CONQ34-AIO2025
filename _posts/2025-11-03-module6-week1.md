---
layout: post
title: "Module 6 â€“ Tuáº§n 1 ğŸ¯ Tá»« Linear Regression Ä‘áº¿n Logistic Regression"
date: 2025-11-05
categories: [AIO2025, Module6, MachineLearning]
---

## ğŸŒŸ Giá»›i thiá»‡u
Tuáº§n nÃ y chÃºng mÃ¬nh báº¯t Ä‘áº§u má»™t cháº·ng má»›i trong hÃ nh trÃ¬nh Machine Learning â€” **bÆ°á»›c chuyá»ƒn tá»« dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c sang phÃ¢n loáº¡i nhá»‹ phÃ¢n**.  
Hay nÃ³i cÃ¡ch khÃ¡c, tá»« viá»‡c *Ä‘oÃ¡n giÃ¡ nhÃ * ğŸ  sang *xÃ¡c Ä‘á»‹nh xem cÃ³ pháº£i mÃ¨o khÃ´ng* ğŸ±âœ¨  
Chá»§ Ä‘á»: **â€œFrom Linear Regression to Logistic Regression.â€**

---

## ğŸ¯ Má»¥c tiÃªu há»c táº­p
- Hiá»ƒu **Linear Regression** vÃ  cÃ¡ch tá»‘i Æ°u báº±ng **Gradient Descent**.  
- Biáº¿t táº¡i sao Linear Regression khÃ´ng phÃ¹ há»£p cho bÃ i toÃ¡n phÃ¢n loáº¡i.  
- Há»c cÃ¡ch dÃ¹ng **Sigmoid Function** Ä‘á»ƒ biáº¿n mÃ´ hÃ¬nh tuyáº¿n tÃ­nh thÃ nh mÃ´ hÃ¬nh xÃ¡c suáº¥t.  
- LÃ m quen vá»›i **Binary Cross Entropy (BCE)** â€“ hÃ m loss dÃ nh cho classification.

---

## ğŸ“ˆ Pháº§n 1 â€“ Ã”n láº¡i Linear Regression
Linear Regression lÃ  mÃ´ hÃ¬nh â€œnháº¹ nhÃ ngâ€ nháº¥t nhÆ°ng cá»±c ká»³ quan trá»ng ğŸŒ±  
CÃ´ng thá»©c cÆ¡ báº£n:
\[
\hat{y} = w \cdot x + b
\]
Ta muá»‘n tÃ¬m `w` vÃ  `b` sao cho sai sá»‘ (loss) nhá» nháº¥t:
\[
L = (\hat{y} - y)^2
\]

**CÃ¡ch há»c cá»§a mÃ´ hÃ¬nh:**
- Dá»± Ä‘oÃ¡n giÃ¡ trá»‹ `Å·`  
- So sÃ¡nh vá»›i giÃ¡ trá»‹ tháº­t `y`  
- TÃ­nh Ä‘áº¡o hÃ m âˆ‚L/âˆ‚w, âˆ‚L/âˆ‚b  
- Cáº­p nháº­t:
  \[
  w = w - Î· \frac{âˆ‚L}{âˆ‚w}, \quad b = b - Î· \frac{âˆ‚L}{âˆ‚b}
  \]

> ğŸ§  Má»—i bÆ°á»›c cáº­p nháº­t lÃ  má»™t â€œbÆ°á»›c Ä‘i nhá»â€ giÃºp mÃ´ hÃ¬nh há»c cÃ¡ch dá»± Ä‘oÃ¡n Ä‘Ãºng hÆ¡n.

---

## ğŸ§­ Pháº§n 2 â€“ Khi Ä‘Æ°á»ng tháº³ng khÃ´ng cÃ²n Ä‘á»§
Linear Regression hoáº¡t Ä‘á»™ng tá»‘t khi dá»¯ liá»‡u tuyáº¿n tÃ­nh,  
nhÆ°ng vá»›i dá»¯ liá»‡u cÃ³ **2 nhÃ£n (0 vÃ  1)**, nÃ³ trá»Ÿ nÃªn vÃ´ lÃ½:
- Káº¿t quáº£ cÃ³ thá»ƒ < 0 hoáº·c > 1 âŒ  
- KhÃ´ng thá»ƒ diá»…n giáº£i nhÆ° â€œxÃ¡c suáº¥tâ€  

ğŸ‘‰ Ta cáº§n má»™t phÃ©p â€œnáº¯nâ€ Ä‘Æ°á»ng tháº³ng Ä‘á»ƒ nÃ³ náº±m gá»n trong [0,1].  
ÄÃ³ lÃ  lÃºc **Sigmoid Function** xuáº¥t hiá»‡n:

\[
Ïƒ(z) = \frac{1}{1 + e^{-z}}
\]

> HÃ£y tÆ°á»Ÿng tÆ°á»£ng sigmoid nhÆ° má»™t â€œÄ‘Æ°á»ng cong tháº§n ká»³â€ ğŸŒ€ biáº¿n má»i sá»‘ thá»±c thÃ nh xÃ¡c suáº¥t.

---

## ğŸª„ Pháº§n 3 â€“ Logistic Regression xuáº¥t hiá»‡n
Logistic Regression váº«n dÃ¹ng cÃ´ng thá»©c tuyáº¿n tÃ­nh:
\[
z = w \cdot x + b
\]
nhÆ°ng Ä‘áº§u ra Ä‘Æ°á»£c â€œnáº¯nâ€ báº±ng sigmoid:
\[
\hat{y} = Ïƒ(z)
\]
â†’ giÃºp mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n xÃ¡c suáº¥t `P(y=1|x)` ğŸ¯

**VÃ­ dá»¥:**  
Náº¿u `Å· = 0.85`, ta cÃ³ thá»ƒ nÃ³i â€œmáº«u nÃ y cÃ³ 85% kháº£ nÄƒng lÃ  mÃ¨o ğŸ±â€.

---

## âš¡ Pháº§n 4 â€“ HÃ m loss má»›i: Binary Cross Entropy
Linear Regression dÃ¹ng **Mean Squared Error**,  
nhÆ°ng Logistic Regression cáº§n **Binary Cross Entropy (BCE)**:
\[
L = -[y \log(\hat{y}) + (1 - y)\log(1 - \hat{y})]
\]

ğŸ’¬ Ã nghÄ©a:
- Náº¿u mÃ´ hÃ¬nh **Ä‘oÃ¡n sai mÃ  láº¡i quÃ¡ tá»± tin**, BCE sáº½ **pháº¡t náº·ng** âš¡  
- GiÃºp mÃ´ hÃ¬nh há»c cáº©n tháº­n hÆ¡n vá»›i xÃ¡c suáº¥t cá»§a mÃ¬nh.

> VÃ­ dá»¥: Dá»± Ä‘oÃ¡n 0.9 nhÆ°ng tháº­t ra nhÃ£n lÃ  0 â†’ loss tÄƒng ráº¥t cao! ğŸ˜…

---

## ğŸ”¢ Pháº§n 5 â€“ Há»c báº±ng tay (stochastic update)
VÃ­ dá»¥ trong slide tháº§y Quang-Vinh ğŸ‘¨â€ğŸ«:

| ThÃ´ng sá»‘ ban Ä‘áº§u | GiÃ¡ trá»‹ |
|------------------|----------|
| x = 1.4 | y = 0 |
| w = -0.1 | b = 0.1 |
| Î· (learning rate) | 0.01 |

- TÃ­nh `z = w*x + b = -0.04`  
- Sigmoid: `Å· = 0.49`  
- Loss (BCE): `0.6733`  
- Cáº­p nháº­t `w` vÃ  `b` â†’ loss giáº£m cÃ²n `0.666` sau 1 bÆ°á»›c âœ…:contentReference[oaicite:0]{index=0}

> Cáº£m giÃ¡c tháº­t thÃº vá»‹ khi tháº¥y mÃ´ hÃ¬nh â€œhá»câ€ dáº§n dáº§n chá»‰ tá»« vÃ i con sá»‘ nhá»!

---

## ğŸ§® Pháº§n 6 â€“ So sÃ¡nh nhanh: MSE vs BCE
| Äáº·c Ä‘iá»ƒm | MSE | BCE |
|-----------|-----|-----|
| Kiá»ƒu bÃ i toÃ¡n | Regression (liÃªn tá»¥c) | Classification (nhá»‹ phÃ¢n) |
| GiÃ¡ trá»‹ há»£p lá»‡ | [âˆ’âˆ, +âˆ] | [0, 1] |
| Äá»™ cong cá»§a hÃ m loss | KhÃ´ng lá»“i, dá»… máº¯c káº¹t | Lá»“i, dá»… tá»‘i Æ°u hÆ¡n |
| Trá»±c quan | Äo sai sá»‘ dá»± Ä‘oÃ¡n | Äo â€œÄ‘á»™ ngáº¡c nhiÃªnâ€ cá»§a mÃ´ hÃ¬nh |

ğŸ“Š TÃ³m láº¡i: **MSE Ä‘o khoáº£ng cÃ¡ch**, **BCE Ä‘o niá»m tin sai láº§m** ğŸ˜†

---

## ğŸ’¬ Cáº£m nháº­n cÃ¡ nhÃ¢n
Tuáº§n nÃ y lÃ  láº§n Ä‘áº§u tiÃªn mÃ¬nh tháº¥y *há»c mÃ¡y tháº­t sá»± biáº¿t há»c*.  
Tá»« má»™t mÃ´ hÃ¬nh tÆ°á»Ÿng chá»«ng Ä‘Æ¡n giáº£n (Ä‘Æ°á»ng tháº³ng),  
nÃ³ trá»Ÿ thÃ nh cÃ´ng cá»¥ cÃ³ thá»ƒ **hiá»ƒu xÃ¡c suáº¥t vÃ  ra quyáº¿t Ä‘á»‹nh**.  

> â€œRegression giÃºp ta Æ°á»›c lÆ°á»£ng ğŸ“.  
> Logistic giÃºp ta lá»±a chá»n ğŸ§­.â€

---

## ğŸ“š TÃ i nguyÃªn
- [Pháº§n code cá»§a tháº§y Quang-Vinh Dinh (Google Drive)](https://drive.google.com/drive/folders/1bvpDCVaPBYvmTvJnteyHxwLy4OmF_7l0?usp=sharing)
- [Wikipedia â€“ Logistic Function](https://en.wikipedia.org/wiki/Logistic_function)

---

## ğŸ§© Mini Quiz (vui há»c ğŸ²)
1ï¸âƒ£ Logistic Regression dá»± Ä‘oÃ¡n **giÃ¡ trá»‹ gÃ¬**?  
   â˜ Sá»‘ thá»±c báº¥t ká»³â€ƒâ˜‘ XÃ¡c suáº¥t trong [0,1]  

2ï¸âƒ£ VÃ¬ sao dÃ¹ng **BCE thay vÃ¬ MSE**?  
   â˜ VÃ¬ nghe hay hÆ¡n  
   â˜‘ VÃ¬ BCE pháº£n Ã¡nh lá»—i xÃ¡c suáº¥t tá»‘t hÆ¡n  

3ï¸âƒ£ Sigmoid function Ä‘áº£m báº£o Ä‘iá»u gÃ¬?  
   â˜‘ Äáº§u ra luÃ´n trong [0,1]  
   â˜ Tá»‘c Ä‘á»™ há»c nhanh hÆ¡n

---

ğŸ’¡ *Háº¹n gáº·p á»Ÿ bÃ i giáº£ng tiáº¿p theo cá»§a tuáº§n 1 â€“ Khi chÃºng ta dive sÃ¢u hÆ¡n vÃ o Multi-feature Logistic Regression nhÃ©!* ğŸš€
