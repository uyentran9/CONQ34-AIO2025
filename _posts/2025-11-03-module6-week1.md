## ğŸ“ˆ Pháº§n 1 â€“ Ã”n láº¡i Linear Regression

**Linear Regression (Há»“i quy tuyáº¿n tÃ­nh)** lÃ  má»™t mÃ´ hÃ¬nh dÃ¹ng Ä‘á»ƒ **dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c** dá»±a trÃªn cÃ¡c biáº¿n Ä‘áº§u vÃ o (*features*).  
Má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh lÃ  tÃ¬m má»™t Ä‘Æ°á»ng tháº³ng (hoáº·c siÃªu pháº³ng trong khÃ´ng gian nhiá»u chiá»u) sao cho tá»•ng sai sá»‘ giá»¯a dá»± Ä‘oÃ¡n vÃ  dá»¯ liá»‡u tháº­t lÃ  nhá» nháº¥t.

PhÆ°Æ¡ng trÃ¬nh tá»•ng quÃ¡t:

\[
\hat{y} = w_1x_1 + w_2x_2 + \dots + w_nx_n + b = \mathbf{X}\boldsymbol{\theta}
\]

Trong Ä‘Ã³:

- \( \mathbf{X} \in \mathbb{R}^{m \times n} \): ma tráº­n dá»¯ liá»‡u Ä‘áº§u vÃ o gá»“m **m máº«u** vÃ  **n Ä‘áº·c trÆ°ng**  
  (má»—i hÃ ng lÃ  1 máº«u dá»¯ liá»‡u, má»—i cá»™t lÃ  1 Ä‘áº·c trÆ°ng)  
- \( \boldsymbol{\theta} = [w_1, w_2, ..., w_n, b]^T \): vector trá»ng sá»‘ vÃ  bias  
- \( \hat{y} \): giÃ¡ trá»‹ dá»± Ä‘oÃ¡n Ä‘áº§u ra  
- \( y \): giÃ¡ trá»‹ thá»±c táº¿ cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n  
- \( b \): há»‡ sá»‘ bias giÃºp Ä‘iá»u chá»‰nh vá»‹ trÃ­ Ä‘Æ°á»ng há»“i quy  

---
Äá»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh, ta dÃ¹ng **Mean Squared Error (MSE)**:

\[
L(\hat{y}, y) = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2
\]

Trong Ä‘Ã³:

- \( y_i \): giÃ¡ trá»‹ tháº­t  
- \( \hat{y}_i \): giÃ¡ trá»‹ dá»± Ä‘oÃ¡n  
- \( m \): sá»‘ lÆ°á»£ng máº«u  

---

Má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh **Linear Regression** lÃ  **tÃ¬m ra cÃ¡c tham sá»‘ tá»‘i Æ°u** â€” cá»¥ thá»ƒ lÃ  **vector trá»ng sá»‘** \( \mathbf{w} \) vÃ  **há»‡ sá»‘ bias** \( b \) â€” sao cho giÃ¡ trá»‹ cá»§a \( L_{\text{MSE}} \) lÃ  **nhá» nháº¥t cÃ³ thá»ƒ**.
![Loss Function](/assets/module6-week1/loss%20function.png)
- Má»—i cáº·p \( (\hat{y}^{(i)}, y^{(i)}) \) thá»ƒ hiá»‡n **má»©c Ä‘á»™ sai lá»‡ch** giá»¯a dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ thá»±c.  
- BÃ¬nh phÆ°Æ¡ng sai sá»‘ \( (\hat{y}^{(i)} - y^{(i)})^2 \) giÃºp **loáº¡i bá» dáº¥u Ã¢m** vÃ  **pháº¡t máº¡nh hÆ¡n cÃ¡c dá»± Ä‘oÃ¡n sai lá»‡ch lá»›n**.  
- Sau khi láº¥y trung bÃ¬nh trÃªn táº¥t cáº£ cÃ¡c máº«u, ta thu Ä‘Æ°á»£c **má»©c sai sá»‘ trung bÃ¬nh toÃ n bá»™ mÃ´ hÃ¬nh**.  
- Báº±ng cÃ¡ch Ä‘iá»u chá»‰nh \( \mathbf{w} \) vÃ  \( b \), mÃ´ hÃ¬nh â€œxoayâ€ hoáº·c â€œdá»‹ch chuyá»ƒnâ€ Ä‘Æ°á»ng há»“i quy Ä‘á»ƒ **lÃ m giáº£m sai sá»‘ tá»•ng thá»ƒ** nÃ y.\

---

**Quy trÃ¬nh tá»•ng quÃ¡t (Pipeline) cá»§a Linear Regression**

Sau khi hiá»ƒu cÃ¡ch mÃ´ hÃ¬nh há»c Ä‘á»ƒ giáº£m sai sá»‘ (loss), ta cÃ³ thá»ƒ khÃ¡i quÃ¡t quy trÃ¬nh huáº¥n luyá»‡n Logistic Regression qua cÃ¡c bÆ°á»›c sau:

![Pipeline Diagram](/assets/module6-week1/pipeline.png)

---

âš ï¸ Háº¡n cháº¿ cá»§a Linear Regression trong bÃ i toÃ¡n phÃ¢n loáº¡i

Máº·c dÃ¹ Linear Regression lÃ  mÃ´ hÃ¬nh ná»n táº£ng, dá»… hiá»ƒu vÃ  hiá»‡u quáº£ trong cÃ¡c bÃ i toÃ¡n **dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c**,  
nhÆ°ng khi Ã¡p dá»¥ng cho **bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n (0/1)**, nÃ³ bá»™c lá»™ má»™t sá»‘ **nhÆ°á»£c Ä‘iá»ƒm nghiÃªm trá»ng**:

**KhÃ´ng giá»›i háº¡n Ä‘áº§u ra trong [0, 1]**
- MÃ´ hÃ¬nh tuyáº¿n tÃ­nh cÃ³ dáº¡ng \( \hat{y} = \mathbf{w}^T\mathbf{x} + b \).  
- Äáº§u ra \( \hat{y} \) cÃ³ thá»ƒ nháº­n **báº¥t ká»³ giÃ¡ trá»‹ nÃ o** trong khoáº£ng \((-\infty, +\infty)\).  
> Äiá»u nÃ y **khÃ´ng thá»ƒ diá»…n giáº£i nhÆ° má»™t xÃ¡c suáº¥t**, vÃ  khiáº¿n viá»‡c **Ä‘áº·t ngÆ°á»¡ng phÃ¢n loáº¡i (threshold)** trá»Ÿ nÃªn tÃ¹y tiá»‡n.

**MÃ´ hÃ¬nh tuyáº¿n tÃ­nh khÃ´ng phÃ¹ há»£p vá»›i ranh giá»›i phÃ¢n loáº¡i phi tuyáº¿n**
- Trong nhiá»u bÃ i toÃ¡n thá»±c táº¿, dá»¯ liá»‡u cá»§a hai lá»›p (0 vÃ  1) **khÃ´ng thá»ƒ phÃ¢n tÃ¡ch báº±ng má»™t Ä‘Æ°á»ng tháº³ng**.  
- Linear Regression chá»‰ há»c Ä‘Æ°á»£c má»‘i quan há»‡ tuyáº¿n tÃ­nh giá»¯a Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra, nÃªn **khÃ´ng thá»ƒ mÃ´ hÃ¬nh hÃ³a ranh giá»›i phi tuyáº¿n**.

**HÃ m máº¥t mÃ¡t (MSE) khÃ´ng hiá»‡u quáº£ cho phÃ¢n loáº¡i**
- Linear Regression sá»­ dá»¥ng **Mean Squared Error (MSE)**, vá»‘n Ä‘Æ°á»£c thiáº¿t káº¿ cho bÃ i toÃ¡n há»“i quy.  
- Trong phÃ¢n loáº¡i, hÃ m nÃ y:
  - KhÃ´ng pháº£n Ã¡nh tá»‘t **má»©c Ä‘á»™ tin cáº­y** cá»§a dá»± Ä‘oÃ¡n xÃ¡c suáº¥t.  
  - Khi káº¿t há»£p vá»›i hÃ m sigmoid, MSE khiáº¿n **gradient há»™i tá»¥ cháº­m** vÃ  **dá»… máº¯c káº¹t táº¡i cá»±c trá»‹ cá»¥c bá»™**.
**KhÃ´ng Ä‘áº£m báº£o mÃ´ hÃ¬nh há»™i tá»¥ á»•n Ä‘á»‹nh**
- VÃ¬ Ä‘áº§u ra \( \hat{y} \) khÃ´ng bá»‹ giá»›i háº¡n, mÃ´ hÃ¬nh cÃ³ thá»ƒ **tÄƒng/giáº£m vÃ´ háº¡n** Ä‘á»ƒ cá»‘ gáº¯ng giáº£m MSE.  
- Káº¿t quáº£ lÃ :
  - Gradient descent **dao Ä‘á»™ng hoáº·c diverge** (khÃ´ng há»™i tá»¥).  
  - MÃ´ hÃ¬nh **quÃ¡ nháº¡y cáº£m vá»›i nhiá»…u hoáº·c ngoáº¡i lá»‡ (outliers)**.

ğŸ‘‰ Giáº£i phÃ¡p lÃ  **Logistic Regression**, má»™t mÃ´ hÃ¬nh má»Ÿ rá»™ng Linear Regression vá»›i **hÃ m kÃ­ch hoáº¡t sigmoid** Ä‘á»ƒ Ã¡nh xáº¡ Ä‘áº§u ra vá» khoáº£ng (0, 1) â€” giÃºp mÃ´ hÃ¬nh **dá»± Ä‘oÃ¡n xÃ¡c suáº¥t chÃ­nh xÃ¡c hÆ¡n**.


## ğŸ”€ Pháº§n 2 â€“ VÃ¬ sao cáº§n Logistic Regression?

BÃ i toÃ¡n **nhá»‹ phÃ¢n** (0/1) cáº§n **xÃ¡c suáº¥t** $P(y=1\mid \mathbf{x}) \in [0,1]$.  
Ta â€œnÃ©nâ€ Ä‘áº§u ra báº±ng **Sigmoid**:
$$
\sigma(z) = \frac{1}{1+e^{-z}},\qquad z=\mathbf{w}^\top \mathbf{x} + b
$$

Dá»± Ä‘oÃ¡n:
- XÃ¡c suáº¥t: $\hat{p} = \sigma(\mathbf{w}^\top \mathbf{x} + b)$
- NhÃ£n: $\hat{y} = \mathbb{1}[\hat{p} \ge \tau]$ (thÆ°á»ng $\tau = 0.5$)

---

## ğŸ¯ Pháº§n 3 â€“ HÃ m máº¥t mÃ¡t: MSE hay BCE?

- DÃ¹ng MSE cho xÃ¡c suáº¥t **khÃ´ng tá»‘t**: Ä‘áº¡o hÃ m nhá» khi gáº§n 0/1 â†’ há»c cháº­m; tá»‘i Æ°u dá»… â€œbÆ°á»›ngâ€.
- **Binary Cross-Entropy (Log Loss)** phÃ¹ há»£p hÆ¡n:
$$
L_{\text{BCE}}
= -\frac{1}{n}\sum_{i=1}^n \Big( y^{(i)}\log \hat{p}^{(i)} + (1-y^{(i)})\log\big(1-\hat{p}^{(i)}\big) \Big)
$$
Trong Ä‘Ã³ $\hat{p}^{(i)}=\sigma(\mathbf{w}^\top \mathbf{x}^{(i)} + b)$.



---

## ğŸ› ï¸ Pháº§n 4 â€“ Há»c tham sá»‘ báº±ng Gradient Descent

Vá»›i 1 máº«u $(\mathbf{x}, y)$, Ä‘áº¡o hÃ m theo $\mathbf{w}$ cá»§a BCE:
$$
\nabla_{\mathbf{w}} L = (\hat{p}-y)\,\mathbf{x},\qquad
\frac{\partial L}{\partial b} = \hat{p}-y
$$
Cáº­p nháº­t:
$$
\mathbf{w} \leftarrow \mathbf{w} - \eta \, (\hat{p}-y)\mathbf{x},\qquad
b \leftarrow b - \eta \, (\hat{p}-y)
$$
($\eta$: learning rate)

---

## ğŸ§­ Pháº§n 5 â€“ Decision Boundary & Prob. View

- BiÃªn quyáº¿t Ä‘á»‹nh khi $\hat{p}=0.5 \iff \mathbf{w}^\top \mathbf{x}+b=0$ â‡’ lÃ  **siÃªu pháº³ng tuyáº¿n tÃ­nh**.
- NhÃ¬n theo **log-odds**:
$$
\log\frac{\hat{p}}{1-\hat{p}} = \mathbf{w}^\top \mathbf{x} + b
$$
â†’ Logistic Regression chÃ­nh lÃ  **há»“i quy tuyáº¿n tÃ­nh trÃªn log-odds**.

---

## ğŸ“š TÃ i nguyÃªn
- [Coding cho bÃ i giáº£ng cá»§a tháº§y Quang-Vinh Dinh (Google Drive)](https://â€¦)  
- [Wikipedia â€“ Logistic Function](https://en.wikipedia.org/wiki/Logistic_function)

---

## ğŸ§© Mini Quiz (vui há»c ğŸ²)  
1ï¸âƒ£ Logistic Regression dá»± Ä‘oÃ¡n **cÃ¡i gÃ¬**? â†’ **XÃ¡c suáº¥t** $P(y=1\mid \mathbf{x})$ trong $[0,1]$.  

2ï¸âƒ£ VÃ¬ sao chá»n **BCE** thay vÃ¬ **MSE**? â†’ PhÃ¹ há»£p xÃ¡c suáº¥t + gradient â€œkhá»eâ€ hÆ¡n.  

3ï¸âƒ£ Khi nÃ o $\hat{y}=1$? â†’ Khi $\sigma(\mathbf{w}^\top\mathbf{x}+b)\ge0.5$ (tÆ°Æ¡ng Ä‘Æ°Æ¡ng $\mathbf{w}^\top\mathbf{x}+b\ge0$). 

4ï¸âƒ£ Sigmoid function Ä‘áº£m báº£o Ä‘iá»u gÃ¬?  
   â˜‘ Äáº§u ra luÃ´n trong [0,1]  
   â˜ Tá»‘c Ä‘á»™ há»c nhanh hÆ¡n  

---

ğŸ’¡ *Háº¹n gáº·p á»Ÿ bÃ i giáº£ng tiáº¿p theo cá»§a tuáº§n 1 â€“ Khi chÃºng ta dive sÃ¢u hÆ¡n vÃ o Multi-feature Logistic Regression nhÃ©!* ğŸš€

{% include mathjax.html %}  

---

[ğŸ  Vá» trang chá»§]({{ '/' | relative_url }})  



