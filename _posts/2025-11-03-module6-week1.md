---
layout: post
title: Module 6 â€“ Tuáº§n 1
date: 06-11-2025
categories: AIO2025 Module6 Deep Learning
use_math:  true
image: /assets/module6-week1/m6w1.jpg
---

{% include mathjax.html %}


## TÃ³m táº¯t bÃ i giáº£ng ngÃ y thá»© TÆ° ngÃ y 05/11/2025  

<p align="center">
  <img src="{{ '/assets/module6-week1/m6w1.jpg' | relative_url }}" alt="Logistic Regression vs Linear Regression illustration" width="80%">
</p>



## ğŸ“ˆ Pháº§n 1 â€“ Ã”n láº¡i Linear Regression

**Linear Regression (Há»“i quy tuyáº¿n tÃ­nh)** lÃ  má»™t mÃ´ hÃ¬nh dÃ¹ng Ä‘á»ƒ **dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c** dá»±a trÃªn cÃ¡c biáº¿n Ä‘áº§u vÃ o (*features*).  
Má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh lÃ  tÃ¬m má»™t Ä‘Æ°á»ng tháº³ng (hoáº·c siÃªu pháº³ng trong khÃ´ng gian nhiá»u chiá»u) sao cho tá»•ng sai sá»‘ giá»¯a dá»± Ä‘oÃ¡n vÃ  dá»¯ liá»‡u tháº­t lÃ  nhá» nháº¥t.

PhÆ°Æ¡ng trÃ¬nh tá»•ng quÃ¡t:
 

$$
\hat{y} = w_1x_1 + w_2x_2 + \dots + w_nx_n + b = \mathbf{X}\boldsymbol{\theta}
$$


Trong Ä‘Ã³:

- $\mathbf{X}\in\mathbb{R}^{m\times n}$: ma tráº­n dá»¯ liá»‡u Ä‘áº§u vÃ o gá»“m **m máº«u** vÃ  **n Ä‘áº·c trÆ°ng**
  (má»—i hÃ ng lÃ  1 máº«u dá»¯ liá»‡u, má»—i cá»™t lÃ  1 Ä‘áº·c trÆ°ng)
- $\boldsymbol{\theta}=[w_1, w_2, \ldots, w_n, b]^T$: vector trá»ng sá»‘ vÃ  bias
- $\hat{y}$: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n Ä‘áº§u ra
- $y$: giÃ¡ trá»‹ thá»±c táº¿ cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n
- $b$: há»‡ sá»‘ bias giÃºp Ä‘iá»u chá»‰nh vá»‹ trÃ­ Ä‘Æ°á»ng há»“i quy
 


---  

$$
L(\hat{y}, y) = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2
$$

Trong Ä‘Ã³:

- $y_i$: giÃ¡ trá»‹ tháº­t  
- $\hat{y}_i$: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n  
- $m$: sá»‘ lÆ°á»£ng máº«u  


---

Má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh **Linear Regression** lÃ  **tÃ¬m ra cÃ¡c tham sá»‘ tá»‘i Æ°u** â€” cá»¥ thá»ƒ lÃ  **vector trá»ng sá»‘** $\mathbf{w}$ vÃ  **há»‡ sá»‘ bias** $b$ â€” sao cho giÃ¡ trá»‹ cá»§a $L_{\text{MSE}}$ lÃ  **nhá» nháº¥t cÃ³ thá»ƒ**.


<p align="center">
  <img src="{{ '/assets/module6-week1/loss-function.png' | relative_url }}" alt="Loss Function" width="600">
</p>  

- Má»—i cáº·p $(\hat{y}^{(i)}, y^{(i)})$ thá»ƒ hiá»‡n **má»©c Ä‘á»™ sai lá»‡ch** giá»¯a dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ thá»±c.  
- BÃ¬nh phÆ°Æ¡ng sai sá»‘ $(\hat{y}^{(i)} - y^{(i)})^2$ giÃºp **loáº¡i bá» dáº¥u Ã¢m** vÃ  **pháº¡t máº¡nh hÆ¡n cÃ¡c dá»± Ä‘oÃ¡n sai lá»‡ch lá»›n**.  
- Sau khi láº¥y trung bÃ¬nh trÃªn táº¥t cáº£ cÃ¡c máº«u, ta thu Ä‘Æ°á»£c **má»©c sai sá»‘ trung bÃ¬nh toÃ n bá»™ mÃ´ hÃ¬nh**.  
- Báº±ng cÃ¡ch Ä‘iá»u chá»‰nh $\mathbf{w}$ vÃ  $b$, mÃ´ hÃ¬nh â€œxoayâ€ hoáº·c â€œdá»‹ch chuyá»ƒnâ€ Ä‘Æ°á»ng há»“i quy Ä‘á»ƒ **lÃ m giáº£m sai sá»‘ tá»•ng thá»ƒ** nÃ y.\

---

**Quy trÃ¬nh tá»•ng quÃ¡t (Pipeline) cá»§a Linear Regression**

Sau khi hiá»ƒu cÃ¡ch mÃ´ hÃ¬nh há»c Ä‘á»ƒ giáº£m sai sá»‘ (loss), ta cÃ³ thá»ƒ khÃ¡i quÃ¡t quy trÃ¬nh huáº¥n luyá»‡n Logistic Regression qua cÃ¡c bÆ°á»›c sau:

![Pipeline Diagram]({{ "/assets/module6-week1/pipeline.png" | relative_url }})


---

**âš ï¸ Háº¡n cháº¿ cá»§a Linear Regression trong bÃ i toÃ¡n phÃ¢n loáº¡i**

Máº·c dÃ¹ Linear Regression lÃ  mÃ´ hÃ¬nh ná»n táº£ng, dá»… hiá»ƒu vÃ  hiá»‡u quáº£ trong cÃ¡c bÃ i toÃ¡n **dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c**,  
nhÆ°ng khi Ã¡p dá»¥ng cho **bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n (0/1)**, nÃ³ bá»™c lá»™ má»™t sá»‘ **nhÆ°á»£c Ä‘iá»ƒm nghiÃªm trá»ng**:

**KhÃ´ng giá»›i háº¡n Ä‘áº§u ra trong [0, 1]**
- MÃ´ hÃ¬nh tuyáº¿n tÃ­nh cÃ³ dáº¡ng $\hat{y} = \mathbf{w}^T\mathbf{x} + b$.  
- Äáº§u ra $\hat{y}$ cÃ³ thá»ƒ nháº­n **báº¥t ká»³ giÃ¡ trá»‹ nÃ o** trong khoáº£ng $(-\infty, +\infty)$.  
> Äiá»u nÃ y **khÃ´ng thá»ƒ diá»…n giáº£i nhÆ° má»™t xÃ¡c suáº¥t**, vÃ  khiáº¿n viá»‡c **Ä‘áº·t ngÆ°á»¡ng phÃ¢n loáº¡i (threshold)** trá»Ÿ nÃªn tÃ¹y tiá»‡n.

**MÃ´ hÃ¬nh tuyáº¿n tÃ­nh khÃ´ng phÃ¹ há»£p vá»›i ranh giá»›i phÃ¢n loáº¡i phi tuyáº¿n**
- Trong nhiá»u bÃ i toÃ¡n thá»±c táº¿, dá»¯ liá»‡u cá»§a hai lá»›p (0 vÃ  1) **khÃ´ng thá»ƒ phÃ¢n tÃ¡ch báº±ng má»™t Ä‘Æ°á»ng tháº³ng**.  
- Linear Regression chá»‰ há»c Ä‘Æ°á»£c má»‘i quan há»‡ tuyáº¿n tÃ­nh giá»¯a Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra, nÃªn **khÃ´ng thá»ƒ mÃ´ hÃ¬nh hÃ³a ranh giá»›i phi tuyáº¿n**.

**HÃ m máº¥t mÃ¡t (MSE) khÃ´ng hiá»‡u quáº£ cho phÃ¢n loáº¡i**
- Linear Regression sá»­ dá»¥ng **Mean Squared Error (MSE)**, vá»‘n Ä‘Æ°á»£c thiáº¿t káº¿ cho bÃ i toÃ¡n há»“i quy.  
- Trong phÃ¢n loáº¡i, hÃ m nÃ y:
  - KhÃ´ng pháº£n Ã¡nh tá»‘t **má»©c Ä‘á»™ tin cáº­y** cá»§a dá»± Ä‘oÃ¡n xÃ¡c suáº¥t.  
  - Khi káº¿t há»£p vá»›i hÃ m sigmoid, MSE khiáº¿n **gradient há»™i tá»¥ cháº­m** vÃ  **dá»… máº¯c káº¹t táº¡i cá»±c trá»‹ cá»¥c bá»™**.

**KhÃ´ng Ä‘áº£m báº£o mÃ´ hÃ¬nh há»™i tá»¥ á»•n Ä‘á»‹nh**
- VÃ¬ Ä‘áº§u ra $\hat{y}$ khÃ´ng bá»‹ giá»›i háº¡n, mÃ´ hÃ¬nh cÃ³ thá»ƒ **tÄƒng/giáº£m vÃ´ háº¡n** Ä‘á»ƒ cá»‘ gáº¯ng giáº£m MSE.  
- Káº¿t quáº£ lÃ :
  - Gradient descent **dao Ä‘á»™ng hoáº·c diverge** (khÃ´ng há»™i tá»¥).  
  - MÃ´ hÃ¬nh **quÃ¡ nháº¡y cáº£m vá»›i nhiá»…u hoáº·c ngoáº¡i lá»‡ (outliers)**.

ğŸ‘‰ Giáº£i phÃ¡p lÃ  **Logistic Regression**, má»™t mÃ´ hÃ¬nh má»Ÿ rá»™ng Linear Regression vá»›i **hÃ m kÃ­ch hoáº¡t sigmoid** Ä‘á»ƒ Ã¡nh xáº¡ Ä‘áº§u ra vá» khoáº£ng (0, 1) â€” giÃºp mÃ´ hÃ¬nh **dá»± Ä‘oÃ¡n xÃ¡c suáº¥t chÃ­nh xÃ¡c hÆ¡n**.


## ğŸ”€ Pháº§n 2 â€“ Logistic Regression

**1. KhÃ¡i niá»‡m cÆ¡ báº£n**

**Logistic Regression** lÃ  má»™t mÃ´ hÃ¬nh **há»c cÃ³ giÃ¡m sÃ¡t (supervised learning)** dÃ¹ng cho **bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n (binary classification)** 

âš–ï¸ So sÃ¡nh nhanh:

| MÃ´ hÃ¬nh              | Dáº¡ng Ä‘áº§u ra                     | VÃ­ dá»¥                          |
|----------------------|----------------------------------|---------------------------------|
| **Linear Regression**  | LiÃªn tá»¥c (*continuous*)          | Dá»± Ä‘oÃ¡n nhiá»‡t Ä‘á»™, giÃ¡ nhÃ        |
| **Logistic Regression**| Rá»i ráº¡c (*discrete*, 0 hoáº·c 1)   | Dá»± Ä‘oÃ¡n cÃ³ bá»‡nh hay khÃ´ng       |


**2. Báº¯t Ä‘áº§u tá»« mÃ´ hÃ¬nh tuyáº¿n tÃ­nh**

TÆ°Æ¡ng tá»± nhÆ° **Linear Regression**, **Logistic Regression** cÅ©ng báº¯t Ä‘áº§u vá»›i **má»™t mÃ´ hÃ¬nh tuyáº¿n tÃ­nh** Ä‘á»ƒ káº¿t há»£p cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o:

$$
z = wx + b
$$

Trong Ä‘Ã³:

- $x$: giÃ¡ trá»‹ Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o (*feature*)  
- $w$: trá»ng sá»‘ (*weight*) mÃ  mÃ´ hÃ¬nh cáº§n há»c  
- $b$: há»‡ sá»‘ chá»‡ch (*bias*)  
- $z$: giÃ¡ trá»‹ tuyáº¿n tÃ­nh (*logit*)

á» bÆ°á»›c nÃ y, $z$ cÃ³ thá»ƒ nháº­n má»i giÃ¡ trá»‹ thá»±c tá»« $-\infty$ Ä‘áº¿n $+\infty$.  
Tuy nhiÃªn, má»¥c tiÃªu cá»§a ta lÃ  **dá»± Ä‘oÃ¡n xÃ¡c suáº¥t** má»™t máº«u thuá»™c lá»›p 1 (vÃ­ dá»¥: â€œcÃ³ bá»‡nhâ€, â€œÄ‘á»—â€, â€œspamâ€â€¦), nÃªn Ä‘áº§u ra cáº§n náº±m trong khoáº£ng **(0, 1)**.

ğŸ‘‰ Äá»ƒ lÃ m Ä‘Æ°á»£c Ä‘iá»u Ä‘Ã³, ta **khÃ´ng dÃ¹ng trá»±c tiáº¿p** $z$ lÃ m káº¿t quáº£ dá»± Ä‘oÃ¡n mÃ  cáº§n má»™t **hÃ m chuyá»ƒn Ä‘á»•i** phÃ¹ há»£p.

**3. HÃ m Sigmoid**

Náº¿u ta dÃ¹ng luÃ´n cÃ´ng thá»©c:

$$
\hat{y} = z = \mathbf{w}x + b
$$

Ä‘á»ƒ dá»± Ä‘oÃ¡n cho bÃ i toÃ¡n **phÃ¢n loáº¡i** (vÃ­ dá»¥ â€œcÃ³ bá»‡nhâ€ hay â€œkhÃ´ng bá»‡nhâ€), ta gáº·p ngay váº¥n Ä‘á»:

- $z$ cÃ³ thá»ƒ nháº­n **báº¥t ká»³ giÃ¡ trá»‹ nÃ o** tá»« $-\infty$ Ä‘áº¿n $+\infty$
- NhÆ°ng **xÃ¡c suáº¥t thá»±c táº¿** pháº£i náº±m trong $[0, 1]$

ChÃ­nh vÃ¬ tháº¿, **hÃ m sigmoid** (hay *logistic function*) xuáº¥t hiá»‡n Ä‘á»ƒ biáº¿n Ä‘á»•i giÃ¡ trá»‹ tuyáº¿n tÃ­nh \(z\) thÃ nh má»™t **xÃ¡c suáº¥t há»£p lá»‡**:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

<p align="center">
  <img src="{{ '/assets/module6-week1/sigmoid-function.png' | relative_url }}" alt="Sigmoid Function" width="500">
</p>


**ğŸ§  Giáº£i thÃ­ch:**

- Khi $z \to +\infty \Rightarrow e^{-z} \to 0 \Rightarrow \sigma(z) \to 1$  
- Khi $z = 0 \Rightarrow \sigma(0) = \frac{1}{2} = 0.5$  
- Khi $z \to -\infty \Rightarrow e^{-z} \to +\infty \Rightarrow \sigma(z) \to 0$

VÃ¬ váº­y, **sigmoid nÃ©n toÃ n bá»™ giÃ¡ trá»‹ thá»±c cá»§a $z$ vÃ o khoáº£ng (0, 1)** â€” chÃ­nh xÃ¡c lÃ  miá»n giÃ¡ trá»‹ cáº§n thiáº¿t Ä‘á»ƒ mÃ´ táº£ xÃ¡c suáº¥t.

## ğŸ¯ Pháº§n 3 â€“ HÃ m máº¥t mÃ¡t: MSE hay BCE?

**1. Thá»­ dÃ¹ng MSE (Mean Squared Error)**

ÄÃ¢y lÃ  **hÃ m máº¥t mÃ¡t quen thuá»™c** cá»§a há»“i quy tuyáº¿n tÃ­nh:

$$
L_{\text{MSE}}(\hat{y}, y) = (\hat{y} - y)^2
$$

Khi Ã¡p dá»¥ng cho **Logistic Regression**:

$$
\hat{y} = \sigma(w x + b) = \frac{1}{1 + e^{-(w x + b)}}
$$

â†’ Ta cÃ³ thá»ƒ tÃ­nh Ä‘Æ°á»£c Ä‘áº¡o hÃ m vÃ  cáº­p nháº­t tham sá»‘ báº±ng **Gradient Descent**.

**âš ï¸ Tuy nhiÃªn, MSE *khÃ´ng phÃ¹ há»£p* cho Logistic Regression vÃ¬:**

**HÃ m sigmoid phi tuyáº¿n â†’ Ä‘á»™ dá»‘c phá»©c táº¡p**

- Káº¿t há»£p **sigmoid + MSE** lÃ m Ä‘á»“ thá»‹ hÃ m máº¥t mÃ¡t trá»Ÿ nÃªn **khÃ´ng lá»“i (non-convex)**.  
- **Gradient** cÃ³ thá»ƒ bá»‹ ráº¥t nhá» (*hiá»‡n tÆ°á»£ng vanishing gradient*) khi $\hat{y}$ tiáº¿n gáº§n 0 hoáº·c 1.  
  â†’ MÃ´ hÃ¬nh há»c **ráº¥t cháº­m**, dá»… **máº¯c káº¹t á»Ÿ cá»±c tiá»ƒu cá»¥c bá»™**.

<p align="center">
  <img src="{{ 'assets/module6-week1/MSE-Sigmoid.png' | absolute_url }}"
       alt="Káº¿t há»£p Sigmoid vÃ  MSE" width="500">
</p>

ğŸ‘‰ ChÃ­nh vÃ¬ sá»± nguyÃªn hiá»ƒm nÃ y, má»™t hÃ m máº¥t mÃ¡t Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t káº¿ riÃªng cho bÃ i toÃ¡n phÃ¢n loáº¡i xÃ¡c suáº¥t, Ä‘Ã³ lÃ  Binary Cross-Entropy (BCE), hay cÃ²n gá»i lÃ  Log Loss.

<p align="center">
  <img src="{{ 'assets/module6-week1/BCE-Sigmoid.png' | absolute_url }}"
       alt="Káº¿t há»£p Sigmoid vÃ  BCE" width="500">
</p>


**2. BCE (Binary Cross-Entropy)**

**a) CÃ´ng thá»©c tá»•ng quÃ¡t**

$$
L_{\text{BCE}}(\hat{y}, y) = -\Big[\, y \log(\hat{y}) + (1 - y)\, \log(1 - \hat{y}) \,\Big]
$$

**b) Giáº£i thÃ­ch chi tiáº¿t cÃ¡c thÃ nh pháº§n trong cÃ´ng thá»©c**

| KÃ½ hiá»‡u | Ã nghÄ©a | Vai trÃ² |
|----------|----------|----------|
| $y$ | **GiÃ¡ trá»‹ tháº­t (label)** cá»§a máº«u dá»¯ liá»‡u, chá»‰ nháº­n hai giÃ¡ trá»‹: 0 hoáº·c 1. | Cho biáº¿t máº«u thuá»™c lá»›p nÃ o (lá»›p dÆ°Æ¡ng = 1, lá»›p Ã¢m = 0). |
| $\hat{y}$ | **XÃ¡c suáº¥t dá»± Ä‘oÃ¡n** mÃ  mÃ´ hÃ¬nh logistic regression sinh ra, Ä‘Æ°á»£c tÃ­nh báº±ng hÃ m sigmoid:  $\hat{y} = \frac{1}{1 + e^{-(wx + b)}}$ | Biá»ƒu diá»…n Ä‘á»™ tin cáº­y cá»§a mÃ´ hÃ¬nh ráº±ng máº«u nÃ y thuá»™c lá»›p 1. |
| $\log(\hat{y})$ | Logarithm tá»± nhiÃªn cá»§a xÃ¡c suáº¥t dá»± Ä‘oÃ¡n lá»›p 1 | DÃ¹ng Ä‘á»ƒ Ä‘o â€œ**má»©c Ä‘á»™ báº¥t ngá»**â€ (*information content*) khi mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n Ä‘Ãºng lá»›p 1. |
| $\log(1 - \hat{y})$ | Logarithm tá»± nhiÃªn cá»§a xÃ¡c suáº¥t dá»± Ä‘oÃ¡n lá»›p 0 | Äo â€œ**má»©c Ä‘á»™ báº¥t ngá»**â€ khi mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n Ä‘Ãºng lá»›p 0. |
| Dáº¥u trá»« (-) bÃªn ngoÃ i | Äáº£o ngÆ°á»£c dáº¥u vÃ¬ log-likelihood cÃ³ giÃ¡ trá»‹ Ã¢m | GiÃºp hÃ m loss trá»Ÿ thÃ nh giÃ¡ trá»‹ dÆ°Æ¡ng cáº§n **minimize**. |

**c) Ã nghÄ©a trá»±c quan**

CÃ´ng thá»©c trÃªn cÃ³ thá»ƒ hiá»ƒu lÃ :

- **Náº¿u máº«u tháº­t \( y = 1 \)**, cÃ²n láº¡i:

$$
L = -\log(\hat{y})
$$

â†’ MÃ´ hÃ¬nh sáº½ bá»‹ **pháº¡t máº¡nh** khi \(\hat{y}\) nhá» (vÃ¬ \(\log(\hat{y})\) ráº¥t Ã¢m khi \(\hat{y}\) gáº§n 0).  
â†’ Äiá»u nÃ y **khuyáº¿n khÃ­ch mÃ´ hÃ¬nh Ä‘áº©y \(\hat{y}\) gáº§n 1\)** cho máº«u thuá»™c lá»›p 1.

---

- **Náº¿u máº«u tháº­t \( y = 0 \)**, cÃ²n láº¡i:

$$
L = -\log(1 - \hat{y})
$$

â†’ MÃ´ hÃ¬nh bá»‹ **pháº¡t máº¡nh** khi \(\hat{y}\) lá»›n (tá»©c dá»± Ä‘oÃ¡n lá»›p 1 sai).  
â†’ **Khuyáº¿n khÃ­ch mÃ´ hÃ¬nh Ä‘áº©y \(\hat{y}\) gáº§n 0\)** cho máº«u thuá»™c lá»›p 0.

---



![BCE + Sigmoid]({{ '/assets/module6-week1/bce-sigmoid.png' | relative_url }})

## ğŸ› ï¸ Pháº§n 4 â€“ Há»c tham sá»‘ báº±ng Gradient Descent

Vá»›i 1 máº«u $(\mathbf{x}, y)$, Ä‘áº¡o hÃ m theo $\mathbf{w}$ cá»§a BCE:
$$
\nabla_{\mathbf{w}} L = (\hat{p}-y)\,\mathbf{x},\qquad
\frac{\partial L}{\partial b} = \hat{p}-y
$$
Cáº­p nháº­t:
$$
\mathbf{w} \leftarrow \mathbf{w} - \eta \, (\hat{p}-y)\mathbf{x},\qquad
b \leftarrow b - \eta \, (\hat{p}-y)
$$
($\eta$: learning rate)

---

## ğŸ§­ Pháº§n 5 â€“ Decision Boundary & Prob. View

- BiÃªn quyáº¿t Ä‘á»‹nh khi $\hat{p}=0.5 \iff \mathbf{w}^\top \mathbf{x}+b=0$ â‡’ lÃ  **siÃªu pháº³ng tuyáº¿n tÃ­nh**.
- NhÃ¬n theo **log-odds**:
$$
\log\frac{\hat{p}}{1-\hat{p}} = \mathbf{w}^\top \mathbf{x} + b
$$
â†’ Logistic Regression chÃ­nh lÃ  **há»“i quy tuyáº¿n tÃ­nh trÃªn log-odds**.

---

## ğŸ“š TÃ i nguyÃªn
- [Coding cho bÃ i giáº£ng cá»§a tháº§y Quang-Vinh Dinh (Google Drive)](https://â€¦)  
- [Wikipedia â€“ Logistic Function](https://en.wikipedia.org/wiki/Logistic_function)

---

## ğŸ“˜ TÃ i Liá»‡u há»c má»Ÿ rá»™ng â€“ Logistic Regression & Vectorization

<div style="border:1px solid #dcdcdc; border-radius:12px; padding:20px; background:#fdfdfd; box-shadow:2px 2px 8px rgba(0,0,0,0.05);">
  <h4 style="margin-top:0;">ğŸ“„ Report MD06W01_v1.pdf</h4>
  <p style="margin:6px 0;">
    Báº£n tÃ³m táº¯t 4 trang A4 ná»™i dung tuáº§n 1 â€“ <b>Logistic Regression</b>, 
    <i>Vectorization and Application</i>.
  </p>
  <p style="margin:6px 0;">
    <a href="{{ site.baseurl }}/assets/module6-week1/Report_MD06W01_v1.pdf" 
       target="_blank" rel="noopener"
       style="background:#e6f0ff; padding:8px 16px; border-radius:8px; text-decoration:none; font-weight:600; color:#0044cc;">
      ğŸ”— Xem hoáº·c táº£i PDF
    </a>
  </p>
</div>

---


## ğŸ§© Mini Quiz (vui há»c ğŸ²)  
1ï¸âƒ£ Logistic Regression dá»± Ä‘oÃ¡n **cÃ¡i gÃ¬**? â†’ **XÃ¡c suáº¥t** $P(y=1\mid \mathbf{x})$ trong $[0,1]$.  

2ï¸âƒ£ VÃ¬ sao chá»n **BCE** thay vÃ¬ **MSE**? â†’ PhÃ¹ há»£p xÃ¡c suáº¥t + gradient â€œkhá»eâ€ hÆ¡n.  

3ï¸âƒ£ Khi nÃ o $\hat{y}=1$? â†’ Khi $\sigma(\mathbf{w}^\top\mathbf{x}+b)\ge0.5$ (tÆ°Æ¡ng Ä‘Æ°Æ¡ng $\mathbf{w}^\top\mathbf{x}+b\ge0$). 

4ï¸âƒ£ Sigmoid function Ä‘áº£m báº£o Ä‘iá»u gÃ¬?  
   â˜‘ Äáº§u ra luÃ´n trong [0,1]  
   â˜ Tá»‘c Ä‘á»™ há»c nhanh hÆ¡n  

---

ğŸ’¡ *Háº¹n gáº·p á»Ÿ bÃ i giáº£ng tiáº¿p theo cá»§a tuáº§n 1 â€“ Khi chÃºng ta dive sÃ¢u hÆ¡n vÃ o Multi-feature Logistic Regression nhÃ©!* ğŸš€


---

---
layout: post
title: "ğŸ’œ Advanced Logistic Regression â€” Vectorization & Application (AIO 2025)"
date: 2025-11-08
---

## TÃ³m táº¯t bÃ i giáº£ng ngÃ y thá»© SÃ¡u ngÃ y 07/11/2025

## ğŸŒ¸ Giá»›i thiá»‡u

Tuáº§n nÃ y, chÃºng ta bÆ°á»›c sang **Logistic Regression nÃ¢ng cao**, vá»›i hai trá»ng tÃ¢m:
1. LÃ m chá»§ **vectorization** â€“ biáº¿n toÃ n bá»™ phÃ©p tÃ­nh thÃ nh dáº¡ng ma tráº­n Ä‘á»ƒ tÄƒng tá»‘c huáº¥n luyá»‡n.
2. Hiá»ƒu sÃ¢u **gradient descent & binary cross-entropy loss** qua gÃ³c nhÃ¬n Ä‘áº¡i sá»‘ tuyáº¿n tÃ­nh.

Má»¥c tiÃªu cuá»‘i cÃ¹ng: viáº¿t láº¡i **toÃ n bá»™ Logistic Regression pipeline** dÆ°á»›i dáº¡ng **vector hÃ³a** â€” nhanh hÆ¡n, ngáº¯n gá»n hÆ¡n, vÃ  dá»… má»Ÿ rá»™ng sang neural network sau nÃ y. ğŸ’»

---

## ğŸŒ¿ Pháº§n 1 â€“ Logistic Regression cÆ¡ báº£n

### ğŸ’ Báº£n cháº¥t mÃ´ hÃ¬nh

Máº·c dÃ¹ tÃªn gá»i lÃ  **"Regression"**, nhÆ°ng Logistic Regression thá»±c cháº¥t lÃ  **má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i nhá»‹ phÃ¢n (binary classification)**, khÃ´ng pháº£i há»“i quy tuyáº¿n tÃ­nh thÃ´ng thÆ°á»ng.  
Thay vÃ¬ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c, ta dá»± Ä‘oÃ¡n **xÃ¡c suáº¥t Ä‘áº§u ra thuá»™c lá»›p 1**, kÃ½ hiá»‡u:

$$
P(y=1|x) = \hat{y} = \sigma(z)
$$

vá»›i:
$$
z = w^T x + b
$$

Trong Ä‘Ã³:  
- $w$ lÃ  vector trá»ng sá»‘ (weights)  
- $b$ lÃ  há»‡ sá»‘ chá»‡ch (bias)  
- $x$ lÃ  vector Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o (feature vector)  
- $\sigma(z)$ lÃ  **hÃ m sigmoid**, giÃºp Ã¡nh xáº¡ má»i giÃ¡ trá»‹ thá»±c vá» khoáº£ng (0,1), phÃ¹ há»£p Ä‘á»ƒ diá»…n giáº£i nhÆ° xÃ¡c suáº¥t.

---

### ğŸ§­ HÃ m sigmoid â€“ Cá»­a ngÃµ giá»¯a tuyáº¿n tÃ­nh vÃ  xÃ¡c suáº¥t

HÃ m sigmoid Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

Äáº·c tÃ­nh:
- Khi $z \to +\infty \Rightarrow \sigma(z) \to 1$
- Khi $z \to -\infty \Rightarrow \sigma(z) \to 0$
- Táº¡i $z = 0 \Rightarrow \sigma(0) = 0.5$

â†’ ÄÃ¢y lÃ  **Ä‘iá»ƒm trung gian** biá»ƒu thá»‹ ranh giá»›i quyáº¿t Ä‘á»‹nh giá»¯a hai lá»›p (0 vÃ  1).

HÃ m sigmoid giÃºp mÃ´ hÃ¬nh **chuyá»ƒn hÃ³a Ä‘áº§u ra tuyáº¿n tÃ­nh thÃ nh xÃ¡c suáº¥t** â€“ má»™t bÆ°á»›c quan trá»ng trong má»i mÃ´ hÃ¬nh phÃ¢n loáº¡i nhá»‹ phÃ¢n.

---

### ğŸ“ˆ Quy táº¯c phÃ¢n loáº¡i

Dá»±a trÃªn xÃ¡c suáº¥t \(\hat{y}\) mÃ  mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n, ta cÃ³ quy táº¯c:

- Náº¿u $\hat{y} \ge 0.5 \Rightarrow y_{\text{pred}} = 1$
- Náº¿u $\hat{y} < 0.5 \Rightarrow y_{\text{pred}} = 0$

VÃ­ dá»¥:  
Náº¿u mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n xÃ¡c suáº¥t 0.83 â†’ ta gÃ¡n nhÃ£n â€œ1â€ (thuá»™c lá»›p dÆ°Æ¡ng).  
Náº¿u xÃ¡c suáº¥t 0.24 â†’ ta gÃ¡n nhÃ£n â€œ0â€ (thuá»™c lá»›p Ã¢m).

---

### âš™ï¸ HÃ m máº¥t mÃ¡t â€“ Binary Cross Entropy (Log Loss)

Äá»ƒ huáº¥n luyá»‡n Logistic Regression, ta cáº§n má»™t hÃ m máº¥t mÃ¡t Ä‘o Ä‘á»™ sai lá»‡ch giá»¯a xÃ¡c suáº¥t dá»± Ä‘oÃ¡n \(\hat{y}\) vÃ  nhÃ£n thá»±c \(y\):

$$
L(\hat{y}, y) = -[y \log(\hat{y}) + (1 - y)\log(1 - \hat{y})]
$$

Giáº£i thÃ­ch:
- Náº¿u nhÃ£n tháº­t $y = 1$: hÃ m loss chá»‰ giá»¯ láº¡i $-\log(\hat{y})$ â†’ pháº¡t náº·ng khi mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n xÃ¡c suáº¥t tháº¥p.  
- Náº¿u nhÃ£n tháº­t $y = 0$: hÃ m loss chá»‰ giá»¯ láº¡i $-\log(1 - \hat{y})$ â†’ pháº¡t náº·ng khi mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n xÃ¡c suáº¥t cao.

Khi huáº¥n luyá»‡n trÃªn nhiá»u máº«u (N máº«u):
$$
J(w, b) = \frac{1}{N} \sum_{i=1}^{N} L(\hat{y}^{(i)}, y^{(i)})
$$

---

### ğŸ”§ Cáº­p nháº­t tham sá»‘ â€“ Gradient Descent cÆ¡ báº£n

Má»¥c tiÃªu: tÃ¬m bá»™ tham sá»‘ \(w, b\) sao cho hÃ m máº¥t mÃ¡t \(J(w,b)\) nhá» nháº¥t.  
Ta dÃ¹ng **phÆ°Æ¡ng phÃ¡p Gradient Descent**, cáº­p nháº­t theo hÆ°á»›ng ngÆ°á»£c chiá»u gradient:

$$
w := w - \eta \frac{\partial J}{\partial w}, \qquad b := b - \eta \frac{\partial J}{\partial b}
$$

vá»›i \( \eta \) lÃ  **learning rate** (tá»‘c Ä‘á»™ há»c).

CÃ´ng thá»©c Ä‘áº¡o hÃ m riÃªng cho má»™t máº«u:

$$
\frac{\partial L}{\partial w} = x(\hat{y} - y), \qquad
\frac{\partial L}{\partial b} = (\hat{y} - y)
$$

â†’ ÄÃ¢y lÃ  cÃ´ng thá»©c cá»‘t lÃµi Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh Logistic Regression trÃªn tá»«ng máº«u dá»¯ liá»‡u.

---

### ğŸ’¡ Minh há»a trá»±c quan: quy trÃ¬nh há»c vá»›i má»™t máº«u

1. **Chá»n má»™t máº«u dá»¯ liá»‡u** $(x, y)$   
2. **TÃ­nh Ä‘áº§u ra dá»± Ä‘oÃ¡n:**  
   $\hat{y} = \sigma(w^T x + b)$
3. **TÃ­nh máº¥t mÃ¡t:**  
   $L = -[y \log(\hat{y}) + (1-y)\log(1-\hat{y})]$  
4. **TÃ­nh gradient:**  
   $dW = x(\hat{y} - y), \quad db = (\hat{y} - y)$  
5. **Cáº­p nháº­t tham sá»‘:**  
   $w := w - \eta dW, \quad b := b - \eta db$  

Quy trÃ¬nh nÃ y Ä‘Æ°á»£c láº·p láº¡i cho Ä‘áº¿n khi mÃ´ hÃ¬nh há»™i tá»¥ â€“ tá»©c khi máº¥t mÃ¡t giáº£m vá» má»©c á»•n Ä‘á»‹nh.

---

### ğŸ§© Ã nghÄ©a hÃ¬nh há»c

- ÄÆ°á»ng **decision boundary** Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi $w^T x + b = 0$.  
- á» ranh giá»›i nÃ y, mÃ´ hÃ¬nh cho xÃ¡c suáº¥t $\hat{y} = 0.5$.  
- Hai phÃ­a cá»§a Ä‘Æ°á»ng nÃ y tÆ°Æ¡ng á»©ng vá»›i hai lá»›p (0 vÃ  1).  
- Má»—i láº§n cáº­p nháº­t $w, b$, mÃ´ hÃ¬nh â€œxoayâ€ vÃ  â€œtá»‹nh tiáº¿nâ€ ranh giá»›i Ä‘á»ƒ phÃ¢n chia dá»¯ liá»‡u tá»‘t hÆ¡n.

---

### âš›ï¸ Má»Ÿ rá»™ng: má»‘i liÃªn há»‡ vá»›i hÃ m máº¥t mÃ¡t cá»§a Linear Regression

Linear Regression dÃ¹ng **Mean Squared Error (MSE)**, cÃ²n Logistic Regression dÃ¹ng **Binary Cross-Entropy (Log Loss)**.  
Tuy khÃ¡c nhau vá» cÃ´ng thá»©c, nhÆ°ng cáº£ hai Ä‘á»u cÃ³ cÃ¹ng má»¥c tiÃªu: giáº£m sai sá»‘ dá»± Ä‘oÃ¡n so vá»›i nhÃ£n tháº­t.  
Sá»± khÃ¡c biá»‡t chá»§ yáº¿u Ä‘áº¿n tá»«:
- MSE giáº£ Ä‘á»‹nh Ä‘áº§u ra lÃ  giÃ¡ trá»‹ thá»±c, cÃ²n Log Loss giáº£ Ä‘á»‹nh Ä‘áº§u ra lÃ  xÃ¡c suáº¥t.  
- MSE khÃ´ng phÃ¹ há»£p khi \(y\) lÃ  nhá»‹ phÃ¢n, vÃ¬ mÃ´ hÃ¬nh cÃ³ thá»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ ngoÃ i [0,1].

---

### ğŸŒˆ Tá»•ng káº¿t pháº§n 1

- Logistic Regression lÃ  **mÃ´ hÃ¬nh phÃ¢n loáº¡i xÃ¡c suáº¥t nhá»‹ phÃ¢n**, khÃ´ng pháº£i há»“i quy.  
- Sá»­ dá»¥ng **hÃ m sigmoid** Ä‘á»ƒ chuyá»ƒn Ä‘áº§u ra tuyáº¿n tÃ­nh thÃ nh xÃ¡c suáº¥t.  
- DÃ¹ng **Binary Cross-Entropy loss** Ä‘á»ƒ Ä‘o sai lá»‡ch.  
- Huáº¥n luyá»‡n báº±ng **Gradient Descent**, cáº­p nháº­t \(w, b\) qua Ä‘áº¡o hÃ m riÃªng.  
- CÃ³ thá»ƒ má»Ÿ rá»™ng sang **vectorization**, **mini-batch**, vÃ  **deep learning layers** sau nÃ y.

---

ğŸ“˜ *Nguá»“n: Advanced Logistic Regression Slides â€“ Quang-Vinh Äinh, PhD (2025)* :contentReference[oaicite:1]{index=1}

---

## ğŸŒ¸ Pháº§n 2 â€“ Vectorization cho máº«u Ä‘Æ¡n, mini-batch vÃ  toÃ n bá»™ táº­p dá»¯ liá»‡u

### ğŸŒ¿ 1ï¸âƒ£ Tá»« 1 máº«u Ä‘Æ¡n â†’ vector hÃ³a

á» pháº§n 1 ta tÃ­nh toÃ¡n vá»›i má»™t máº«u $(x, y)$ duy nháº¥t:
$$
z = w^T x + b,\qquad 
\hat y = \sigma(z) = \frac{1}{1+e^{-z}}
\]
\[
\nabla_w L = x(\hat y - y),\quad 
\nabla_b L = \hat y - y
$$  

Vector hÃ³a giÃºp gá»™p cÃ¡c phÃ©p nhÃ¢n, cá»™ng vÃ  Ä‘áº¡o hÃ m vÃ o **cÃ¡c phÃ©p toÃ¡n ma tráº­n** Ä‘á»ƒ tÄƒng tá»‘c vÃ  giáº£m sai sá»‘ láº­p trÃ¬nh.  
Ta gá»™p \(b\) vÃ o vector tham sá»‘ \(\theta = [b, w_1,\dots,w_n]^T\) vÃ  má»Ÿ rá»™ng Ä‘áº·c trÆ°ng $x \to x' = [1,x_1,\dots,x_n]^T$.  

Khi Ä‘Ã³:
$$
z = \theta^T x',\qquad 
\hat y = \sigma(z)
\]
\[
\nabla_\theta L = x'(\hat y - y)
\]
\[
\theta := \theta - \eta\nabla_\theta L
$$
CÃ´ng thá»©c nÃ y giá»¯ nguyÃªn cho má»i máº«u, vÃ  lÃ  háº¡t nhÃ¢n Ä‘á»ƒ chuyá»ƒn tá»« **1 máº«u â†’ m máº«u**.

---

### ğŸª· 2ï¸âƒ£ Vectorization cho *m-samples* (mini-batch)

Khi huáº¥n luyá»‡n trÃªn m máº«u Ä‘á»“ng thá»i $(x^{(1)},â€¦,x^{(m)})$, ta sáº¯p chÃºng thÃ nh ma tráº­n:
$$
X = 
\begin{bmatrix}
1 & x^{(1)}_1 & â€¦ & x^{(1)}_n \\
1 & x^{(2)}_1 & â€¦ & x^{(2)}_n \\
â‹® & â‹® & â‹± & â‹® \\
1 & x^{(m)}_1 & â€¦ & x^{(m)}_n
\end{bmatrix}_{m\times(n+1)},\quad 
\theta = [b,w_1,â€¦,w_n]^T
$$  

#### ğŸ”¹ BÆ°á»›c 1 â€“ TÃ­nh Ä‘áº§u ra
$$
z = X \theta,\qquad 
\hat y = \sigma(z)
$$

#### ğŸ”¹ BÆ°á»›c 2 â€“ HÃ m máº¥t mÃ¡t toÃ n mini-batch
$$
L(\hat y,y)
 = -\frac1m\sum_{i=1}^m 
\big[y^{(i)}\log\hat y^{(i)}+(1-y^{(i)})\log(1-\hat y^{(i)})\big]
$$

#### ğŸ”¹ BÆ°á»›c 3 â€“ Äáº¡o hÃ m vector hÃ³a
$$
\nabla_\theta L = \frac1m X^T(\hat y - y)
$$

#### ğŸ”¹ BÆ°á»›c 4 â€“ Cáº­p nháº­t tham sá»‘
$$
\theta := \theta - \eta\nabla_\theta L
$$

---

### ğŸŒ¼ 3ï¸âƒ£ Ã nghÄ©a hÃ¬nh há»c vÃ  tá»‘i Æ°u hÃ³a

- Má»—i dÃ²ng cá»§a $X$ lÃ  má»™t vector Ä‘áº·c trÆ°ng cá»§a má»™t máº«u.  
- TÃ­ch $X \theta$ lÃ  tá»•ng há»£p tuyáº¿n tÃ­nh â†’ tÃ­nh Ä‘á»“ng thá»i nhiá»u máº«u.  
- Gradient $\frac1m X^T(\hat y - y)$ lÃ  trung bÃ¬nh hÆ°á»›ng sai sá»‘ â†’ cáº­p nháº­t tham sá»‘ theo hÆ°á»›ng giáº£m máº¥t mÃ¡t toÃ n batch.  

---

### ğŸª 4ï¸âƒ£ VÃ­ dá»¥ cá»¥ thá»ƒ (tá»« slides)

Vá»›i $m = 2$:
$$
X = 
\begin{bmatrix}
1 & 1.5 & 0.2\\
1 & 4.1 & 1.3
\end{bmatrix},
\quad 
y = \begin{bmatrix}0\\1\end{bmatrix},
\quad 
\theta = 
\begin{bmatrix}
0.1\\0.5\\-0.1
\end{bmatrix}
$$

TÃ­nh:
$$
z = X\theta = 
\begin{bmatrix}
0.83\\2.02
\end{bmatrix},\quad 
\hat y = \sigma(z) = 
\begin{bmatrix}
0.6963\\0.8828
\end{bmatrix}
$$

Máº¥t mÃ¡t:
$$
L = -\frac12
\big[\log(1-0.6963)+\log(0.8828)\big]
â‰ˆ 0.65815
$$

Gradient:
$$
\nabla_\theta L = \frac12 X^T(\hat y - y)
=
\begin{bmatrix}
0.2896\\0.2822\\-0.0064
\end{bmatrix}
$$

Cáº­p nháº­t tham sá»‘:
$$
\theta_{new} = \theta - \eta\nabla_\theta L
\Rightarrow 
\begin{bmatrix}
0.0971\\0.4971\\-0.099
\end{bmatrix}
\quad(\eta = 0.01)
$$  
ğŸ“Š ÄÃ¢y chÃ­nh lÃ  vector hÃ³a phiÃªn báº£n **mini-batch (m = 2)**.

---

### ğŸŒº 5ï¸âƒ£ Vectorization toÃ n bá»™ táº­p dá»¯ liá»‡u (*Batch Gradient Descent*)

Khi dá»¯ liá»‡u gá»“m \(N\) máº«u, ta tá»•ng quÃ¡t hÃ³a:
$$
Z = X\theta,\qquad 
\hat Y = \sigma(Z)
\]
\[
L(\theta)
 = -\frac1N\sum_{i=1}^N[y^{(i)}\log\hat y^{(i)}+(1-y^{(i)})\log(1-\hat y^{(i)})]
\]
\[
\nabla_\theta L = \frac1N X^T(\hat Y - Y)
\]
\[
\theta := \theta - \eta\nabla_\theta L
$$

VÃ­ dá»¥ trong slides:
$$
X =
\begin{bmatrix}
1 & 1.4 & 0.2\\
1 & 1.5 & 0.2\\
1 & 3.0 & 1.1\\
1 & 4.1 & 1.3
\end{bmatrix},
\quad 
y =
\begin{bmatrix}
0\\0\\1\\1
\end{bmatrix}
\]
\[
Z = X\theta =
\begin{bmatrix}
0.78\\0.83\\1.49\\2.02
\end{bmatrix}
\Rightarrow 
\hat y =
\begin{bmatrix}
0.6856\\0.6963\\0.8160\\0.8828
\end{bmatrix}
\]
\[
L â‰ˆ 0.6691,\qquad 
\nabla_\theta L =
\begin{bmatrix}
0.2702\\0.2431\\-0.019
\end{bmatrix}
\Rightarrow 
\theta_{new} =
\begin{bmatrix}
0.0971\\0.4971\\-0.099
\end{bmatrix}
$$  
ğŸ“˜ ToÃ n bá»™ cÃ¡c máº«u Ä‘Æ°á»£c xá»­ lÃ½ Ä‘á»“ng thá»i, giÃºp **á»•n Ä‘á»‹nh gradient vÃ  há»™i tá»¥ nhanh**.

---

### ğŸŒ¸ 6ï¸âƒ£ Tá»•ng há»£p so sÃ¡nh ba cáº¥p Ä‘á»™ vector hÃ³a

| Cáº¥p Ä‘á»™ | KÃ½ hiá»‡u | Quy mÃ´ | CÃ´ng thá»©c gradient | Äáº·c Ä‘iá»ƒm |
|:--:|:--|:--|:--|:--|
| 1ï¸âƒ£ | One sample | 1 máº«u | \(\nabla_\theta L = x(\hat y-y)\) | Cáº­p nháº­t cháº­m, nhiá»…u lá»›n |
| 2ï¸âƒ£ | Mini-batch | m máº«u | \(\nabla_\theta L = \frac1m X^T(\hat y-y)\) | CÃ¢n báº±ng tá»‘c Ä‘á»™ vÃ  Ä‘á»™ chÃ­nh xÃ¡c |
| 3ï¸âƒ£ | Full batch | N máº«u | \(\nabla_\theta L = \frac1N X^T(\hat y-y)\) | á»”n Ä‘á»‹nh nháº¥t nhÆ°ng tá»‘n bá»™ nhá»› |

---

### ğŸª¶ 7ï¸âƒ£ Ã nghÄ©a tá»‘i Æ°u hÃ³a vÃ  liÃªn há»‡ thá»±c tiá»…n

- **Batch gradient descent** phÃ¹ há»£p cho dataset nhá» vÃ  há»c toÃ n bá»™ má»™t láº§n.  
- **Mini-batch** lÃ  chuáº©n trong Deep Learning â€“ káº¿t há»£p hiá»‡u nÄƒng GPU vÃ  Ä‘á»™ á»•n Ä‘á»‹nh há»™i tá»¥.  
- **Stochastic (one sample)** phÃ¹ há»£p dá»¯ liá»‡u stream hoáº·c online learning.

---

### ğŸ’ 8ï¸âƒ£ MÃ£ Python vector hÃ³a (Numpy demo)

```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def vectorized_gradient(X, y, theta):
    m = X.shape[0]
    y_hat = sigmoid(X @ theta)
    grad = (1/m) * (X.T @ (y_hat - y))
    return grad

def train(X, y, lr=0.1, epochs=1000):
    theta = np.zeros(X.shape[1])
    for i in range(epochs):
        grad = vectorized_gradient(X, y, theta)
        theta -= lr * grad
        if i % 200 == 0:
            loss = -np.mean(y*np.log(sigmoid(X@theta)) + (1-y)*np.log(1-sigmoid(X@theta)))
            print(f"Iter {i:4d} | Loss={loss:.6f}")
    return theta
```

---

## ğŸ’œ Pháº§n 3 â€“ HÃ m kÃ­ch hoáº¡t (Activation Functions) vÃ  HÃ nh vi Gradient

---

### ğŸŒ· 1ï¸âƒ£ Táº¡i sao cáº§n hÃ m kÃ­ch hoáº¡t?

Trong mÃ´ hÃ¬nh Logistic Regression hoáº·c máº¡ng nÆ¡-ron, pháº§n tuyáº¿n tÃ­nh $z = w^T x + b$ chá»‰ táº¡o ra **Ä‘Æ°á»ng/hyperplane phÃ¢n chia tuyáº¿n tÃ­nh**.  
Náº¿u ta giá»¯ nguyÃªn $z$, mÃ´ hÃ¬nh chá»‰ há»c Ä‘Æ°á»£c quan há»‡ **tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n** giá»¯a Ä‘áº·c trÆ°ng vÃ  nhÃ£n.  

ğŸ‘‰ HÃ m kÃ­ch hoáº¡t (activation function) táº¡o **phi tuyáº¿n tÃ­nh (non-linearity)**, cho phÃ©p mÃ´ hÃ¬nh mÃ´ táº£ má»‘i quan há»‡ phá»©c táº¡p hÆ¡n, mÃ´ phá»ng cÃ¡ch â€œkÃ­ch hoáº¡t neuronâ€ trong nÃ£o ngÆ°á»i.

Má»—i hÃ m kÃ­ch hoáº¡t Ã¡nh xáº¡ Ä‘áº§u ra \(z\) sang má»™t khoáº£ng giÃ¡ trá»‹ xÃ¡c Ä‘á»‹nh vÃ  Ä‘iá»u khiá»ƒn cÃ¡ch gradient lan truyá»n trong quÃ¡ trÃ¬nh há»c.

---

### ğŸŒ¸ 2ï¸âƒ£ HÃ m sigmoid â€“ ná»n táº£ng cá»§a Logistic Regression

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

#### âœ¨ Äáº·c Ä‘iá»ƒm:
- Äáº§u ra luÃ´n náº±m trong khoáº£ng \((0,1)\) â†’ dá»… diá»…n giáº£i nhÆ° xÃ¡c suáº¥t.  
- Äáº¡o hÃ m:  
  $$
  \sigma'(z) = \sigma(z) \big(1 - \sigma(z)\big)
  $$
- Khi $z$ ráº¥t lá»›n hoáº·c ráº¥t nhá», $\sigma'(z)$ tiáº¿n gáº§n 0 â†’ gÃ¢y **hiá»‡n tÆ°á»£ng vanishing gradient**.

#### ğŸ¨ VÃ­ dá»¥ trá»±c quan:
- Náº¿u $z=0$ â†’ $\sigma(z)=0.5$  
- Náº¿u $z=5$ â†’ $\sigma(z)\approx0.993$  
- Náº¿u $z=-5$ â†’ $\sigma(z)\approx0.0067$

ğŸ‘‰ Äiá»u nÃ y giáº£i thÃ­ch táº¡i sao mÃ´ hÃ¬nh Logistic Regression â€œmá»mâ€ á»Ÿ giá»¯a (xÃ¡c suáº¥t thay Ä‘á»•i máº¡nh quanh $z=0$ vÃ  â€œbÃ£o hÃ²aâ€ á»Ÿ hai Ä‘áº§u.

---

### ğŸª· 3ï¸âƒ£ HÃ m tanh â€“ cáº£i tiáº¿n tá»« sigmoid

$$
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
$$

#### âœ¨ Äáº·c Ä‘iá»ƒm:
- Äáº§u ra trong khoáº£ng $(-1,1)$ thay vÃ¬ $(0,1)$.  
- Trung tÃ¢m á»Ÿ 0, nÃªn gradient lan truyá»n tá»‘t hÆ¡n (Ã­t bias hÆ¡n).  
- Äáº¡o hÃ m:  
  $$
  \tanh'(z) = 1 - \tanh^2(z)
  $$
- Tuy nhiÃªn, khi $|z|$ quÃ¡ lá»›n â†’ Ä‘áº¡o hÃ m váº«n gáº§n 0 â†’ váº«n cÃ³ hiá»‡n tÆ°á»£ng **vanishing gradient**.

#### ğŸ¯ So sÃ¡nh sigmoid vs tanh:
| Äáº·c tÃ­nh | Sigmoid | tanh |
|:--|:--|:--|
| Khoáº£ng giÃ¡ trá»‹ | (0,1) | (-1,1) |
| Trung tÃ¢m táº¡i 0 | âŒ | âœ… |
| Tá»‘c Ä‘á»™ há»™i tá»¥ | Cháº­m hÆ¡n | Nhanh hÆ¡n |
| Vanishing Gradient | CÃ³ | CÃ³ (nhÆ°ng nháº¹ hÆ¡n) |
| á»¨ng dá»¥ng chÃ­nh | Logistic Regression, Output layer | Hidden layers trong MLP |

---

### ğŸ§® 4ï¸âƒ£ Má»‘i liÃªn há»‡ giá»¯a Sigmoid vÃ  Gradient Descent

Vá»›i Logistic Regression:
$$
\hat y = \sigma(z) = \frac{1}{1+e^{-(w^Tx+b)}}
$$

Äáº¡o hÃ m theo $w$:
$$
\frac{\partial L}{\partial w} = ( \hat y - y )x
$$

Khi $\sigma'(z)$ ráº¥t nhá» (vÃ­ dá»¥ khi $|z| > 5\$, gradient gáº§n nhÆ° báº±ng 0 â†’ mÃ´ hÃ¬nh gáº§n nhÆ° **ngá»«ng há»c**.  
ÄÃ³ lÃ  lÃ½ do khi Ã¡p dá»¥ng sigmoid trong cÃ¡c máº¡ng sÃ¢u (Deep Neural Networks), gradient tá»« cÃ¡c táº§ng cuá»‘i sáº½ **â€œtan biáº¿nâ€ dáº§n vá» 0**, khiáº¿n viá»‡c huáº¥n luyá»‡n khÃ³ khÄƒn.

---

### ğŸ©µ 5ï¸âƒ£ So sÃ¡nh hÃ nh vi gradient qua biá»ƒu Ä‘á»“ (theo slides)

HÃ¬nh tá»« slides minh há»a:

| HÃ m | Miá»n hoáº¡t Ä‘á»™ng máº¡nh | Khu vá»±c gradient nhá» |
|:--|:--|:--|
| Sigmoid | quanh z = 0 | z < âˆ’4 hoáº·c z > 4 |
| tanh | âˆ’2 < z < 2 | |z| > 3 |
| ReLU | z > 0 | z < 0 (gradient = 0) |

Cá»¥ thá»ƒ:
- Sigmoid vÃ  tanh Ä‘á»u cÃ³ vÃ¹ng â€œbÃ£o hÃ²aâ€ á»Ÿ hai Ä‘áº§u â†’ gradient nhá», cáº­p nháº­t cháº­m.  
- ReLU (Rectified Linear Unit) kháº¯c phá»¥c nhÆ°á»£c Ä‘iá»ƒm Ä‘Ã³ báº±ng cÃ¡ch **giá»¯ gradient = 1 khi z > 0**.

---

### ğŸŒ¿ 6ï¸âƒ£ ReLU â€“ bÆ°á»›c ngoáº·t cho Deep Learning

$$
\text{ReLU}(z) = \max(0, z)
\]
\[
\text{ReLU}'(z) = 
\begin{cases}
1, & z > 0 \\
0, & z \le 0
\end{cases}
$$

#### âœ¨ Æ¯u Ä‘iá»ƒm:
- Gradient khÃ´ng bá»‹ bÃ£o hÃ²a khi \(z > 0\).  
- TÃ­nh toÃ¡n cá»±c nhanh (chá»‰ cáº§n so sÃ¡nh).  
- GiÃºp máº¡ng sÃ¢u há»c nhanh hÆ¡n nhiá»u so vá»›i sigmoid/tanh.

#### âš ï¸ NhÆ°á»£c Ä‘iá»ƒm:
- Khi \(z \le 0\), gradient = 0 â†’ neuron â€œcháº¿tâ€ (dead ReLU).  
- Cáº§n ká»¹ thuáº­t khá»Ÿi táº¡o vÃ  learning rate há»£p lÃ½.

---

### ğŸŒˆ 7ï¸âƒ£ HÃ m Leaky ReLU vÃ  cÃ¡c biáº¿n thá»ƒ

Äá»ƒ trÃ¡nh â€œdead ReLUâ€, ta cho phÃ©p gradient nhá» á»Ÿ vÃ¹ng Ã¢m:
$$
\text{LeakyReLU}(z) =
\begin{cases}
z, & z>0\\
\alpha z, & z\le0
\end{cases}
$$

vá»›i $\alpha \approx 0.01$.  
CÃ¡c biáº¿n thá»ƒ khÃ¡c: **ELU, GELU, Swish** (dÃ¹ng trong BERT/Transformer).

---

### ğŸ’« 8ï¸âƒ£ TÃ¡c Ä‘á»™ng cá»§a activation lÃªn loss surface

Khi dÃ¹ng sigmoid, loss surface (bá» máº·t hÃ m máº¥t mÃ¡t) cong hÆ¡n nhiá»u, dáº«n Ä‘áº¿n:
- Gradient nhá» â†’ bÆ°á»›c Ä‘i cá»§a optimizer ngáº¯n.  
- CÃ³ thá»ƒ máº¯c káº¹t trong vÃ¹ng báº±ng pháº³ng â†’ cháº­m há»™i tá»¥.

ReLU táº¡o loss surface â€œmá»m hÆ¡nâ€ (piecewise linear), cho phÃ©p gradient descent **di chuyá»ƒn nhanh vÃ  á»•n Ä‘á»‹nh hÆ¡n**.

---

### ğŸª 9ï¸âƒ£ Minh há»a code so sÃ¡nh cÃ¡c activation

```python
import numpy as np
import matplotlib.pyplot as plt

z = np.linspace(-6, 6, 400)
sigmoid = 1 / (1 + np.exp(-z))
tanh = np.tanh(z)
relu = np.maximum(0, z)
leaky = np.where(z > 0, z, 0.01*z)

plt.figure(figsize=(8,5))
plt.plot(z, sigmoid, label='Sigmoid')
plt.plot(z, tanh, label='tanh')
plt.plot(z, relu, label='ReLU')
plt.plot(z, leaky, label='Leaky ReLU')
plt.title("Activation Functions")
plt.legend()
plt.grid(True)
plt.show()
```

### ğŸª¶ 9ï¸âƒ£ TÃ¡c Ä‘á»™ng lÃªn loss surface

- **Sigmoid** â†’ loss surface cong, dá»… káº¹t á»Ÿ vÃ¹ng báº±ng pháº³ng.  
- **ReLU** â†’ loss surface â€œmá»mâ€, gradient descent di chuyá»ƒn nhanh hÆ¡n.  
- **tanh** â†’ cÃ¢n báº±ng giá»¯a hai loáº¡i.  

---

## ğŸ’œ ğŸ”Ÿ Káº¿t luáº­n pháº§n 3

| HÃ m | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | á»¨ng dá»¥ng chÃ­nh |
|:--|:--|:--|:--|
| **Sigmoid** | Diá»…n giáº£i xÃ¡c suáº¥t dá»…, trÆ¡n | Vanishing gradient | Output nhá»‹ phÃ¢n |
| **tanh** | Trung tÃ¢m táº¡i 0, há»™i tá»¥ nhanh | Vanishing gradient nháº¹ | Hidden layer |
| **ReLU** | TÃ­nh nhanh, khÃ´ng bÃ£o hÃ²a | Dead neurons | Hidden layer (chuáº©n) |
| **Leaky ReLU** | Giá»¯ gradient vÃ¹ng Ã¢m | Cáº§n Ä‘iá»u chá»‰nh Î± | CNN/Transformer |

Tá»« Logistic Regression Ä‘áº¿n Deep Learning, hÃ nh vi gradient lÃ  **trÃ¡i tim cá»§a quÃ¡ trÃ¬nh huáº¥n luyá»‡n**.  
Hiá»ƒu rÃµ cÃ¡c hÃ m kÃ­ch hoáº¡t giÃºp chá»n Ä‘Ãºng â€œcáº§u ná»‘i phi tuyáº¿nâ€ cho tá»«ng táº§ng máº¡ng â€” Ä‘áº£m báº£o mÃ´ hÃ¬nh **vá»«a há»™i tá»¥ nhanh, vá»«a tá»•ng quÃ¡t tá»‘t.**


---


---

## ğŸŒ¿ Pháº§n 4 â€“ Vectorization Summary & Gradient Formulas

> ğŸ¯ *Má»¥c tiÃªu:* Hiá»ƒu cÃ¡ch biá»ƒu diá»…n toÃ n bá»™ cÃ´ng thá»©c Logistic Regression dÆ°á»›i dáº¡ng **ma tráº­n (vectorized)**  
> Ä‘á»ƒ chuáº©n bá»‹ bÆ°á»›c sang **cÃ i Ä‘áº·t thá»±c táº¿ báº±ng Python (Pháº§n 4).**

---

### ğŸ’¡ 1ï¸âƒ£ TÆ° duy vector hÃ³a

Thay vÃ¬ tÃ­nh toÃ¡n tá»«ng máº«u dá»¯ liá»‡u báº±ng vÃ²ng `for`,  
ta gom táº¥t cáº£ dá»¯ liá»‡u Ä‘áº§u vÃ o thÃ nh **ma tráº­n \( X \)**:

$$
X =
\begin{bmatrix}
x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\
x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\
\vdots & \vdots & \ddots & \vdots \\
x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)}
\end{bmatrix}
$$

- $m$: sá»‘ lÆ°á»£ng máº«u  
- $n$: sá»‘ Ä‘áº·c trÆ°ng (features)  
- $X \in \mathbb{R}^{m \times n}$

Ta biá»ƒu diá»…n toÃ n bá»™ phÃ©p tÃ­nh nhÆ° sau:

$$
Z = XW + b
\]
\[
\hat{Y} = \sigma(Z)
$$

â†’ Trong Ä‘Ã³,  
- $W \in \mathbb{R}^{n \times 1}$: vector trá»ng sá»‘  
- $b \in \mathbb{R}$: bias  
- $\sigma(z) = \frac{1}{1 + e^{-z}}$: hÃ m kÃ­ch hoáº¡t sigmoid

---

### ğŸ§  2ï¸âƒ£ Forward Propagation (Lan truyá»n thuáº­n)

ÄÃ¢y lÃ  bÆ°á»›c tÃ­nh **dá»± Ä‘oÃ¡n Ä‘áº§u ra** dá»±a trÃªn Ä‘áº§u vÃ o \( X \):

$$
\hat{Y} = \sigma(XW + b)
$$

Káº¿t quáº£ $\hat{Y} \in \mathbb{R}^{m \times 1}$  
â†’ chá»©a xÃ¡c suáº¥t â€œmá»—i máº«u thuá»™c lá»›p 1â€.

---

### ğŸ’” 3ï¸âƒ£ Binary Cross-Entropy Loss (HÃ m máº¥t mÃ¡t)

HÃ m BCE Ä‘o Ä‘á»™ sai lá»‡ch giá»¯a dá»± Ä‘oÃ¡n \( \hat{Y} \) vÃ  giÃ¡ trá»‹ tháº­t \( Y \):

$$
J(W, b) = -\frac{1}{m}\sum_{i=1}^{m} \Big( y^{(i)}\log \hat{y}^{(i)} + (1 - y^{(i)})\log (1 - \hat{y}^{(i)}) \Big)
$$

Biá»ƒu diá»…n dáº¡ng vector hÃ³a:

$$
J = -\frac{1}{m}\Big( Y^T \log(\hat{Y}) + (1 - Y)^T \log(1 - \hat{Y}) \Big)
$$

> ğŸ’¬ *Ã nghÄ©a:*  
> - Khi dá»± Ä‘oÃ¡n Ä‘Ãºng (gáº§n 1 náº¿u y=1, gáº§n 0 náº¿u y=0) â†’ loss nhá»  
> - Khi sai lá»‡ch nhiá»u â†’ loss lá»›n (pháº¡t máº¡nh)

---

### âš™ï¸ 4ï¸âƒ£ Gradient Derivation (Äáº¡o hÃ m vector hÃ³a)

Äá»ƒ cáº­p nháº­t $W$ vÃ  $b$, ta cáº§n tÃ­nh gradient cá»§a hÃ m máº¥t mÃ¡t:

$$
\frac{\partial J}{\partial W} = \frac{1}{m} X^T (\hat{Y} - Y)
\]
\[
\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})
$$

> ğŸ§© *Giáº£i thÃ­ch:*  
> - $\hat{Y} - Y)$ lÃ  vector sai sá»‘ cho tá»«ng máº«u  
> - $X^T(\hat{Y} - Y)$ gá»™p toÃ n bá»™ sai sá»‘ cá»§a cÃ¡c feature  
> - Chia cho $m$ Ä‘á»ƒ tÃ­nh trung bÃ¬nh gradient trÃªn batch

---

### ğŸ” 5ï¸âƒ£ Gradient Descent Update Rules

Sau khi cÃ³ Ä‘áº¡o hÃ m, ta cáº­p nháº­t trá»ng sá»‘ theo hÆ°á»›ng giáº£m loss:

$$
W := W - \alpha \frac{\partial J}{\partial W}
\]
\[
b := b - \alpha \frac{\partial J}{\partial b}
$$

- $\alpha$: learning rate  
- Cáº­p nháº­t láº·p Ä‘i láº·p láº¡i â†’ mÃ´ hÃ¬nh há»™i tá»¥ dáº§n.

---

### ğŸª„ 6ï¸âƒ£ So sÃ¡nh: Loop vs Vectorization

| TiÃªu chÃ­ | CÃ¡ch cÅ© (loop tá»«ng máº«u) | Vector hÃ³a |
|:--|:--|:--|
| Cáº¥u trÃºc | `for i in range(m): ...` | 1 dÃ²ng ma tráº­n |
| Tá»‘c Ä‘á»™ | Cháº­m, tá»‘n CPU | Nhanh, tá»‘i Æ°u GPU |
| Code | DÃ i, khÃ³ debug | Ngáº¯n gá»n, dá»… hiá»ƒu |
| Má»Ÿ rá»™ng | Giá»›i háº¡n batch nhá» | Dá»… má»Ÿ rá»™ng mini-batch / full-batch |
| á»¨ng dá»¥ng | Demo toy model | Deep Learning thá»±c chiáº¿n |

> ğŸŒˆ *Vectorization = há»c â€œsong songâ€ táº¥t cáº£ máº«u cÃ¹ng lÃºc â†’ tá»‘c Ä‘á»™ tÄƒng theo cáº¥p sá»‘ nhÃ¢n.*

---

### ğŸŒ¸ 7ï¸âƒ£ TÃ³m lÆ°á»£c toÃ n cÃ´ng thá»©c huáº¥n luyá»‡n

| BÆ°á»›c | CÃ´ng thá»©c | Ã nghÄ©a |
|:--|:--|:--|
| **1. Forward** | $\hat{Y} = \sigma(XW + b)$ | Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t |
| **2. Loss** | $J = -\frac{1}{m}(Y^T\log\hat{Y} + (1-Y)^T\log(1-\hat{Y}))$ | Sai lá»‡ch dá»± Ä‘oÃ¡n |
| **3. Gradient** | $dW = \frac{1}{m}X^T(\hat{Y}-Y)$, $db = \frac{1}{m}\sum(\hat{Y}-Y)$ | Äáº¡o hÃ m |
| **4. Update** | $W := W - \alpha dW,\ b := b - \alpha db$ | Cáº­p nháº­t trá»ng sá»‘ |

---

### ğŸ’« 8ï¸âƒ£ Minh há»a trá»±c quan



## ğŸ’» Pháº§n 5 â€“ Code minh há»a (Vectorized Implementation)

```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def forward(X, W, b):
    Z = X @ W + b
    return sigmoid(Z)

def bce_loss(Y, Y_hat):
    eps = 1e-12
    Y_hat = np.clip(Y_hat, eps, 1 - eps)
    return -np.mean(Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat))

def gradients(X, Y, Y_hat):
    m = X.shape[0]
    dW = (1/m) * (X.T @ (Y_hat - Y))
    db = (1/m) * np.sum(Y_hat - Y)
    return dW, db

def train(X, Y, lr=0.1, epochs=1000):
    n = X.shape[1]
    W = np.zeros((n, 1))
    b = 0.0
    for t in range(epochs):
        Y_hat = forward(X, W, b)
        loss = bce_loss(Y, Y_hat)
        dW, db = gradients(X, Y, Y_hat)
        W -= lr * dW
        b -= lr * db
        if t % 200 == 0:
            print(f"Epoch {t:4d} | Loss = {loss:.6f}")
    return W, b

# Demo mini
if __name__ == "__main__":
    np.random.seed(42)
    X = np.random.randn(6, 2)
    Y = np.array([[0],[1],[0],[1],[1],[0]])
    W, b = train(X, Y, lr=0.3, epochs=1000)
    preds = forward(X, W, b)
    print("\nâœ… Final weights:\n", np.round(W,3), "\nBias:", round(b,3))
    print("\nğŸ”® Predictions:", np.round(preds,3).ravel())
    print("ğŸ“Š Classes:", (preds>=0.5).astype(int).ravel())

```
---

## ğŸŒˆ Pháº§n 6 â€“ á»¨ng dá»¥ng vÃ  phÃ¢n tÃ­ch

### ğŸš€ Batch vs. Mini-batch Gradient

- **Batch GD:** cáº­p nháº­t sau má»—i epoch â†’ á»•n Ä‘á»‹nh, nhÆ°ng cháº­m.  
- **Mini-batch GD:** cáº­p nháº­t tá»«ng nhÃ³m nhá» â†’ nhanh hÆ¡n, noise giÃºp trÃ¡nh local minima.

---

### ğŸ” Æ¯u Ä‘iá»ƒm vectorization

| Háº¡ng má»¥c | KhÃ´ng vector | Vector hÃ³a |
|-----------|---------------|-------------|
| â±ï¸ **Tá»‘c Ä‘á»™** | Cháº­m, pháº£i dÃ¹ng loop | Nhanh, dÃ¹ng phÃ©p nhÃ¢n ma tráº­n |
| ğŸ’» **Code** | DÃ i dÃ²ng, khÃ³ má»Ÿ rá»™ng | Gá»n gÃ ng, dá»… debug |
| ğŸ§® **Dá»… má»Ÿ rá»™ng** | Giá»›i háº¡n CPU | Tá»‘i Æ°u GPU/TPU |
| ğŸ¯ **á»¨ng dá»¥ng** | Model toy nhá» | Deep Learning thá»±c chiáº¿n |

---

### ğŸ’¡ Kiáº¿n thá»©c má»Ÿ rá»™ng

Vectorization lÃ  ná»n táº£ng cho:

- **ğŸ§  Neural Networks:** má»i layer Ä‘á»u lÃ  phÃ©p nhÃ¢n ma tráº­n.  
- **ğŸ” Backpropagation:** lan truyá»n gradient qua tá»«ng layer.  
- **âš™ï¸ Autograd engine (PyTorch/TF):** tÃ­nh toÃ¡n Ä‘áº¡o hÃ m tá»± Ä‘á»™ng tá»« vectorized ops.

> ğŸ‘‰ Nhá» vectorization, cÃ¡c mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i cÃ³ thá»ƒ huáº¥n luyá»‡n hÃ ng trÄƒm triá»‡u tham sá»‘ chá»‰ trong vÃ i giá» trÃªn GPU.

---

## ğŸ’œ Káº¿t luáº­n & Háº¹n gáº·p tuáº§n sau

Tuáº§n nÃ y, chÃºng ta Ä‘Ã£:

âœ… Náº¯m Ä‘Æ°á»£c cÃ¡ch vector hÃ³a Logistic Regression  
âœ… Hiá»ƒu rÃµ cÆ¡ cháº¿ gradient descent vÃ  binary cross-entropy  
âœ… Viáº¿t pipeline hoÃ n chá»‰nh trong Python vÃ  kiá»ƒm thá»­ mini demo  

Tuáº§n tá»›i, ta sáº½ khÃ¡m phÃ¡:

ğŸŒ¸ **Multi-feature Logistic Regression**  
ğŸŒ¸ **Regularization (L1/L2)** â€“ Ä‘á»ƒ trÃ¡nh overfitting  
ğŸŒ¸ **Feature scaling & normalization** â€“ giÃºp mÃ´ hÃ¬nh há»c á»•n Ä‘á»‹nh hÆ¡n  

> âœ¨ â€œVectorization khÃ´ng chá»‰ giÃºp mÃ¡y tÃ­nh cháº¡y nhanh hÆ¡n â€”  
> mÃ  cÃ²n giÃºp ta hiá»ƒu rÃµ hÆ¡n vá» báº£n cháº¥t cá»§a há»c mÃ¡y.â€ ğŸŒ¿

---

## ğŸ“š TÃ i nguyÃªn  

- ğŸ‘¨â€ğŸ« **Coding cho bÃ i giáº£ng cá»§a tháº§y Quangâ€“Vinh Äinh (Google Drive)**  
 
  ğŸ”— [Má»Ÿ thÆ° má»¥c Google Drive táº¡i Ä‘Ã¢y](https://drive.google.com/drive/folders/1haJTnN3AFM4ZpuBJZfrKK-fhH_GGj3RP?usp=sharing)    



ğŸ’¡ *Háº¹n gáº·p á»Ÿ bÃ i giáº£ng tiáº¿p theo cá»§a tuáº§n 2 â€“ Khi chÃºng ta dive sÃ¢u hÆ¡n vÃ o Deep Learning nhÃ©!* ğŸš€    



ğŸ  [Vá» trang chá»§]({{ '/' | relative_url }})    




