---
layout: post
title: Module 6 ‚Äì Tu·∫ßn 2
date: 10-11-2025
categories: [AIO2025, Module6, Deep Learning]
use_math: true
---

{% include mathjax.html %}

## T√≥m t·∫Øt b√†i gi·∫£ng ng√†y th·ª© T∆∞ ng√†y 12/11/2025 

## üìà Ph·∫ßn 1 ‚Äì √în l·∫°i Linear Regression v√† Logistic Regression

# Linear Regression 

**Linear Regression** l√† m√¥ h√¨nh h·ªçc c√≥ gi√°m s√°t d√πng ƒë·ªÉ d·ª± ƒëo√°n gi√° tr·ªã li√™n t·ª•c 

**M·ª•c ti√™u** l√† m√¥ h√¨nh Linear Regression s·∫Ω **c·∫≠p nh·∫≠t c√°c tr·ªçng s·ªë w v√† h·ªá s·ªë bias b** qua m·ªói l·∫ßn hu·∫•n luy·ªán sao cho gi√° tr·ªã $L_{\text{MSE}}$ l√† **nh·ªè nh·∫•t c√≥ th·ªÉ**.

---
Quy tr√¨nh t·ªïng qu√°t (Pipeline) c·ªßa Linear Regression:
<p align="center">
  <img src="{{ '/assets/module6-week1/pipeline.png' | relative_url }}" alt="Pipeline Diagram" width="600">
</p> 

**H·∫°n ch·∫ø** c·ªßa Linear Regression
- Kh√¥ng gi·ªõi h·∫°n ƒë·∫ßu ra [0,1]
- Kh√¥ng ph√π h·ª£p v·ªõi ranh gi·ªõi ph√¢n lo·∫°i phi tuy·∫øn
- H√†m m·∫•t m√°t (MSE) kh√¥ng ph√π h·ª£p ƒë·ªëi v·ªõi c√°c b√†i to√°n ph√¢n lo·∫°i
- Kh√¥ng ƒë·∫£m b·∫£o m√¥ h√¨nh h·ªôi t·ª• ·ªïn ƒë·ªãnh

# Logistic Regression
**Logistic Regression** l√† m·ªôt m√¥ h√¨nh **h·ªçc c√≥ gi√°m s√°t (supervised learning)** d√πng cho b√†i to√°n **ph√¢n lo·∫°i nh·ªã ph√¢n (binary classification)**

**M·ª•c ti√™u** d·ª± ƒëo√°n **x√°c su·∫•t m·ªôt m·∫´u thu·ªôc v·ªÅ m·ªôt l·ªõp c·ª• th·ªÉ** (th∆∞·ªùng l√† l·ªõp ‚Äú1‚Äù) trong b√†i to√°n ph√¢n lo·∫°i nh·ªã ph√¢n.

Quy tr√¨nh t·ªïng qu√°t (Pipeline) c·ªßa Logistic Regression:
<p align="center">
  <img src="{{ 'assets/module6-week1/LogisticR_training.png' | absolute_url }}"
       alt="Logistic RegressionE" width="800">
</p>

**H·∫°n ch·∫ø** c·ªßa Logistic Regression
- D·ªÖ b·ªã overfitting khi s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng (features) l·ªõn h∆°n s·ªë l∆∞·ª£ng quan s√°t (samples)
- Ch·ªâ d√πng cho bi·∫øn ph·ª• thu·ªôc r·ªùi r·∫°c (Discrete output)
- Kh√¥ng x·ª≠ l√Ω t·ªët c√°c b√†i to√°n phi tuy·∫øn (Non-linear)
- Nh·∫°y c·∫£m v·ªõi ƒëa c·ªông tuy·∫øn (Multicollinearity)
- Kh√≥ n·∫Øm b·∫Øt m·ªëi quan h·ªá ph·ª©c t·∫°p gi·ªØa c√°c bi·∫øn

**Gi·∫£i ph√°p** ƒë·ªëi v·ªõi c√°c b√†i to√°n **ph√¢n lo·∫°i nhi·ªÅu l·ªõp (multiclass classification)** l√† **Softmax Regression** hay c√≤n c√≥ t√™n g·ªçi kh√°c l√† **Multinomial Logistic Regression (H·ªìi quy Logistic ƒëa th·ª©c)**.

## Ph·∫ßn 2 - Softmax Regression
**1. Kh√°i ni·ªám**

**Softmax Regression** hay c√≥ t√™n g·ªçi kh√°c l√† **Multinomial Logistic Regression** l√† d·∫°ng t·ªïng qu√°t c·ªßa Logistic Regression d√πng ƒë·ªëi v·ªõi b√†i to√°n **ph√¢n lo·∫°i nhi·ªÅu l·ªõp (multiclass classification)**. Qu√° tr√¨nh t√≠nh to√°n c·ªßa h√†m n√†y ƒë∆∞·ª£c chia th√†nh 2 b∆∞·ªõc ch√≠nh:

**B∆∞·ªõc 1: T√≠nh to√°n K ƒëi·ªÉm s·ªë tuy·∫øn t√≠nh**

T∆∞∆°ng t·ª± nh∆∞ Linear v√† Logistic Regression, ta c≈©ng b·∫Øt ƒë·∫ßu v·ªõi m√¥ h√¨nh tuy·∫øn t√≠nh cho c√°c ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o nh∆∞ng ta s·∫Ω t√≠nh cho K l·ªõp:

$$
z_k = w^T_j*x + b_k (k = 1,2,...,K) \tag{1}
$$

Trong ƒë√≥:
- $z_k$: ƒëi·ªÉm s·ªë (logit) c·ªßa l·ªõp k
- $w_k$: vector tr·ªçng s·ªë (weight vector) t∆∞∆°ng ·ª©ng v·ªõi l·ªõp k
- $x$: vector ƒë·∫∑c tr∆∞ng (feature vector) c·ªßa m·∫´u ƒë·∫ßu v√†o.
- $b_k$: h·ªá s·ªë bias c·ªßa l·ªõp k
- $K$: t·ªïng s·ªë l·ªõp 

**B∆∞·ªõc 2: Chuy·ªÉn ƒë·ªïi K ƒëi·ªÉm s·ªë th√†nh K x√°c su·∫•t**

H√†m Softmax nh·∫≠n ƒë·∫ßu v√†o vector $z = \lbrace z_1, z_2, \ldots, z_k \rbrace$
v√† tr·∫£ v·ªÅ vector x√°c su·∫•t $\hat{y} = \lbrace \hat{y}_1, \hat{y}_2, \ldots, \hat{y}_k \rbrace$ sao cho $\sum{\hat{y_k}} = 1$

**H√†m Softmax**

$$
\hat{y_k} = P(y = k | x) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}} \tag{2}
$$

Trong ƒë√≥:

- $\hat{y_k}$: x√°c su·∫•t d·ª± ƒëo√°n m·∫´u thu·ªôc l·ªõp *k*.  
- $P(y = k \| x)$: k√Ω hi·ªáu x√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán c·ªßa l·ªõp k khi bi·∫øt ƒë·∫ßu v√†o x.  
- $e^{z_k}$: h√†m m≈© nh·∫±m l√†m n·ªïi b·∫≠t s·ª± kh√°c bi·ªát gi·ªØa c√°c ƒëi·ªÉm s·ªë.  
- $\sum_{j=1}^{K} e^{z_j}$: t·ªïng c√°c gi√° tr·ªã m≈© c·ªßa t·∫•t c·∫£ c√°c l·ªõp ‚Üí d√πng ƒë·ªÉ chu·∫©n h√≥a sao cho t·ªïng x√°c su·∫•t = 1.

Tuy nhi√™n, h√†m $e^x$ tƒÉng r·∫•t nhanh v√† s·∫Ω b·ªã overflow.

<p align="center">
  <img src="{{ '/assets/module6-week2/exponential_graph.png' | relative_url }}" alt="Exponential function" width="600">
</p>

Gi·∫£i ph√°p l√† ta ƒë·∫∑t $m = max(z)$ v√† sau ƒë√≥ h√†m softmax s·∫Ω bi·∫øn ƒë·ªïi nh∆∞ sau:

$$
\hat{y_k} = P(y = k | x) = \frac{e^{z_k - m}}{\sum_{j=1}^{K} e^{z_j - m}} \tag{3}
$$

<p align="center">
  <img src="{{ '/assets/module6-week2/exponential_graph_2.png' | relative_url }}" alt="Justifying exponential function" width="600">
</p>

√ù nghƒ©a:
- **Chu·∫©n h√≥a**: ƒë·∫£m b·∫£o t·ªïng c√°c x√°c su·∫•t ƒë·∫ßu ra lu√¥n b·∫±ng 1

$$
\sum_{k=1}^{K} \hat{y_k}
= \sum_{k=1}^{K} \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}
= \frac{1}{\sum_{j=1}^{K} e^{z_j}} \sum_{k=1}^{K} e^{z_k}
= 1 \tag{4}
$$

- **L√†m m·ªÅm (Softmax)**: L·ªõp c√≥ ƒëi·ªÉm $z_k$ cao s·∫Ω tr·∫£ ra x√°c su·∫•t $\hat{y_k}$ v∆∞·ª£t tr·ªôi h∆°n. T√™n g·ªçi ‚ÄúSoftmax‚Äù th·ªÉ
hi·ªán r·∫±ng ƒë√¢y l√† m·ªôt phi√™n b·∫£n ‚Äúm·ªÅm‚Äù v√† kh·∫£ vi c·ªßa h√†m 'argmax' (m·ªôt h√†m 'max' th√¥ng th∆∞·ªùng v·ªën ch·ªâ ch·ªçn ra m·ªôt gi√° tr·ªã l·ªõn nh·∫•t duy nh·∫•t v√† tr·∫£ v·ªÅ 1 cho l·ªõp ƒë√≥, 0 cho c√°c l·ªõp
c√≤n l·∫°i)

## Ph·∫ßn 3: H√†m m·∫•t m√°t: MSE, BCE, hay CCE?

**1. MSE (Mean Squared Error)**

H√†m MSE n·∫øu √°p d·ª•ng cho Softmax Regression:

$$
L_{MSE} = \frac{1}{2} \sum_j (\hat{y_j} - y_j)^2
$$

ƒê·∫°o h√†m c·ªßa h√†m m·∫•t m√°t theo logit $\( z_j \)$:

$$
\frac{\partial L_{MSE}}{\partial z_j} = \sum_k (\hat{y_k} - y_k) \frac{\partial \hat{y_k}}{\partial z_j}
$$

v·ªõi **Jacobian c·ªßa softmax** ƒë∆∞·ª£c cho b·ªüi:

$$
\frac{\partial \hat{y_k}}{\partial z_j} = \hat{y_k}(\delta_{kj} - \hat{y_j})
$$

Suy ra:

$$
\frac{\partial L_{MSE}}{\partial z_j}
= (\hat{y_j} - y_j)\hat{y_j}(1 - \hat{y_j})
- \sum_{k \ne j} (\hat{y_k} - y_k)\hat{y_k}\hat{y_j}
$$

 **Gradient MSE** b·ªã nh√¢n th√™m c√°c h·ªá s·ªë $\hat{y}(1 - \hat{y})$‚üπ b·ªã **tri·ªát ti√™u** khi softmax b√£o h√≤a ($\hat{y} \to 0$ ho·∫∑c $\hat{y} \to 1$).

 **2. BCE (Binary Cross Entropy)**

Ta c√πng nh√¨n l·∫°i c√¥ng th·ª©c c·ªßa h√†m Softmax:

$$
\hat{y_k} = P(y = k | x) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}
$$

Suy ra l√† t·ªïng x√°c su·∫•t c√°c m·∫´u $\sum_k{\hat{y_k}} = 1$. Hay n√≥i c√°ch kh√°c l√† c√°c m·∫´u ƒë·ªÅu c√≥ s·ª± ph·ª• thu·ªôc nh·∫•t ƒë·ªãnh

Tuy nhi√™n ƒë·ªëi v·ªõi BCE l·∫°i gi·∫£ v·ªù r·∫±ng:

$$
P(y_1, \ldots, y_K \mid x) = \prod_i P(y_i \mid x)
$$

t·ª©c l√† ƒë·ªôc l·∫≠p c√≥ ƒëi·ªÅu ki·ªán. V·∫≠y BCE kh√¥ng ph√π h·ª£p ƒë·ªÉ √°p d·ª•ng chung v·ªõi h√†m Softmax.

Gi·∫£i ph√°p l√† ta s·ª≠ d·ª•ng h√†m m·∫•t m√°t Categorical Cross-Entropy.

**3. Categorical Cross Entropy (CCE)**

**Categorical Cross Entropy** ƒëo l∆∞·ªùng **s·ª± kh√°c bi·ªát** gi·ªØa **ph√¢n ph·ªëi x√°c su·∫•t d·ª± ƒëo√°n** v√† **ph√¢n ph·ªëi th·ª±c t·∫ø** (ƒë√∫ng) c·ªßa c√°c l·ªõp. C√¥ng th·ª©c n√†y ƒë∆∞·ª£c √°p d·ª•ng ƒë·ªëi v·ªõi b√†i to√°n **ph√¢n lo·∫°i nhi·ªÅu l·ªõp (multi-class classification)**

D·∫°ng c√¥ng th·ª©c t·ªïng qu√°t:

$$
L_{\mathrm{CE}} = - \sum_{i=1}^{K} y_i \log \hat{y_i} \tag{5}
$$

Trong ƒë√≥:
- **$L_{CE}$** l√† h√†m loss *categorical cross-entropy*.

- **$y_i$** l√† nh√£n th·∫≠t (0 ho·∫∑c 1 cho m·ªói l·ªõp), l·∫•y t·ª´ vector nh√£n ƒë∆∞·ª£c m√£ h√≥a one-hot.

- **$\hat{y}_i$** l√† x√°c su·∫•t d·ª± ƒëo√°n c·ªßa m√¥ h√¨nh cho l·ªõp $i$.

- **$C$** l√† s·ªë l∆∞·ª£ng l·ªõp.

## Ph·∫ßn 4: Gradient Descent cho Softmax Regression ##

Sau khi t√≠nh to√°n loss th√¥ng qua Categorical Cross Entropy, b∆∞·ªõc ti·∫øp theo ta s·∫Ω t·ªëi ∆∞u h√≥a tham s·ªë l√† tr·ªçng s·ªë W v√† h·ªá s·ªë bias b.

Ph∆∞∆°ng ph√°p ph·ªï bi·∫øn nh·∫•t l√† Gradient Descent, c·∫≠p nh·∫≠t tham s·ªë ng∆∞·ª£c l·∫°i v·ªõi gradient c·ªßa h√†m m·∫•t m√°t.

ƒê·ªÉ √°p d·ª•ng Gradient Descent, ch√∫ng ta c·∫ßn t√≠nh ƒë·∫°o h√†m ri√™ng (gradient) c·ªßa L theo W v√† b. Qu√° tr√¨nh n√†y b·∫Øt bu·ªôc ph·∫£i d√πng quy t·∫Øc chu·ªói (chain rule):

$$
\frac{\partial L_i}{\partial z_{ik}}
= 
\sum_{j=1}^{K}
\frac{\partial L_i}{\partial \hat{y}_{ij}}
\frac{\partial \hat{y}_{ij}}{\partial z_{ik}}. \tag{6}
$$

**B∆∞·ªõc 1: ƒê·∫°o h√†m c·ªßa Loss theo ƒêi·ªÉm s·ªë (Logits)**

**Th√†nh ph·∫ßn 1 $\frac{\partial L_i}{\partial \hat{y}_{ij}}$ (ƒê·∫°o h√†m c·ªßa Loss theo d·ª± ƒëo√°n)**

$$
L_i = - \sum_{j=1}^{K} y_{ij} \log(\hat{y}_{ij}).
$$

Khi l·∫•y ƒë·∫°o h√†m $L_i$ theo m·ªôt $\hat{y}_{ij}$ c·ª• th·ªÉ, ch·ªâ c√≥ th√†nh ph·∫ßn j trong t·ªïng l√† c√≤n l·∫°i

$$
\frac{\partial L_i}{\partial \hat{y}_{ij}}
= 
\frac{\partial}{\partial \hat{y}_{ij}}
\left( -y_{ij} \log(\hat{y}_{ij}) \right)
= 
- y_{ij} \frac{1}{\hat{y}_{ij}}. \tag{7}
$$

**Th√†nh ph·∫ßn 2: $ \frac{\partial \hat{y_{ij}}}{\partial z_{ik}} $ (ƒê·∫°o h√†m c·ªßa h√†m Softmax)**

C√≥ 2 tr∆∞·ªùng h·ª£p:

**Tr∆∞·ªùng h·ª£p 1**: j = k (ƒê·∫°o h√†m c·ªßa $\hat{y_{ij}}$ theo $z_{ij}$)

$$
\hat{y}_{ij} = \frac{e^{z_{ij}}}{\sum_{l=1}^{K} e^{z_{il}}}
$$

S·ª≠ d·ª•ng quy t·∫Øc ƒë·∫°o h√†m th∆∞∆°ng (quotient rule) $ \left( \frac{u}{v} \right)' = \frac{u'v - uv'}{v^2} $:

$$
\frac{\partial \hat{y}_{ij}}{\partial z_{ij}}
=
\frac{
(e^{z_{ij}})' \left( \sum_{l} e^{z_{il}} \right)
-
e^{z_{ij}} \left( \sum_{l} e^{z_{il}} \right)'
}
{\left( \sum_{l} e^{z_{il}} \right)^2}
$$

$$
= \frac{e^{z_{ij}}(\sum{e^{il}}) - e^{z_{ij}}(e^{z_{ij}}) } {(\sum{e^{z_{il}}})^2} = \frac{e^{z_{ij}}}{\sum{e^{z_{il}}}}\frac{(\sum{e^{z_{il}}})- e^{z_{ij}}}{(\sum{e^{z_{il}}})}
$$

$$
= (\frac{e^{z_{ij}}}{\sum{e^{z_{il}}}})(1-\frac{e^{z_{ij}}}{(\sum{e^{z_{il}}})}) = \hat{y_{ij}}(1-\hat{y_{ij}}) \tag{8}
$$

**Tr∆∞·ªùng h·ª£p 2: $j \ne k$ (ƒê·∫°o h√†m c·ªßa $\hat{y_{ij}}$ theo $z_{ik}$)**  
T∆∞∆°ng t·ª±, s·ª≠ d·ª•ng quy t·∫Øc th∆∞∆°ng:

$$
\frac{\partial \hat{y}_{ij}}{\partial z_{ik}}
=
\frac{
(e^{z_{ij}})' \left( \sum_{l} e^{z_{il}} \right)
-
e^{z_{ij}} \left( \sum_{l} e^{z_{il}} \right)'
}
{
\left( \sum_{l} e^{z_{il}} \right)^2
}
$$

L√∫c n√†y,  
- $(e^{z_{ij}})' = 0$ v√¨ ƒë·∫°o h√†m theo $z_{ik}$ nh∆∞ng $j \ne k$  
- $\left( \sum_{l} e^{z_{il}} \right)' = e^{z_{ik}}$

Do ƒë√≥:

$$
\frac{0(\sum{e^{z_{il}}}) - e^{z_{ij}}(e^{z_ik})}{(\sum{e^{z_{il}}})^2} = -(\frac{e^{z_{ij}}}{\sum{e^{z_{il}}}})(\frac{e^{z_{ik}}}{\sum{e^{z_{il}}}})
= -\hat{y_{ij}}\hat{y_{ik}} \tag{9}
$$

K·∫øt h·ª£p l·∫°i, ta c√≥:

$$
\frac{\partial L_i}{\partial z_{ik}}
= 
\sum_{j=1}^{K}
\frac{\partial L_i}{\partial \hat{y}_{ij}}
\frac{\partial \hat{y}_{ij}}{\partial z_{ik}}.
$$

Ch√∫ng ta t√°ch t·ªïng n√†y ra th√†nh 2 ph·∫ßn: ph·∫ßn $j = k$ v√† ph·∫ßn $j \neq k$:

$$
\frac{\partial L_i}{\partial z_{ik}}
=
\underbrace{
\left(
\frac{\partial L_i}{\partial \hat{y}_{ik}}
\frac{\partial \hat{y}_{ik}}{\partial z_{ik}}
\right)
}_{j = k}
\;+\;
\underbrace{
\sum_{j \ne k}
\left(
\frac{\partial L_i}{\partial \hat{y}_{ij}}
\frac{\partial \hat{y}_{ij}}{\partial z_{ik}}
\right)
}_{j \ne k}.
$$

Th·∫ø c√¥ng th·ª©c $(7), (8), (9)$ v√†o, ta c√≥:

Thay c√°c k·∫øt qu·∫£ ta v·ª´a t√¨m ƒë∆∞·ª£c v√†o:

$$
= ((-\frac{y_{ik}}{\hat{y_{ik}}})(\hat{y_{ik}}(1-\hat{y_{ik}}))) + \sum_{j \ne k}((-\frac{y_{ij}}{\hat{y_{ij}}})(-\hat{y_{ij}}\hat{y_{ik}}))
$$

$$
= (-y_{ik}(1-\hat{y_{ik}})) + \sum_{j \ne k} (y_{ij}\hat{y_{ik}})
$$

$$
= (-y_{ik}+y_{ik}\hat{y_{ik}}) + \sum_{j \ne k} (y_{ij}\hat{y_{ik}})
$$

$$
= -y_{ik} + \hat{y_{ik}}(y_{ik} + \sum_{j \ne k} (y_{ij}))
$$

**ƒêi·ªÉm m·∫•u ch·ªët:** V√¨ $y_i$ l√† vector one-hot, n√™n t·ªïng t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ c·ªßa n√≥ $\sum_{j=1}^{K} y_{ij} = 1$.  

Do ƒë√≥:

$$
\left( y_{ik} + \sum_{j \ne k} y_{ij} \right) = 1.
$$

Ph∆∞∆°ng tr√¨nh tr·ªü th√†nh:

$$
\frac{\partial L_i}{\partial z_{ik}}
=
- y_{ik} + \hat{y}_{ik}(1)
=
\hat{y}_{ik} - y_{ik}.
\tag{7}
$$

V·∫≠y k·∫øt qu·∫£ b∆∞·ªõc 1 ƒë·∫°o h√†m theo Logits l√†:

$$
\frac{\partial L_i}{\partial z_i}
=
\hat{y}_{i} - y_{i} = e_i \tag{8}
$$

ƒê√¢y ch√≠nh l√† **vector l·ªói (error vector)** $e_i$, l√† s·ª± ch√™nh l·ªách tr·ª±c ti·∫øp gi·ªØa d·ª± ƒëo√°n v√† nh√£n th·ª±c t·∫ø.

**B∆∞·ªõc 2: ƒê·∫°o h√†m theo tham s·ªë (W v√† b)**

**M·ª•c ti√™u A: T√¨m  $\dfrac{\partial L}{\partial b}$**

√Åp d·ª•ng quy t·∫Øc Chain Rule, ta c√≥.

$$
\frac{\partial L_i}{\partial b_k}
=
\frac{\partial L_i}{\partial z_{ik}}
\frac{\partial z_{ik}}{\partial b_k}.
$$

---

- T·ª´ b∆∞·ªõc 1, ta c√≥: $$\frac{\partial L_i}{\partial z_{ik}} = \hat{y}_{ik} - y_{ik} = e_{ik}$$

- Ta c√≥: $$z_{ik} = (w_k^T x_i) + b_k.$$ => $$\frac{\partial z_{ik}}{\partial b_k} = 1.$$

Suy ra:

$$
\frac{\partial L_i}{\partial b_k}
=
e_{ik} \times 1
=
e_{ik}.
$$

---

L·∫•y trung b√¨nh tr√™n $N$ m·∫´u v√† vi·∫øt ·ªü d·∫°ng vector:

$$
\frac{\partial L}{\partial b}
=
\frac{1}{N}
\sum_{i=1}^{N}
\frac{\partial L_i}{\partial b}
=
\frac{1}{N}
\sum_{i=1}^{N}
e_i
=
\frac{1}{N}
\sum_{i=1}^{N}
(\hat{y}_i - y_i). \tag{9}
$$

**M·ª•c ti√™u B: T√¨m  $\dfrac{\partial L}{\partial W}$**

√Åp d·ª•ng quy t·∫Øc Chain Rule, ta c√≥

$$
\frac{\partial L_i}{\partial w_{kd}}
=
\sum_{j=1}^{K}
\frac{\partial L_i}{\partial z_{ij}}
\frac{\partial z_{ij}}{\partial w_{kd}}.
\tag{10}
$$

Tuy nhi√™n, $w_{kd}$ (tr·ªçng s·ªë c·ªßa l·ªõp $k$) ch·ªâ ·∫£nh h∆∞·ªüng ƒë·∫øn $z_{ik}$ (ƒëi·ªÉm s·ªë c·ªßa l·ªõp $k$). Do ƒë√≥, $\frac{\partial z_{ij}}{\partial w_{kd}} = 0$ v·ªõi m·ªçi $j \ne k$.  

T·ªïng $\sum$ ƒë∆∞·ª£c r√∫t g·ªçn:

$$
\frac{\partial L_i}{\partial w_{kd}}
=
\frac{\partial L_i}{\partial z_{ik}}
\frac{\partial z_{ik}}{\partial w_{kd}}.
\tag{11}
$$

---

- Ta ƒë√£ c√≥: $$ \frac{\partial L_i}{\partial z_{ik}} = \hat{y}_{ik} - y_{ik} = e_{ik} $$ (·ªü c√¥ng th·ª©c s·ªë $(8)$ )

- Ta c√≥: $$ z_{ik} = \sum_{d'=1}^{D} w_{kd'} x_{id'} + b_k$$

- L·∫•y ƒë·∫°o h√†m $z_{ik}$ theo $w_{kd}$:
  $$
  \frac{\partial z_{ik}}{\partial w_{kd}} = x_{id}.
  $$

Suy ra:

$$
\frac{\partial L_i}{\partial w_{kd}}
=
e_{ik} \cdot x_{id}.
\tag{12}
$$

---

ƒê√¢y l√† ƒë·∫°o h√†m cho **m·ªôt ph·∫ßn t·ª≠** $(k, d)$ c·ªßa ma tr·∫≠n $W$.  
V·∫≠y:

$$
\frac{\partial L_i}{\partial W}
=
e_i x_i^T
=
(\hat{y}_i - y_i) x_i^T.
\tag{13}
$$

---

L·∫•y trung b√¨nh tr√™n $N$ m·∫´u:

$$
\frac{\partial L}{\partial W}
=
\frac{1}{N}
\sum_{i=1}^{N}
\frac{\partial L_i}{\partial W}
=
\frac{1}{N}
\sum_{i=1}^{N}
(\hat{y}_i - y_i) x_i^T.
\tag{14}
$$


**C√¥ng th·ª©c Gradient**

Sau khi r√∫t g·ªçn c√°c k·∫øt qu·∫£ ph·ª©c t·∫°p tr√™n, ta thu ƒë∆∞·ª£c c√¥ng th·ª©c sau:

$$
\nabla_W L
=
\frac{\partial L}{\partial W}
=
\frac{1}{N}
\sum_{i=1}^{N}
(\hat{y}_{i} - y_{i}) x_i^T
$$

$$
\nabla_b L
=
\frac{\partial L}{\partial b}
=
\frac{1}{N}
\sum_{i=1}^{N}
(\hat{y}_{i} - y_{i})
$$

---
Hay c√≥ th·ªÉ hi·ªÉu theo c√°ch ƒë∆°n gi·∫£n sau:

- $\nabla_{\text{weights}} L = \dfrac{1}{N} \sum (\text{d·ª± ƒëo√°n} - \text{th·ª±c t·∫ø}) \cdot (\text{ƒë·∫ßu v√†o})^T$

- $\nabla_{\text{bias}} L = \dfrac{1}{N} \sum (\text{d·ª± ƒëo√°n} - \text{th·ª±c t·∫ø})$



 









## Ph·∫ßn 5: Quy tr√¨nh hu·∫•n luy·ªán ##

Qu√° tr√¨nh hu·∫•n luy·ªán cho Softmax Regression kh√° gi·ªëng v·ªõi Linear v√† Logistic Regression. Qu√° tr√¨nh n√†y c≈©ng s·ª≠ d·ª•ng Batch Gradient Descent ƒë·ªÉ t·ªëi ∆∞u h√≥a c√°c tham s·ªë l√† ma tr·∫≠n W v√† vector h·ªá s·ªë bias b

<p align="center">
  <img src="{{ '/assets/module6-week2/Pipeline_Softmax_Regression.png' | relative_url }}" alt="Pipeline Diagram" width="600">
</p>

{% include mathjax.html %}

## T√≥m t·∫Øt b√†i gi·∫£ng ng√†y th·ª© S√°u ng√†y 14/11/2025 

## üìà Ph·∫ßn 1 ‚Äì C∆° b·∫£n v·ªÅ Pytorch
Ng√†y nay, ·ª©ng d·ª•ng c·ªßa Deep Learning c√≥ m·∫∑t c√≥ kh·∫Øp m·ªçi n∆°i: d·ªãch m√°y (machine translation), x√°c ƒë·ªãnh v·∫≠t th·ªÉ trong ·∫£nh (object detection), √¥ t√¥ t·ª± l√°i (self-driving car), ... V√¨ v·∫≠y, ch√∫ng ta c·∫ßn c√°c Deep Learning framework ƒë·ªÉ h·ªó tr·ª£ ph√°t tri·ªÉn ·ª©ng d·ª•ng c≈©ng nh∆∞ tri·ªÉn khai s·∫£n ph·∫©m ra th·ª±c t·∫ø. C√≥ r·∫•t nhi·ªÅu framework cho Deep Learning nh∆∞: Pytorch, Tensorflow, Keras, Mxnet,‚Ä¶ Trong s·ªë ƒë√≥, PyTorch ƒë∆∞·ª£c bi·∫øt ƒë·∫øn l√† m·ªôt framework h·ªçc s√¢u (Deep Learning Framework) m√£ ngu·ªìn m·ªü ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi Facebook's AI Research lab (FAIR). PyTorch n·ªïi ti·∫øng v·ªõi kh·∫£ nƒÉng cung c·∫•p t√≠nh to√°n tensor m·∫°nh m·∫Ω v√† t√≠nh linh ho·∫°t cao, ƒë·∫∑c bi·ªát trong vi·ªác x√¢y d·ª±ng v√† hu·∫•n luy·ªán c√°c m√¥ h√¨nh m·∫°ng n∆°-ron.

## C√°c th√†nh ph·∫ßn c·ªët l√µi:
**Tensor:** L√† c·∫•u tr√∫c d·ªØ li·ªáu c∆° b·∫£n nh·∫•t trong PyTorch, t∆∞∆°ng t·ª± nh∆∞ **numpy.array** nh∆∞ng c√≥ kh·∫£ nƒÉng t·∫≠n d·ª•ng s·ª©c m·∫°nh c·ªßa GPU ƒë·ªÉ tƒÉng t·ªëc t√≠nh to√°n. M·ªçi d·ªØ li·ªáu (t·ª´ ƒë·∫ßu v√†o ƒë·∫øn tr·ªçng s·ªë m√¥ h√¨nh) ƒë·ªÅu ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng Tensor

**Module torch.nn**: Cung c·∫•p c√°c kh·ªëi x√¢y d·ª±ng cho M·∫°ng N∆°-ron (Neural Networks) nh∆∞ c√°c l·ªõp (Linear, Conv2D, Recurrent...), h√†m m·∫•t m√°t (Loss functions) ƒë·ªÉ t·ªï ch·ª©c c·∫•u tr√∫c m√¥ h√¨nh.

**Module torch.optim**: Ch·ª©a c√°c thu·∫≠t to√°n t·ªëi ∆∞u h√≥a (Optimizer) nh∆∞ Stochastic Gradient Descent (SGD), Adam, RMSprop... d√πng ƒë·ªÉ c·∫≠p nh·∫≠t tr·ªçng s·ªë c·ªßa m√¥ h√¨nh trong qu√° tr√¨nh hu·∫•n luy·ªán.

**Autograd**: H·ªá th·ªëng ƒë·∫°o h√†m t·ª± ƒë·ªông cho ph√©p t√≠nh to√°n gradient (ƒë·∫°o h√†m) c·ªßa h√†m m·∫•t m√°t ƒë·ªëi v·ªõi c√°c tham s·ªë c·ªßa m√¥ h√¨nh m·ªôt c√°ch t·ª± ƒë·ªông v√† hi·ªáu qu·∫£, ƒë√¢y l√† y·∫øu t·ªë then ch·ªët cho thu·∫≠t to√°n lan truy·ªÅn ng∆∞·ª£c (Backpropagation).

## üöÄ Ph·∫ßn 2: Tri·ªÉn khai Pytorch cho Linear Regression - Logistic Regression - Softmax Regression
PyTorch cung c·∫•p module torch.nn ƒë·ªÉ x√¢y d·ª±ng c·∫•u tr√∫c m√¥ h√¨nh (ki·∫øn tr√∫c m·∫°ng n∆°-ron) v√† torch.optim ƒë·ªÉ th·ª±c hi·ªán thu·∫≠t to√°n t·ªëi ∆∞u h√≥a.

**1. Linear Regression (H·ªìi quy Tuy·∫øn t√≠nh)**

**Linear Regression** l√† m√¥ h√¨nh c∆° b·∫£n nh·∫•t, d√πng ƒë·ªÉ d·ª± ƒëo√°n m·ªôt gi√° tr·ªã li√™n t·ª•c $y$ d·ª±a tr√™n c√°c bi·∫øn ƒë·∫ßu v√†o $x$.

* **H√†m gi·∫£ thuy·∫øt**: $$\hat{y} = w \cdot x + b$$

* **H√†m m·∫•t m√°t (Loss Function)**: Th∆∞·ªùng s·ª≠ d·ª•ng Mean Squared Error (MSE) ƒë·ªÉ ƒëo l∆∞·ªùng sai s·ªë b√¨nh ph∆∞∆°ng gi·ªØa gi√° tr·ªã d·ª± ƒëo√°n ($\hat{y}$) v√† gi√° tr·ªã th·ª±c t·∫ø ($y$). 

    $$L(\theta) = \text{MSE} = \frac{1}{N}\sum_{i=1}^{N}(\hat{y}_i - y_i)^2$$

**C·∫•u tr√∫c PyTorch**: S·ª≠ d·ª•ng l·ªõp **torch.nn.Linear(in_features, out_features)** ƒë·ªÉ ƒë·ªãnh nghƒ©a l·ªõp t√≠nh to√°n tuy·∫øn t√≠nh $w \cdot x + b$.

**2. Logistic Regression (H·ªìi quy Logistic)**

Logistic Regression ƒë∆∞·ª£c s·ª≠ d·ª•ng cho b√†i to√°n Ph√¢n lo·∫°i Nh·ªã ph√¢n (Binary Classification), d·ª± ƒëo√°n x√°c su·∫•t m·ªôt m·∫´u thu·ªôc v·ªÅ m·ªôt trong hai l·ªõp.
* **H√†m gi·∫£ thuy·∫øt:**
    * **L·ªõp tuy·∫øn t√≠nh**: T√≠nh to√°n $z = w \cdot x + b$.
    * **H√†m k√≠ch ho·∫°t Sigmoid**: Chuy·ªÉn ƒë·ªïi $z$ th√†nh x√°c su·∫•t $\hat{y} \in [0, 1]$.

    $$\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}$$

* **H√†m m·∫•t m√°t**: Th∆∞·ªùng s·ª≠ d·ª•ng Binary Cross-Entropy (BCE). 
$$L(\theta) = -\frac{1}{N}\sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]$$

**C·∫•u tr√∫c PyTorch**: S·ª≠ d·ª•ng **torch.nn.Linear** (t√≠nh $z$) v√† **torch.nn.Sigmoid** (t√≠nh $\hat{y}$).

**3. Softmax Regression (H·ªìi quy Softmax)**

**Softmax Regression** l√† s·ª± m·ªü r·ªông c·ªßa Logistic Regression cho b√†i to√°n Ph√¢n lo·∫°i ƒêa l·ªõp (Multi-class Classification).

* **H√†m gi·∫£ thuy·∫øt:**
    * **L·ªõp tuy·∫øn t√≠nh**: T√≠nh to√°n $z_i$ cho m·ªói l·ªõp $i$ ($z = Wx + b$). 
                        $$z_i = w_i \cdot x + b_i$$
    * **H√†m k√≠ch ho·∫°t Softmax**: Chuy·ªÉn ƒë·ªïi c√°c gi√° tr·ªã $z_i$ th√†nh m·ªôt ph√¢n ph·ªëi x√°c su·∫•t $\hat{y}$ v·ªõi t·ªïng x√°c su·∫•t b·∫±ng 1. 

    $$\hat{y}_i = \text{Softmax}(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

L∆∞u √Ω: Trong th·ª±c t·∫ø, ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh ·ªïn ƒë·ªãnh (tr√°nh tr√†n s·ªë), ta th∆∞·ªùng d√πng c√¥ng th·ª©c Stable Softmax: 
$$\hat{y}_i = \frac{e^{z_i - \max(z)}}{\sum_j e^{z_j - \max(z)}}$$

* **H√†m m·∫•t m√°t**: S·ª≠ d·ª•ng Cross-Entropy Loss. 
$$L(\theta) = -\sum_{i=1}^{K} y_i \log(\hat{y}_i)$$

Trong ƒë√≥ $y_i$ l√† nh√£n m√£ h√≥a One-Hot Encoding (ch·ªâ c√≥ 1 gi√° tr·ªã l√† 1 v√† c√°c gi√° tr·ªã kh√°c l√† 0).

**C·∫•u tr√∫c PyTorch**: Trong PyTorch, h√†m m·∫•t m√°t **torch.nn.CrossEntropyLoss** ƒë√£ t√≠ch h·ª£p c·∫£ b∆∞·ªõc Softmax v√† Cross-Entropy Loss, n√™n ta ch·ªâ c·∫ßn truy·ªÅn tr·ª±c ti·∫øp ƒë·∫ßu ra $z$ (c√≤n g·ªçi l√† logits) v√†o h√†m n√†y.


## ‚ö°Ph·∫ßn 3: Hu·∫•n luy·ªán m√¥ h√¨nh
Qu√° tr√¨nh hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh trong PyTorch lu√¥n bao g·ªìm c√°c b∆∞·ªõc c∆° b·∫£n trong m·ªói v√≤ng l·∫∑p (Epoch) nh∆∞ d∆∞·ªõi ƒë√¢y:

**1. Kh·ªüi t·∫°o M√¥ h√¨nh ‚Äì Loss ‚Äì Optimizer**

| Th√†nh ph·∫ßn | Vai tr√≤ | V√≠ d·ª• Pytorch |
|-----------|---------|----------------|
| M√¥ h√¨nh (Model) | ƒê·ªãnh nghƒ©a ki·∫øn tr√∫c (`w ¬∑ x + b`) | `model = nn.Linear(in_features, out_features)` |
| H√†m m·∫•t m√°t (Loss Function) | ƒêo l∆∞·ªùng l·ªói | `loss_fn = torch.nn.MSELoss()` **(Linear Regression)** <br> `loss_fn = torch.nn.BCELoss()` **(Logistic Regression)** <br> `loss_fn = torch.nn.CrossEntropyLoss()` **(Softmax Regression)** |
| B·ªô t·ªëi ∆∞u h√≥a (Optimizer) | C·∫≠p nh·∫≠t tham s·ªë m√¥ h√¨nh | `optimizer = torch.optim.SGD(model.parameters(), lr=0.01)` |

**2. V√≤ng l·∫∑p Hu·∫•n luy·ªán (Training Loop)**

Trong m·ªói v√≤ng l·∫∑p hu·∫•n luy·ªán, c√°c b∆∞·ªõc sau ƒë∆∞·ª£c th·ª±c hi·ªán tu·∫ßn t·ª±:

**2.1. Forward Propagation**: Chuy·ªÉn d·ªØ li·ªáu ƒë·∫ßu v√†o $x$ qua m√¥ h√¨nh ƒë·ªÉ nh·∫≠n ƒë∆∞·ª£c gi√° tr·ªã d·ª± ƒëo√°n $\hat{y}$.
```markdown
y_hat = model(x_data)                  ## Linear Regression/Softmax Regression
y_hat = torch.sigmoid(linear(x_data))  ## Logistic Regression
```
**2.2. T√≠nh to√°n Loss**: So s√°nh $\hat{y}$ v·ªõi nh√£n th·ª±c t·∫ø $y$ ƒë·ªÉ t√≠nh to√°n l·ªói $L$.
```markdown
loss = loss_fn(y_hat, y_data)
```

**2.3. Zero Gradient**: X√≥a c√°c ƒë·∫°o h√†m ƒë√£ t√≠nh to√°n t·ª´ b∆∞·ªõc tr∆∞·ªõc, v√¨ Autograd s·∫Ω t√≠ch l≈©y ch√∫ng theo m·∫∑c ƒë·ªãnh.
```markdown
optimizer.zero_grad()
```
**2.4. Backward Propagation**: T√≠nh to√°n ƒë·∫°o h√†m c·ªßa h√†m m·∫•t m√°t theo t·ª´ng tham s·ªë $w$ v√† $b$ th√¥ng qua Autograd.
```markdown
loss.backward()
```
**2.5. Update Parameters**: C·∫≠p nh·∫≠t c√°c tham s·ªë m√¥ h√¨nh d·ª±a tr√™n ƒë·∫°o h√†m ƒë√£ t√≠nh v√† t·ªëc ƒë·ªô h·ªçc $\eta$.
```markdown
optimizer.step()
```
Qu√° tr√¨nh n√†y l·∫∑p ƒëi l·∫∑p l·∫°i nhi·ªÅu l·∫ßn cho ƒë·∫øn khi m√¥ h√¨nh h·ªôi t·ª• (gi√° tr·ªã m·∫•t m√°t gi·∫£m ƒë·∫øn m·ª©c ch·∫•p nh·∫≠n ƒë∆∞·ª£c).

## Ph·∫ßn 4: K·∫øt lu·∫≠n
**PyTorch** cung c·∫•p m·ªôt n·ªÅn t·∫£ng m·∫°nh m·∫Ω v√† tr·ª±c quan ƒë·ªÉ tri·ªÉn khai c√°c m√¥ h√¨nh Machine Learning c∆° b·∫£n nh∆∞ Linear Regression, Logistic Regression, v√† Softmax Regression.

**T√≠nh linh ho·∫°t**: V·ªõi c·∫•u tr√∫c nn.Module v√† c∆° ch·∫ø Autograd m·∫°nh m·∫Ω, PyTorch cho ph√©p ng∆∞·ªùi h·ªçc d·ªÖ d√†ng x√¢y d·ª±ng, t√πy ch·ªânh v√† th·ª≠ nghi·ªám c√°c ki·∫øn tr√∫c m√¥ h√¨nh.

**Kh·∫£ nƒÉng m·ªü r·ªông**: C√°c m√¥ h√¨nh c∆° b·∫£n n√†y l√† n·ªÅn t·∫£ng ƒë·ªÉ m·ªü r·ªông sang c√°c ki·∫øn tr√∫c h·ªçc s√¢u ph·ª©c t·∫°p h∆°n nh∆∞ CNN (m√¥ h√¨nh Image Classification) v√† RNN/Transformer (m√¥ h√¨nh X·ª≠ l√Ω Ng√¥n ng·ªØ T·ª± nhi√™n), l√† nh·ªØng ·ª©ng d·ª•ng m√† PyTorch th·ªÉ hi·ªán r√µ r√†ng s·ª©c m·∫°nh c·ªßa m√¨nh.

Vi·ªác n·∫Øm v·ªØng quy tr√¨nh Forward-Backward-Update trong PyTorch l√† b∆∞·ªõc ƒëi ƒë·∫ßu ti√™n v√† quan tr·ªçng nh·∫•t ƒë·ªÉ l√†m ti·∫øp c·∫≠n c√°c gi·∫£i thu·∫≠t trong Deep Learning.
