---
layout: post
title: Module 6 ‚Äì Tu·∫ßn 2
date: 10-11-2025
categories: [AIO2025, Module6, Deep Learning]
use_math: true
---

{% include mathjax.html %}

## T√≥m t·∫Øt b√†i gi·∫£ng ng√†y th·ª© T∆∞ ng√†y 12/11/2025 

## üìà Ph·∫ßn 1 ‚Äì √în l·∫°i Linear Regression v√† Logistic Regression

# Linear Regression 

**Linear Regression** l√† m√¥ h√¨nh h·ªçc c√≥ gi√°m s√°t d√πng ƒë·ªÉ d·ª± ƒëo√°n gi√° tr·ªã li√™n t·ª•c 

**M·ª•c ti√™u** l√† m√¥ h√¨nh Linear Regression s·∫Ω **c·∫≠p nh·∫≠t c√°c tr·ªçng s·ªë w v√† h·ªá s·ªë bias b** qua m·ªói l·∫ßn hu·∫•n luy·ªán sao cho gi√° tr·ªã $L_{\text{MSE}}$ l√† **nh·ªè nh·∫•t c√≥ th·ªÉ**.

---
Quy tr√¨nh t·ªïng qu√°t (Pipeline) c·ªßa Linear Regression:
<p align="center">
  <img src="{{ '/assets/module6-week1/pipeline.png' | relative_url }}" alt="Pipeline Diagram" width="600">
</p> 

**H·∫°n ch·∫ø** c·ªßa Linear Regression
- Kh√¥ng gi·ªõi h·∫°n ƒë·∫ßu ra [0,1]
- Kh√¥ng ph√π h·ª£p v·ªõi ranh gi·ªõi ph√¢n lo·∫°i phi tuy·∫øn
- H√†m m·∫•t m√°t (MSE) kh√¥ng ph√π h·ª£p ƒë·ªëi v·ªõi c√°c b√†i to√°n ph√¢n lo·∫°i
- Kh√¥ng ƒë·∫£m b·∫£o m√¥ h√¨nh h·ªôi t·ª• ·ªïn ƒë·ªãnh

# Logistic Regression
**Logistic Regression** l√† m·ªôt m√¥ h√¨nh **h·ªçc c√≥ gi√°m s√°t (supervised learning)** d√πng cho b√†i to√°n **ph√¢n lo·∫°i nh·ªã ph√¢n (binary classification)**

**M·ª•c ti√™u** d·ª± ƒëo√°n **x√°c su·∫•t m·ªôt m·∫´u thu·ªôc v·ªÅ m·ªôt l·ªõp c·ª• th·ªÉ** (th∆∞·ªùng l√† l·ªõp ‚Äú1‚Äù) trong b√†i to√°n ph√¢n lo·∫°i nh·ªã ph√¢n.

Quy tr√¨nh t·ªïng qu√°t (Pipeline) c·ªßa Logistic Regression:
<p align="center">
  <img src="{{ 'assets/module6-week1/LogisticR_training.png' | absolute_url }}"
       alt="Logistic RegressionE" width="800">
</p>

**H·∫°n ch·∫ø** c·ªßa Logistic Regression
- D·ªÖ b·ªã overfitting khi s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng (features) l·ªõn h∆°n s·ªë l∆∞·ª£ng quan s√°t (samples)
- Ch·ªâ d√πng cho bi·∫øn ph·ª• thu·ªôc r·ªùi r·∫°c (Discrete output)
- Kh√¥ng x·ª≠ l√Ω t·ªët c√°c b√†i to√°n phi tuy·∫øn (Non-linear)
- Nh·∫°y c·∫£m v·ªõi ƒëa c·ªông tuy·∫øn (Multicollinearity)
- Kh√≥ n·∫Øm b·∫Øt m·ªëi quan h·ªá ph·ª©c t·∫°p gi·ªØa c√°c bi·∫øn

**Gi·∫£i ph√°p** ƒë·ªëi v·ªõi c√°c b√†i to√°n **ph√¢n lo·∫°i nhi·ªÅu l·ªõp (multiclass classification)** l√† **Softmax Regression** hay c√≤n c√≥ t√™n g·ªçi kh√°c l√† **Multinomial Logistic Regression (H·ªìi quy Logistic ƒëa th·ª©c)**.

## Ph·∫ßn 2 - Softmax Regression
**1. Kh√°i ni·ªám**

**Softmax Regression** hay c√≥ t√™n g·ªçi kh√°c l√† **Multinomial Logistic Regression** l√† d·∫°ng t·ªïng qu√°t c·ªßa Logistic Regression d√πng ƒë·ªëi v·ªõi b√†i to√°n **ph√¢n lo·∫°i nhi·ªÅu l·ªõp (multiclass classification)**. Qu√° tr√¨nh t√≠nh to√°n c·ªßa h√†m n√†y ƒë∆∞·ª£c chia th√†nh 2 b∆∞·ªõc ch√≠nh:

**B∆∞·ªõc 1: T√≠nh to√°n K ƒëi·ªÉm s·ªë tuy·∫øn t√≠nh**

T∆∞∆°ng t·ª± nh∆∞ Linear v√† Logistic Regression, ta c≈©ng b·∫Øt ƒë·∫ßu v·ªõi m√¥ h√¨nh tuy·∫øn t√≠nh cho c√°c ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o nh∆∞ng ta s·∫Ω t√≠nh cho K l·ªõp:

$$
z_k = w^T_j*x + b_k (k = 1,2,...,K) \tag{1}
$$

Trong ƒë√≥:
- $z_k$: ƒëi·ªÉm s·ªë (logit) c·ªßa l·ªõp k
- $w_k$: vector tr·ªçng s·ªë (weight vector) t∆∞∆°ng ·ª©ng v·ªõi l·ªõp k
- $x$: vector ƒë·∫∑c tr∆∞ng (feature vector) c·ªßa m·∫´u ƒë·∫ßu v√†o.
- $b_k$: h·ªá s·ªë bias c·ªßa l·ªõp k
- $K$: t·ªïng s·ªë l·ªõp 

**B∆∞·ªõc 2: Chuy·ªÉn ƒë·ªïi K ƒëi·ªÉm s·ªë th√†nh K x√°c su·∫•t**

H√†m Softmax nh·∫≠n ƒë·∫ßu v√†o vector $z = \lbrace z_1, z_2, \ldots, z_k \rbrace$
v√† tr·∫£ v·ªÅ vector x√°c su·∫•t $\hat{y} = \lbrace \hat{y}_1, \hat{y}_2, \ldots, \hat{y}_k \rbrace$ sao cho $\sum{\hat{y_k}} = 1$

**H√†m Softmax**

$$
\hat{y_k} = P(y = k | x) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}} \tag{2}
$$

Trong ƒë√≥:

- $\hat{y_k}$: x√°c su·∫•t d·ª± ƒëo√°n m·∫´u thu·ªôc l·ªõp *k*.  
- $P(y = k \| x)$: k√Ω hi·ªáu x√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán c·ªßa l·ªõp k khi bi·∫øt ƒë·∫ßu v√†o x.  
- $e^{z_k}$: h√†m m≈© nh·∫±m l√†m n·ªïi b·∫≠t s·ª± kh√°c bi·ªát gi·ªØa c√°c ƒëi·ªÉm s·ªë.  
- $\sum_{j=1}^{K} e^{z_j}$: t·ªïng c√°c gi√° tr·ªã m≈© c·ªßa t·∫•t c·∫£ c√°c l·ªõp ‚Üí d√πng ƒë·ªÉ chu·∫©n h√≥a sao cho t·ªïng x√°c su·∫•t = 1.

Tuy nhi√™n, h√†m $e^x$ tƒÉng r·∫•t nhanh v√† s·∫Ω b·ªã overflow.

<p align="center">
  <img src="{{ '/assets/module6-week2/exponential_graph.png' | relative_url }}" alt="Exponential function" width="600">
</p>

Gi·∫£i ph√°p l√† ta ƒë·∫∑t $m = max(z)$ v√† sau ƒë√≥ h√†m softmax s·∫Ω bi·∫øn ƒë·ªïi nh∆∞ sau:

$$
\hat{y_k} = P(y = k | x) = \frac{e^{z_k - m}}{\sum_{j=1}^{K} e^{z_j - m}} \tag{3}
$$

<p align="center">
  <img src="{{ '/assets/module6-week2/exponential_graph_2.png' | relative_url }}" alt="Justifying exponential function" width="600">
</p>

√ù nghƒ©a:
- **Chu·∫©n h√≥a**: ƒë·∫£m b·∫£o t·ªïng c√°c x√°c su·∫•t ƒë·∫ßu ra lu√¥n b·∫±ng 1

$$
\sum_{k=1}^{K} \hat{y_k}
= \sum_{k=1}^{K} \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}
= \frac{1}{\sum_{j=1}^{K} e^{z_j}} \sum_{k=1}^{K} e^{z_k}
= 1 \tag{4}
$$

- **L√†m m·ªÅm (Softmax)**: L·ªõp c√≥ ƒëi·ªÉm $z_k$ cao s·∫Ω tr·∫£ ra x√°c su·∫•t $\hat{y_k}$ v∆∞·ª£t tr·ªôi h∆°n. T√™n g·ªçi ‚ÄúSoftmax‚Äù th·ªÉ
hi·ªán r·∫±ng ƒë√¢y l√† m·ªôt phi√™n b·∫£n ‚Äúm·ªÅm‚Äù v√† kh·∫£ vi c·ªßa h√†m 'argmax' (m·ªôt h√†m 'max' th√¥ng th∆∞·ªùng v·ªën ch·ªâ ch·ªçn ra m·ªôt gi√° tr·ªã l·ªõn nh·∫•t duy nh·∫•t v√† tr·∫£ v·ªÅ 1 cho l·ªõp ƒë√≥, 0 cho c√°c l·ªõp
c√≤n l·∫°i)

## Ph·∫ßn 3: H√†m m·∫•t m√°t: MSE, BCE, hay CCE?

**1. MSE (Mean Squared Error)**

H√†m MSE n·∫øu √°p d·ª•ng cho Softmax Regression:

$$
L_{MSE} = \frac{1}{2} \sum_j (\hat{y_j} - y_j)^2
$$

ƒê·∫°o h√†m c·ªßa h√†m m·∫•t m√°t theo logit $\( z_j \)$:

$$
\frac{\partial L_{MSE}}{\partial z_j} = \sum_k (\hat{y_k} - y_k) \frac{\partial \hat{y_k}}{\partial z_j}
$$

v·ªõi **Jacobian c·ªßa softmax** ƒë∆∞·ª£c cho b·ªüi:

$$
\frac{\partial \hat{y_k}}{\partial z_j} = \hat{y_k}(\delta_{kj} - \hat{y_j})
$$

Suy ra:

$$
\frac{\partial L_{MSE}}{\partial z_j}
= (\hat{y_j} - y_j)\hat{y_j}(1 - \hat{y_j})
- \sum_{k \ne j} (\hat{y_k} - y_k)\hat{y_k}\hat{y_j}
$$

 **Gradient MSE** b·ªã nh√¢n th√™m c√°c h·ªá s·ªë $\hat{y}(1 - \hat{y})$‚üπ b·ªã **tri·ªát ti√™u** khi softmax b√£o h√≤a ($\hat{y} \to 0$ ho·∫∑c $\hat{y} \to 1$).

 **2. BCE (Binary Cross Entropy)**

Ta c√πng nh√¨n l·∫°i c√¥ng th·ª©c c·ªßa h√†m Softmax:

$$
\hat{y_k} = P(y = k | x) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}
$$

Suy ra l√† t·ªïng x√°c su·∫•t c√°c m·∫´u $\sum_k{\hat{y_k}} = 1$. Hay n√≥i c√°ch kh√°c l√† c√°c m·∫´u ƒë·ªÅu c√≥ s·ª± ph·ª• thu·ªôc nh·∫•t ƒë·ªãnh

Tuy nhi√™n ƒë·ªëi v·ªõi BCE l·∫°i gi·∫£ v·ªù r·∫±ng:

$$
P(y_1, \ldots, y_K \mid x) = \prod_i P(y_i \mid x)
$$

t·ª©c l√† ƒë·ªôc l·∫≠p c√≥ ƒëi·ªÅu ki·ªán. V·∫≠y BCE kh√¥ng ph√π h·ª£p ƒë·ªÉ √°p d·ª•ng chung v·ªõi h√†m Softmax.

Gi·∫£i ph√°p l√† ta s·ª≠ d·ª•ng h√†m m·∫•t m√°t Categorical Cross-Entropy.

**3. Categorical Cross Entropy (CCE)**

**Categorical Cross Entropy** ƒëo l∆∞·ªùng **s·ª± kh√°c bi·ªát** gi·ªØa **ph√¢n ph·ªëi x√°c su·∫•t d·ª± ƒëo√°n** v√† **ph√¢n ph·ªëi th·ª±c t·∫ø** (ƒë√∫ng) c·ªßa c√°c l·ªõp. C√¥ng th·ª©c n√†y ƒë∆∞·ª£c √°p d·ª•ng ƒë·ªëi v·ªõi b√†i to√°n **ph√¢n lo·∫°i nhi·ªÅu l·ªõp (multi-class classification)**

D·∫°ng c√¥ng th·ª©c t·ªïng qu√°t:

$$
L_{\mathrm{CE}} = - \sum_{i=1}^{K} y_i \log \hat{y_i} \tag{5}
$$

Trong ƒë√≥:
- **$L_{CE}$** l√† h√†m loss *categorical cross-entropy*.

- **$y_i$** l√† nh√£n th·∫≠t (0 ho·∫∑c 1 cho m·ªói l·ªõp), l·∫•y t·ª´ vector nh√£n ƒë∆∞·ª£c m√£ h√≥a one-hot.

- **$\hat{y}_i$** l√† x√°c su·∫•t d·ª± ƒëo√°n c·ªßa m√¥ h√¨nh cho l·ªõp $i$.

- **$C$** l√† s·ªë l∆∞·ª£ng l·ªõp.

## Ph·∫ßn 4: Gradient Descent cho Softmax Regression ##

Sau khi t√≠nh to√°n loss th√¥ng qua Categorical Cross Entropy, b∆∞·ªõc ti·∫øp theo ta s·∫Ω t·ªëi ∆∞u h√≥a tham s·ªë l√† tr·ªçng s·ªë W v√† h·ªá s·ªë bias b.

Ph∆∞∆°ng ph√°p ph·ªï bi·∫øn nh·∫•t l√† Gradient Descent, c·∫≠p nh·∫≠t tham s·ªë ng∆∞·ª£c l·∫°i v·ªõi gradient c·ªßa h√†m m·∫•t m√°t.

ƒê·ªÉ √°p d·ª•ng Gradient Descent, ch√∫ng ta c·∫ßn t√≠nh ƒë·∫°o h√†m ri√™ng (gradient) c·ªßa L theo W v√† b. Qu√° tr√¨nh n√†y b·∫Øt bu·ªôc ph·∫£i d√πng quy t·∫Øc chu·ªói (chain rule):

$$
\frac{\partial L_i}{\partial z_{ik}}
= 
\sum_{j=1}^{K}
\frac{\partial L_i}{\partial \hat{y}_{ij}}
\frac{\partial \hat{y}_{ij}}{\partial z_{ik}}. \tag{6}
$$

**B∆∞·ªõc 1: ƒê·∫°o h√†m c·ªßa Loss theo ƒêi·ªÉm s·ªë (Logits)**

**Th√†nh ph·∫ßn 1 $\frac{\partial L_i}{\partial \hat{y}_{ij}}$ (ƒê·∫°o h√†m c·ªßa Loss theo d·ª± ƒëo√°n)**

$$
L_i = - \sum_{j=1}^{K} y_{ij} \log(\hat{y}_{ij}).
$$

Khi l·∫•y ƒë·∫°o h√†m $L_i$ theo m·ªôt $\hat{y}_{ij}$ c·ª• th·ªÉ, ch·ªâ c√≥ th√†nh ph·∫ßn j trong t·ªïng l√† c√≤n l·∫°i

$$
\frac{\partial L_i}{\partial \hat{y}_{ij}}
= 
\frac{\partial}{\partial \hat{y}_{ij}}
\left( -y_{ij} \log(\hat{y}_{ij}) \right)
= 
- y_{ij} \frac{1}{\hat{y}_{ij}}. \tag{7}
$$

**Th√†nh ph·∫ßn 2: $ \frac{\partial \hat{y_{ij}}}{\partial z_{ik}} $ (ƒê·∫°o h√†m c·ªßa h√†m Softmax)**

C√≥ 2 tr∆∞·ªùng h·ª£p:

**Tr∆∞·ªùng h·ª£p 1**: j = k (ƒê·∫°o h√†m c·ªßa $\hat{y_{ij}}$ theo $z_{ij}$)

$$
\hat{y}_{ij} = \frac{e^{z_{ij}}}{\sum_{l=1}^{K} e^{z_{il}}}
$$

S·ª≠ d·ª•ng quy t·∫Øc ƒë·∫°o h√†m th∆∞∆°ng (quotient rule) $ \left( \frac{u}{v} \right)' = \frac{u'v - uv'}{v^2} $:

$$
\frac{\partial \hat{y}_{ij}}{\partial z_{ij}}
=
\frac{
(e^{z_{ij}})' \left( \sum_{l} e^{z_{il}} \right)
-
e^{z_{ij}} \left( \sum_{l} e^{z_{il}} \right)'
}
{\left( \sum_{l} e^{z_{il}} \right)^2}
$$

$$
= \frac{e^{z_{ij}}(\sum{e^{il}}) - e^{z_{ij}}(e^{z_{ij}}) } {(\sum{e^{z_{il}}})^2} = \frac{e^{z_{ij}}}{\sum{e^{z_{il}}}}\frac{(\sum{e^{z_{il}}})- e^{z_{ij}}}{(\sum{e^{z_{il}}})}
$$

$$
= (\frac{e^{z_{ij}}}{\sum{e^{z_{il}}}})(1-\frac{e^{z_{ij}}}{(\sum{e^{z_{il}}})}) = \hat{y_{ij}}(1-\hat{y_{ij}}) \tag{8}
$$

**Tr∆∞·ªùng h·ª£p 2: $j \ne k$ (ƒê·∫°o h√†m c·ªßa $\hat{y_{ij}}$ theo $z_{ik}$)**  
T∆∞∆°ng t·ª±, s·ª≠ d·ª•ng quy t·∫Øc th∆∞∆°ng:

$$
\frac{\partial \hat{y}_{ij}}{\partial z_{ik}}
=
\frac{
(e^{z_{ij}})' \left( \sum_{l} e^{z_{il}} \right)
-
e^{z_{ij}} \left( \sum_{l} e^{z_{il}} \right)'
}
{
\left( \sum_{l} e^{z_{il}} \right)^2
}
$$

L√∫c n√†y,  
- $(e^{z_{ij}})' = 0$ v√¨ ƒë·∫°o h√†m theo $z_{ik}$ nh∆∞ng $j \ne k$  
- $\left( \sum_{l} e^{z_{il}} \right)' = e^{z_{ik}}$

Do ƒë√≥:

$$
\frac{0(\sum{e^{z_{il}}}) - e^{z_{ij}}(e^{z_ik})}{(\sum{e^{z_{il}}})^2} = -(\frac{e^{z_{ij}}}{\sum{e^{z_{il}}}})(\frac{e^{z_{ik}}}{\sum{e^{z_{il}}}})
= -\hat{y_{ij}}\hat{y_{ik}} \tag{9}
$$

K·∫øt h·ª£p l·∫°i, ta c√≥:

$$
\frac{\partial L_i}{\partial z_{ik}}
= 
\sum_{j=1}^{K}
\frac{\partial L_i}{\partial \hat{y}_{ij}}
\frac{\partial \hat{y}_{ij}}{\partial z_{ik}}.
$$

Ch√∫ng ta t√°ch t·ªïng n√†y ra th√†nh 2 ph·∫ßn: ph·∫ßn $j = k$ v√† ph·∫ßn $j \neq k$:

$$
\frac{\partial L_i}{\partial z_{ik}}
=
\underbrace{
\left(
\frac{\partial L_i}{\partial \hat{y}_{ik}}
\frac{\partial \hat{y}_{ik}}{\partial z_{ik}}
\right)
}_{j = k}
\;+\;
\underbrace{
\sum_{j \ne k}
\left(
\frac{\partial L_i}{\partial \hat{y}_{ij}}
\frac{\partial \hat{y}_{ij}}{\partial z_{ik}}
\right)
}_{j \ne k}.
$$

Th·∫ø c√¥ng th·ª©c $(7), (8), (9)$ v√†o, ta c√≥:

Thay c√°c k·∫øt qu·∫£ ta v·ª´a t√¨m ƒë∆∞·ª£c v√†o:

$$
= ((-\frac{y_{ik}}{\hat{y_{ik}}})(\hat{y_{ik}}(1-\hat{y_{ik}}))) + \sum_{j \ne k}((-\frac{y_{ij}}{\hat{y_{ij}}})(-\hat{y_{ij}}\hat{y_{ik}}))
$$

$$
= (-y_{ik}(1-\hat{y_{ik}})) + \sum_{j \ne k} (y_{ij}\hat{y_{ik}})
$$

$$
= (-y_{ik}+y_{ik}\hat{y_{ik}}) + \sum_{j \ne k} (y_{ij}\hat{y_{ik}})
$$

$$
= -y_{ik} + \hat{y_{ik}}(y_{ik} + \sum_{j \ne k} (y_{ij}))
$$

**ƒêi·ªÉm m·∫•u ch·ªët:** V√¨ $y_i$ l√† vector one-hot, n√™n t·ªïng t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ c·ªßa n√≥ $\sum_{j=1}^{K} y_{ij} = 1$.  

Do ƒë√≥:

$$
\left( y_{ik} + \sum_{j \ne k} y_{ij} \right) = 1.
$$

Ph∆∞∆°ng tr√¨nh tr·ªü th√†nh:

$$
\frac{\partial L_i}{\partial z_{ik}}
=
- y_{ik} + \hat{y}_{ik}(1)
=
\hat{y}_{ik} - y_{ik}.
\tag{7}
$$

V·∫≠y k·∫øt qu·∫£ b∆∞·ªõc 1 ƒë·∫°o h√†m theo Logits l√†:

$$
\frac{\partial L_i}{\partial z_i}
=
\hat{y}_{i} - y_{i} = e_i \tag{8}
$$

ƒê√¢y ch√≠nh l√† **vector l·ªói (error vector)** $e_i$, l√† s·ª± ch√™nh l·ªách tr·ª±c ti·∫øp gi·ªØa d·ª± ƒëo√°n v√† nh√£n th·ª±c t·∫ø.

**B∆∞·ªõc 2: ƒê·∫°o h√†m theo tham s·ªë (W v√† b)**

**M·ª•c ti√™u A: T√¨m  $\dfrac{\partial L}{\partial b}$**

√Åp d·ª•ng quy t·∫Øc Chain Rule, ta c√≥.

$$
\frac{\partial L_i}{\partial b_k}
=
\frac{\partial L_i}{\partial z_{ik}}
\frac{\partial z_{ik}}{\partial b_k}.
$$

---

- T·ª´ b∆∞·ªõc 1, ta c√≥: $$\frac{\partial L_i}{\partial z_{ik}} = \hat{y}_{ik} - y_{ik} = e_{ik}$$

- Ta c√≥: $$z_{ik} = (w_k^T x_i) + b_k.$$ => $$\frac{\partial z_{ik}}{\partial b_k} = 1.$$

Suy ra:

$$
\frac{\partial L_i}{\partial b_k}
=
e_{ik} \times 1
=
e_{ik}.
$$

---

L·∫•y trung b√¨nh tr√™n $N$ m·∫´u v√† vi·∫øt ·ªü d·∫°ng vector:

$$
\frac{\partial L}{\partial b}
=
\frac{1}{N}
\sum_{i=1}^{N}
\frac{\partial L_i}{\partial b}
=
\frac{1}{N}
\sum_{i=1}^{N}
e_i
=
\frac{1}{N}
\sum_{i=1}^{N}
(\hat{y}_i - y_i). \tag{9}
$$

**M·ª•c ti√™u B: T√¨m  $\dfrac{\partial L}{\partial W}$**

√Åp d·ª•ng quy t·∫Øc Chain Rule, ta c√≥

$$
\frac{\partial L_i}{\partial w_{kd}}
=
\sum_{j=1}^{K}
\frac{\partial L_i}{\partial z_{ij}}
\frac{\partial z_{ij}}{\partial w_{kd}}.
\tag{10}
$$

Tuy nhi√™n, $w_{kd}$ (tr·ªçng s·ªë c·ªßa l·ªõp $k$) ch·ªâ ·∫£nh h∆∞·ªüng ƒë·∫øn $z_{ik}$ (ƒëi·ªÉm s·ªë c·ªßa l·ªõp $k$). Do ƒë√≥, $\frac{\partial z_{ij}}{\partial w_{kd}} = 0$ v·ªõi m·ªçi $j \ne k$.  

T·ªïng $\sum$ ƒë∆∞·ª£c r√∫t g·ªçn:

$$
\frac{\partial L_i}{\partial w_{kd}}
=
\frac{\partial L_i}{\partial z_{ik}}
\frac{\partial z_{ik}}{\partial w_{kd}}.
\tag{11}
$$

---

- Ta ƒë√£ c√≥: $$ \frac{\partial L_i}{\partial z_{ik}} = \hat{y}_{ik} - y_{ik} = e_{ik} $$ (·ªü c√¥ng th·ª©c s·ªë $(8)$ )

- Ta c√≥: $$ z_{ik} = \sum_{d'=1}^{D} w_{kd'} x_{id'} + b_k$$

- L·∫•y ƒë·∫°o h√†m $z_{ik}$ theo $w_{kd}$:
  $$
  \frac{\partial z_{ik}}{\partial w_{kd}} = x_{id}.
  $$

Suy ra:

$$
\frac{\partial L_i}{\partial w_{kd}}
=
e_{ik} \cdot x_{id}.
\tag{12}
$$

---

ƒê√¢y l√† ƒë·∫°o h√†m cho **m·ªôt ph·∫ßn t·ª≠** $(k, d)$ c·ªßa ma tr·∫≠n $W$.  
V·∫≠y:

$$
\frac{\partial L_i}{\partial W}
=
e_i x_i^T
=
(\hat{y}_i - y_i) x_i^T.
\tag{13}
$$

---

L·∫•y trung b√¨nh tr√™n $N$ m·∫´u:

$$
\frac{\partial L}{\partial W}
=
\frac{1}{N}
\sum_{i=1}^{N}
\frac{\partial L_i}{\partial W}
=
\frac{1}{N}
\sum_{i=1}^{N}
(\hat{y}_i - y_i) x_i^T.
\tag{14}
$$


**C√¥ng th·ª©c Gradient**

Sau khi r√∫t g·ªçn c√°c k·∫øt qu·∫£ ph·ª©c t·∫°p tr√™n, ta thu ƒë∆∞·ª£c c√¥ng th·ª©c sau:

$$
\nabla_W L
=
\frac{\partial L}{\partial W}
=
\frac{1}{N}
\sum_{i=1}^{N}
(\hat{y}_{i} - y_{i}) x_i^T
$$

$$
\nabla_b L
=
\frac{\partial L}{\partial b}
=
\frac{1}{N}
\sum_{i=1}^{N}
(\hat{y}_{i} - y_{i})
$$

---
Hay c√≥ th·ªÉ hi·ªÉu theo c√°ch ƒë∆°n gi·∫£n sau:

- $\nabla_{\text{weights}} L = \dfrac{1}{N} \sum (\text{d·ª± ƒëo√°n} - \text{th·ª±c t·∫ø}) \cdot (\text{ƒë·∫ßu v√†o})^T$

- $\nabla_{\text{bias}} L = \dfrac{1}{N} \sum (\text{d·ª± ƒëo√°n} - \text{th·ª±c t·∫ø})$



 









## Ph·∫ßn 5: Quy tr√¨nh hu·∫•n luy·ªán ##

Qu√° tr√¨nh hu·∫•n luy·ªán cho Softmax Regression kh√° gi·ªëng v·ªõi Linear v√† Logistic Regression. Qu√° tr√¨nh n√†y c≈©ng s·ª≠ d·ª•ng Batch Gradient Descent ƒë·ªÉ t·ªëi ∆∞u h√≥a c√°c tham s·ªë l√† ma tr·∫≠n W v√† vector h·ªá s·ªë bias b

<p align="center">
  <img src="{{ '/assets/module6-week2/Pipeline_Softmax_Regression.png' | relative_url }}" alt="Pipeline Diagram" width="600">
</p>













